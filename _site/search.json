[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is a Quarto-based course website for IAFF 6501 (Quantitative Analysis for IA Practitioners) at George Washington University. The course teaches R programming, data visualization, statistical inference, and modeling for international affairs students.\nPrimary Website: https://dataviz-gwu.rocks (built from https://quarto4ia.rocks)\n\n\n\n\nQuarto: Website builder and document renderer\nR/RStudio: Primary programming language and IDE\nR Packages: tidyverse (dplyr, ggplot2, readr, tidyr), vdemdata, leaflet, unvotes, lubridate, scales, and others\nQuarto Extensions: Located in _extensions/coatless/\n\n\n\n\n\n\n# Preview the website locally with live reload\nquarto preview\n\n# Render the entire website\nquarto render\n\n# Render a single document\nquarto render path/to/file.qmd\n\n# Render to specific format\nquarto render file.qmd --to html\nquarto render file.qmd --to pdf\n\n# Publish to configured destination\nquarto publish\n\n\n\n# Check Quarto version\nquarto --version\n\n# Install/update Quarto extensions\n# (Extensions are in _extensions/ directory)\n\n\n\n\n\n\nThe repository follows a modular structure with three parallel content hierarchies:\n\nmodules/ - Detailed learning modules with code examples and exercises\nslides/ - Reveal.js presentations corresponding to each module (named week-X.Y.qmd)\nweeks/ - Weekly landing pages that aggregate readings, videos, and links\n\n\n\n\n\nassignments/ - Homework assignments (homework-1.qmd, homework-2.qmd, homework-3.qmd)\nproject/ - Final project description and dataset information\nimages/ - Course images and logo files\nmodules/data/ - CSV data files used in modules (dem_data.csv, wb_data_clean.csv, etc.)\nmodules/functions/ - Reusable R functions (helper.R, wb-maps.R)\nslides/data/ - Data files used in slides\n_site/ - Generated website output (gitignored)\n_freeze/ - Quarto freeze cache for faster rebuilds\n.quarto/ - Quarto project cache\n\n\n\n\n\nindex.qmd - Main course schedule page (uses helper.R for date calculations)\ncourse-syllabus.qmd - Complete course syllabus\ncourse-support.qmd - Support resources\ncourse-links.qmd - Useful external links\ninstructor.qmd - Instructor information\n\n\n\n\n\n\n\nThe main Quarto project configuration file: - Website metadata (title, description, URLs) - Navigation sidebar structure - Theme settings (cosmo/cyborg for light/dark modes) - Font: “Atkinson Hyperlegible” - Output format settings\n\n\n\nDate calculation utilities used in index.qmd: - Defines semester start date (mon and tues variables) - advdate() function: calculates dates for course schedule - advdate2() function: formats dates with day names\nWhen updating the semester, change the start date in helper.R:\nmon &lt;- as_date(\"2026-01-12\")  # Update this for new semester\n\n\n\n\n\n\nStandard structure:\n---\ntitle: \"Module X.Y\"\nsubtitle: \"Topic Name\"\nformat: html\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\nModules typically include: - Embedded videos using  - Code chunks with #| label: syntax - Callout blocks for tips and notes: ::: {.callout-tip} - R code examples using tidyverse patterns - Links to external resources and documentation\n\n\n\nReveal.js presentations with standard YAML:\n---\ntitle: \"Course Title\"\nsubtitle: \"IAFF 6501\"\nfooter: \"[IAFF 6501 Website](https://quant4ia.rocks)\"\nlogo: images/iaff6501-logo.png\nformat:\n  revealjs:\n    theme: [simple, custom.scss]\n    transition: fade\n    slide-number: true\n    chalkboard: true\nexecute:\n  echo: false\n  message: false\n  warning: false\n  freeze: auto\n---\nSlides use: - ## for slide titles - Incremental lists: ::: incremental - Timer widgets from countdown package - Custom styling from slides/custom.scss\n\n\n\nHomework assignments follow a step-by-step structure: - Overview with context and academic references - Numbered steps with point values (e.g., “Step 1: Gather your data (20 pts)”) - Instructions in italics for students to complete - Bonus questions for extra credit - Submission instructions in callout blocks\n\n\n\n\n\n\nCommon pattern for downloading and filtering V-Dem democracy data:\nlibrary(vdemdata)\ndemocracy &lt;- vdem |&gt;\n  filter(year &gt;= 1990) |&gt;\n  select(\n    country = country_name,\n    vdem_ctry_id = country_id,\n    year,\n    polyarchy = v2x_polyarchy,\n    gdp_pc = e_gdppc,\n    region = e_regionpol_6C\n  ) |&gt;\n  mutate(\n    region = case_match(region,\n      1 ~ \"Eastern Europe\",\n      2 ~ \"Latin America\",\n      3 ~ \"Middle East\",\n      4 ~ \"Africa\",\n      5 ~ \"The West\",\n      6 ~ \"Asia\")\n  )\n\n\n\nStandard ggplot2 patterns used throughout: - Color-blind friendly palettes: viridis, manual scales - Theme: theme_minimal() is preferred - Proper labeling with labs() or xlab()/ylab()/ggtitle() - Custom color palette: cbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\n\n\n\n\n\n\nModules and Slides: Check if there’s a corresponding slide deck when editing modules\nSchedule Updates: Remember to update helper.R for new semester dates\nData Files: Module-specific data files are in modules/data/, shared data in slides/data/\nLinks: Internal links use relative paths (e.g., /modules/module-1.1.html)\nVideos: Embedded using Quarto video shortcode, hosted on YouTube\n\n\n\n\n\nFollow the naming convention: module-X.Y.qmd or week-X.Y.qmd\nAdd new pages to _quarto.yml sidebar navigation\nUpdate the schedule in index.qmd with appropriate links\nStore reusable functions in modules/functions/\nPlace data files in appropriate data/ subdirectories\n\n\n\n\n\nfreeze: false in _quarto.yml means code executes on every render\nIndividual files can override with freeze: auto to cache results\nSlide files typically use freeze: auto to speed up rendering\nClear the _freeze/ directory if experiencing caching issues\n\n\n\n\n\n\n\nThe course emphasizes: - Reproducible research using Quarto - Tidy data principles (tidyverse ecosystem) - Policy-relevant analysis for international affairs - Visualization skills for effective communication - Statistical literacy (inference, modeling, hypothesis testing) - Native pipe operator (|&gt;) over magrittr pipe (%&gt;%)\n\n\n\nStudents are expected to: - Use RStudio Desktop with Quarto - Create project-oriented workflows (no setwd()) - Submit rendered HTML documents to Blackboard - Optionally publish to Quarto Pub - Use GitHub Classroom for version control (referenced but not in repo)\n\n\n\n\nV-Dem: Democracy indicators, accessed via vdemdata package\nWorld Bank: Economic data, cleaned versions in wb_data_clean.csv\nUN Votes: Voting patterns, from unvotes package\nCustom CSVs: Pre-processed datasets in modules/data/\n\n\n\n\n\n\nThe course website is published to both dataviz-gwu.rocks and quant4ia.rocks\nRepository name references “spring-2024” but is used across multiple semesters\nHelper.R semester start date should be updated for each new term\nExtensions in _extensions/coatless/ are course-specific customizations\n.luarc.json present for Lua language server configuration"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is a Quarto-based course website for IAFF 6501 (Quantitative Analysis for IA Practitioners) at George Washington University. The course teaches R programming, data visualization, statistical inference, and modeling for international affairs students.\nPrimary Website: https://dataviz-gwu.rocks (built from https://quarto4ia.rocks)"
  },
  {
    "objectID": "CLAUDE.html#technology-stack",
    "href": "CLAUDE.html#technology-stack",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto: Website builder and document renderer\nR/RStudio: Primary programming language and IDE\nR Packages: tidyverse (dplyr, ggplot2, readr, tidyr), vdemdata, leaflet, unvotes, lubridate, scales, and others\nQuarto Extensions: Located in _extensions/coatless/"
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# Preview the website locally with live reload\nquarto preview\n\n# Render the entire website\nquarto render\n\n# Render a single document\nquarto render path/to/file.qmd\n\n# Render to specific format\nquarto render file.qmd --to html\nquarto render file.qmd --to pdf\n\n# Publish to configured destination\nquarto publish\n\n\n\n# Check Quarto version\nquarto --version\n\n# Install/update Quarto extensions\n# (Extensions are in _extensions/ directory)"
  },
  {
    "objectID": "CLAUDE.html#repository-structure",
    "href": "CLAUDE.html#repository-structure",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The repository follows a modular structure with three parallel content hierarchies:\n\nmodules/ - Detailed learning modules with code examples and exercises\nslides/ - Reveal.js presentations corresponding to each module (named week-X.Y.qmd)\nweeks/ - Weekly landing pages that aggregate readings, videos, and links\n\n\n\n\n\nassignments/ - Homework assignments (homework-1.qmd, homework-2.qmd, homework-3.qmd)\nproject/ - Final project description and dataset information\nimages/ - Course images and logo files\nmodules/data/ - CSV data files used in modules (dem_data.csv, wb_data_clean.csv, etc.)\nmodules/functions/ - Reusable R functions (helper.R, wb-maps.R)\nslides/data/ - Data files used in slides\n_site/ - Generated website output (gitignored)\n_freeze/ - Quarto freeze cache for faster rebuilds\n.quarto/ - Quarto project cache\n\n\n\n\n\nindex.qmd - Main course schedule page (uses helper.R for date calculations)\ncourse-syllabus.qmd - Complete course syllabus\ncourse-support.qmd - Support resources\ncourse-links.qmd - Useful external links\ninstructor.qmd - Instructor information"
  },
  {
    "objectID": "CLAUDE.html#key-configuration-files",
    "href": "CLAUDE.html#key-configuration-files",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The main Quarto project configuration file: - Website metadata (title, description, URLs) - Navigation sidebar structure - Theme settings (cosmo/cyborg for light/dark modes) - Font: “Atkinson Hyperlegible” - Output format settings\n\n\n\nDate calculation utilities used in index.qmd: - Defines semester start date (mon and tues variables) - advdate() function: calculates dates for course schedule - advdate2() function: formats dates with day names\nWhen updating the semester, change the start date in helper.R:\nmon &lt;- as_date(\"2026-01-12\")  # Update this for new semester"
  },
  {
    "objectID": "CLAUDE.html#content-patterns",
    "href": "CLAUDE.html#content-patterns",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Standard structure:\n---\ntitle: \"Module X.Y\"\nsubtitle: \"Topic Name\"\nformat: html\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\nModules typically include: - Embedded videos using  - Code chunks with #| label: syntax - Callout blocks for tips and notes: ::: {.callout-tip} - R code examples using tidyverse patterns - Links to external resources and documentation\n\n\n\nReveal.js presentations with standard YAML:\n---\ntitle: \"Course Title\"\nsubtitle: \"IAFF 6501\"\nfooter: \"[IAFF 6501 Website](https://quant4ia.rocks)\"\nlogo: images/iaff6501-logo.png\nformat:\n  revealjs:\n    theme: [simple, custom.scss]\n    transition: fade\n    slide-number: true\n    chalkboard: true\nexecute:\n  echo: false\n  message: false\n  warning: false\n  freeze: auto\n---\nSlides use: - ## for slide titles - Incremental lists: ::: incremental - Timer widgets from countdown package - Custom styling from slides/custom.scss\n\n\n\nHomework assignments follow a step-by-step structure: - Overview with context and academic references - Numbered steps with point values (e.g., “Step 1: Gather your data (20 pts)”) - Instructions in italics for students to complete - Bonus questions for extra credit - Submission instructions in callout blocks"
  },
  {
    "objectID": "CLAUDE.html#data-science-workflow-patterns",
    "href": "CLAUDE.html#data-science-workflow-patterns",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Common pattern for downloading and filtering V-Dem democracy data:\nlibrary(vdemdata)\ndemocracy &lt;- vdem |&gt;\n  filter(year &gt;= 1990) |&gt;\n  select(\n    country = country_name,\n    vdem_ctry_id = country_id,\n    year,\n    polyarchy = v2x_polyarchy,\n    gdp_pc = e_gdppc,\n    region = e_regionpol_6C\n  ) |&gt;\n  mutate(\n    region = case_match(region,\n      1 ~ \"Eastern Europe\",\n      2 ~ \"Latin America\",\n      3 ~ \"Middle East\",\n      4 ~ \"Africa\",\n      5 ~ \"The West\",\n      6 ~ \"Asia\")\n  )\n\n\n\nStandard ggplot2 patterns used throughout: - Color-blind friendly palettes: viridis, manual scales - Theme: theme_minimal() is preferred - Proper labeling with labs() or xlab()/ylab()/ggtitle() - Custom color palette: cbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")"
  },
  {
    "objectID": "CLAUDE.html#development-guidelines",
    "href": "CLAUDE.html#development-guidelines",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Modules and Slides: Check if there’s a corresponding slide deck when editing modules\nSchedule Updates: Remember to update helper.R for new semester dates\nData Files: Module-specific data files are in modules/data/, shared data in slides/data/\nLinks: Internal links use relative paths (e.g., /modules/module-1.1.html)\nVideos: Embedded using Quarto video shortcode, hosted on YouTube\n\n\n\n\n\nFollow the naming convention: module-X.Y.qmd or week-X.Y.qmd\nAdd new pages to _quarto.yml sidebar navigation\nUpdate the schedule in index.qmd with appropriate links\nStore reusable functions in modules/functions/\nPlace data files in appropriate data/ subdirectories\n\n\n\n\n\nfreeze: false in _quarto.yml means code executes on every render\nIndividual files can override with freeze: auto to cache results\nSlide files typically use freeze: auto to speed up rendering\nClear the _freeze/ directory if experiencing caching issues"
  },
  {
    "objectID": "CLAUDE.html#course-specific-context",
    "href": "CLAUDE.html#course-specific-context",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The course emphasizes: - Reproducible research using Quarto - Tidy data principles (tidyverse ecosystem) - Policy-relevant analysis for international affairs - Visualization skills for effective communication - Statistical literacy (inference, modeling, hypothesis testing) - Native pipe operator (|&gt;) over magrittr pipe (%&gt;%)\n\n\n\nStudents are expected to: - Use RStudio Desktop with Quarto - Create project-oriented workflows (no setwd()) - Submit rendered HTML documents to Blackboard - Optionally publish to Quarto Pub - Use GitHub Classroom for version control (referenced but not in repo)\n\n\n\n\nV-Dem: Democracy indicators, accessed via vdemdata package\nWorld Bank: Economic data, cleaned versions in wb_data_clean.csv\nUN Votes: Voting patterns, from unvotes package\nCustom CSVs: Pre-processed datasets in modules/data/"
  },
  {
    "objectID": "CLAUDE.html#important-notes",
    "href": "CLAUDE.html#important-notes",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The course website is published to both dataviz-gwu.rocks and quant4ia.rocks\nRepository name references “spring-2024” but is used across multiple semesters\nHelper.R semester start date should be updated for each new term\nExtensions in _extensions/coatless/ are course-specific customizations\n.luarc.json present for Lua language server configuration"
  },
  {
    "objectID": "modules/module-5.2.html",
    "href": "modules/module-5.2.html",
    "title": "Module 5.2",
    "section": "",
    "text": "Prework\n\n\n\nClick on Code toggle below to unfold the setup code chunk. Then, copy and run the code in your Quarto notebook to load the necessary packages and create the data frame for this lesson.\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\n    \"v2x_polyarchy\",\n    \"v2x_gender\",\n    \"v2cacamps\",\n    \"v2x_regime\",\n    \"e_regionpol_6C\"\n    ),\n    start_year = 2022, \n    end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    polyarchy = v2x_polyarchy, \n    women_empowerment = v2x_gender,\n    polarization = v2cacamps,\n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~ \"Electoral Democracy\",\n                    3 ~ \"Liberal Democracy\")\n  )\n\n#glimpse(vdem2022)",
    "crumbs": [
      "Course Modules",
      "Module 5.2"
    ]
  },
  {
    "objectID": "modules/module-5.2.html#overview",
    "href": "modules/module-5.2.html#overview",
    "title": "Module 5.2",
    "section": "Overview",
    "text": "Overview\nIn our last module, we explored tools available for examining categorical data—variables that capture groupings or labels, like regime type or world region. We looked at bar charts and how grouping by a categorical variable can help us uncover patterns in data. In this module, we turn our attention to continuous data.\nContinuous variables are numeric measurements that can take on an infinite range of values within a given interval. Think of indicators like GDP per capita, population size, or life expectancy—these are variables that allow us to compare magnitude, observe variation, and investigate relationships between quantities.\nIn this lesson, we will learn how to explore continuous variables. We will look at different types of distributions and learn how to visualize a single continuous variable with histograms and density plots. Then we will look at how to compare distributions across groups using ridge plots.",
    "crumbs": [
      "Course Modules",
      "Module 5.2"
    ]
  },
  {
    "objectID": "modules/module-5.2.html#what-does-the-distribution-look-like",
    "href": "modules/module-5.2.html#what-does-the-distribution-look-like",
    "title": "Module 5.2",
    "section": "What Does the Distribution Look Like?",
    "text": "What Does the Distribution Look Like?\nTo understand continuous variables, we often begin by examining their distribution through a visual representation of how values are spread out across the range with a histogram or a density plot. These shapes tell us a lot about the nature of the data and can guide our decisions about how to summarize or transform variables.\nLet’s look at some histograms displaying common types of distributions you’re likely to encounter. A histogram shows how many observations fall into different ranges of values, allowing us to see patterns like skewness, modality, and clustering. They do this by grouping values into bins and counting how many observations fall into each one. Think of it as slicing up the number line into segments and stacking up bars based on how many countries fall into each slice.\nSymmetric and Bell-Shaped\nWhen most values are clustered around the center, with fewer values tapering off evenly on both sides, we call the distribution symmetric or bell-shaped. This kind of distribution is common in physical measurements and standardized test scores. It’s also the foundation of many statistical techniques that assume normality.\n\n\n\n\n\n\n\n\nRight-Skewed\nA right-skewed distribution (also known as positively skewed) has a long tail stretching to the right. This often occurs when values are bounded at zero but can stretch very far in the positive direction. GDP per capita is a classic example: most countries are clustered at the lower end, with a few very wealthy outliers pulling the tail to the right.\n\n\n\n\n\n\n\n\nLeft-Skewed\nLess common, but still important, are left-skewed (negatively skewed) distributions, where the tail stretches to the left. This might happen with variables that have an upper bound, like survey responses with a maximum score, where a majority of responses are at the top end but a few fall below.\n\n\n\n\n\n\n\n\nBimodal\nA bimodal distribution has two peaks — two distinct groups of values. This often signals that your data may actually come from two different populations. For example, if you combine data on voter turnout from democratic and authoritarian regimes, you might see one peak for each group.\n\n\n\n\n\n\n\n\nUniform\nA uniform distribution has no peaks or valleys — all values are equally likely. This is relatively rare in real-world data but can occur in random sampling or when measuring something that has been evenly distributed across a range.\n\n\n\n\n\n\n\n\nEach of these shapes tells a different story about how values are distributed — and that story helps us decide how to summarize the variable. For example, when a distribution is symmetric, the mean and median are usually close together. But in a skewed distribution, the mean gets pulled toward the long tail, making it a less reliable summary on its own.",
    "crumbs": [
      "Course Modules",
      "Module 5.2"
    ]
  },
  {
    "objectID": "modules/module-5.2.html#creating-a-histogram",
    "href": "modules/module-5.2.html#creating-a-histogram",
    "title": "Module 5.2",
    "section": "Creating a Histogram",
    "text": "Creating a Histogram\nNow that we have been exposed to a number of different types of distributions, lets practice making histograms so that we can explore continous data on our own. To make a histogram in ggplot2, we use the geom_histogram() function. This function takes an aesthetic mapping (aes()) that specifies which variable to plot on the x-axis, and it automatically counts how many observations fall into each bin.\nLet’s use the vdem2022 data frame that we created in the setup chunk to visualize the distribution of the V-Dem polyarchy scores for 2022:\n\nggplot(vdem2022, aes(x = polyarchy)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of V-Dem Polyarchy Scores, 2022\",\n    x = \"Polyarchy Score\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\nWhen we visualize polyarchy, we see that it is non-normal. The distribution appears to be multi-modal or perhaps even, with peaks occurring in multiple places. It is somewhat hard to tell what is going on with the data here.",
    "crumbs": [
      "Course Modules",
      "Module 5.2"
    ]
  },
  {
    "objectID": "modules/module-5.2.html#visualizing-distributions-with-density-plots",
    "href": "modules/module-5.2.html#visualizing-distributions-with-density-plots",
    "title": "Module 5.2",
    "section": "Visualizing Distributions with Density Plots",
    "text": "Visualizing Distributions with Density Plots\nSometimes it can be easier to see what is happening with the distribution if we look at a density plot instead of a histogram. Density plots are similar in spirit to histograms but use a smoothed curve to estimate the shape of the distribution. Rather than count how many values fall into each bin, density plots estimate how likely it is to see values in different parts of the range.\nThey are particularly helpful when you want to compare distributions across groups or highlight subtler patterns that might be obscured by binning choices in a histogram. To create a density plot with ggplot2 we use the geom_density() function. This function also takes an aesthetic mapping (aes()) that specifies which variable to plot on the x-axis, and it automatically estimates the density of values across the range.\nHere’s how you could create a density plot of the polyarchy score:\n\nggplot(vdem2022, aes(x = polyarchy)) +\n  geom_density(fill = \"steelblue\", alpha = 0.6) +\n  theme_minimal() +\n  labs(\n    title = \"Smoothed Distribution of Polyarchy Scores, 2022\",\n    x = \"Polyarchy Score\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\nHere it looks like we have two distinct peaks in the distribution, which suggests a that there are two groups of countries–one concentrating around a low polyarchy score and another around a high polyarchy score. In other words, the distribution appears to be bimodal, with one peak occuring at a polyarchy score of 0.2 and another around 0.8.\nBecause density plots are continuous, they’re often more elegant for comparison across groups—especially when you overlay multiple distributions on the same plot. To do that, we can add a fill aesthetic inside aes() to color the density curves by region:\n\nggplot(vdem2022, aes(x = polyarchy, fill = region)) +\n  geom_density(alpha = 0.6) +\n  theme_minimal() +\n  labs(\n    title = \"Smoothed Distribution of Polyarchy Scores, 2022\",\n    x = \"GDP per Capita\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that if you use the fill aesthetic, you should remove the fill = argument from geom_density(). However, you should still set the alpha parameter to make the colors semi-transparent. This allows you to see overlapping areas more clearly.\n\n\nAn even better solution for this is a ridge plot, which is a type of density plot that displays multiple distributions stacked on top of each other. This allows us to see how the distributions compare across groups more clearly. To create a ridge plot, we can use the geom_density_ridges() function from the ggridges package:\n\nlibrary(ggridges)\n  ggplot(vdem2022, aes(x = polyarchy, y = region, fill = region)) +\n    geom_density_ridges() +\n  labs(\n    x = \"Electoral Democracy\",\n    y = \"Region\",\n    title = \"A Ridge Plot\",\n    caption = \"Source: V-Dem Institute\",\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nNow we can really see what is going on with these data. Each region has an almost unique distribution: the Middle East and Africa appear to be right-skewed; Latin America is left skewed; and the West is normally distributed. The distribution of democracy scores in other regions appears to be multimodal or unimodal. This is a very different conclusion that we would have reached had we just looked at a simple histogram!\n\n\n\n\n\n\nYour Turn!!\n\n\n\nLet’s practice visualizing a continuous variable with both histograms and density plots.\nUse the vdem2022 dataset to do the following:\n\nCreate a histogram of the women_empowerment variable. Try adjusting the number of bins to see how it affects the visualization.\nNow create a density plot of the women_empowerment variable. What do you notice about the shape? Does it make the distribution clearer than the histogram?\nTry adding a fill = region aesthetic inside aes() to visualize the distribution of population by region. What patterns do you see? How does the distribution of women’s empowerment vary across regions?\nNow try the above steps with the polarization variable. What does the distribution look like? How does it compare to the other variables?",
    "crumbs": [
      "Course Modules",
      "Module 5.2"
    ]
  },
  {
    "objectID": "modules/module-5.1.html",
    "href": "modules/module-5.1.html",
    "title": "Module 5.1",
    "section": "",
    "text": "Prework\n\n\n\nClick on Code toggle below to unfold the setup code chunk. Then, copy and run the code in your Quarto notebook to load the necessary packages and create the data frame for this lesson.\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\n    \"v2x_polyarchy\",\n    \"v2x_gender\",\n    \"v2cacamps\",\n    \"v2x_regime\",\n    \"e_regionpol_6C\"\n    ),\n    start_year = 2022, \n    end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    polyarchy = v2x_polyarchy, \n    women_empowerment = v2x_gender,\n    polarization = v2cacamps,\n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~ \"Electoral Democracy\",\n                    3 ~ \"Liberal Democracy\")\n  )\n\n#glimpse(vdem2022)",
    "crumbs": [
      "Course Modules",
      "Module 5.1"
    ]
  },
  {
    "objectID": "modules/module-5.1.html#overview",
    "href": "modules/module-5.1.html#overview",
    "title": "Module 5.1",
    "section": "Overview",
    "text": "Overview\nIn this module, we explore how to work with categorical data, focusing on how to classify, summarize, and visualize it effectively. We begin by discussing the different types of data—categorical vs. numerical, discrete vs. continuous, and so on—and why it matters to distinguish among them. Then, using democracy indicators from the V-Dem dataset, we learn how to visualize distributions of categorical variables with geom_bar(). Finally, we take our analysis further by examining how regime types vary by world region, introducing the distinction between nominal and ordinal categorical variables and learning how to create comparative bar plots using proportions. Along the way, you’ll have opportunities to apply what you’ve learned through hands-on coding and interpretation.",
    "crumbs": [
      "Course Modules",
      "Module 5.1"
    ]
  },
  {
    "objectID": "modules/module-5.1.html#what-kind-of-data-do-we-have",
    "href": "modules/module-5.1.html#what-kind-of-data-do-we-have",
    "title": "Module 5.1",
    "section": "What Kind of Data Do We Have?",
    "text": "What Kind of Data Do We Have?\nBefore we can summarize or model any dataset, we need to pause and reflect on what kind of data we’re actually dealing with. This is an important consideration because how we classify our data shapes the types of questions we can ask, the summaries we can produce, and the visualizations we can make.\nData can differ in many important ways. One useful distinction is between anecdotal and representative data. Anecdotal data might come from a single experience or a small number of observations, like a friend’s story about a trip or a journalist’s report on a protest. These can be powerful or evocative, but they don’t necessarily generalize. In contrast, representative data are collected systematically with an eye toward capturing a larger population or phenomenon. For example, a random sample of households in a country tells us much more about living conditions overall than a few interviews with individual families.\nAnother key distinction is between census and sample data. A census tries to gather data on the entire population—think of the U.S. Census or a complete list of all registered voters. Most often, though, we work with samples: smaller, more manageable subsets of data drawn from a larger group. Whether we’re looking at a dozen countries or a thousand people, it’s important to understand how that sample was drawn and what population it represents.\nWe also need to be clear on whether our data come from an observational or an experimental study. Observational data are collected without interfering with the system we’re studying—just watching and recording. Experimental data, on the other hand, involve interventions or treatments, like randomly assigning people to different programs and comparing outcomes.\nOne of the most fundamental distinctions—and the focus of this module—is between categorical and numerical variables. Categorical variables place observations into groups, like types of political regimes or preferred news sources. Numerical variables are measured on a scale, such as GDP per capita or number of protests.\nAmong numerical variables, we often distinguish between discrete variables, which take whole number values (like counts), and continuous variables, which can take on any value in a range (like income or temperature).\nThe way we collect data also matters. Are we working with a cross-sectional snapshot, showing one moment in time? Or is it a time series, tracking changes over time? In this class, we will be focused primarily on cross-sectional data, but it is important to realize when you are looking at a dataset that incorporates a time-series dimension.\nFinally, not all data come in neat rows and columns. In this class we work with structured data: spreadsheets, tables, and rectangular data frames. But data can also be unstructured, like text, images, or videos, which require special tools to analyze.\nUnderstanding these distinctions gives us a foundation for exploring the types of variables we encounter in political, social, and economic datasets.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nClassify the following variables based on the distinctions we’ve just covered:\n\nIs a country a democracy? (yes/no)\nPolity score (ranges from -10 to 10)\nV-Dem Polyarchy index (0 to 1)\nV-Dem Regimes of the World classification (closed autocracy, electoral autocracy, etc.)\nNumber of protest events\nProtest types (sit-in, march, strike, etc.)\n\nThink about whether each variable is categorical or numerical, and if so, what kind. Can you spot any that might be tricky to classify?",
    "crumbs": [
      "Course Modules",
      "Module 5.1"
    ]
  },
  {
    "objectID": "modules/module-5.1.html#exploring-categorical-data",
    "href": "modules/module-5.1.html#exploring-categorical-data",
    "title": "Module 5.1",
    "section": "Exploring Categorical Data",
    "text": "Exploring Categorical Data\nLet’s now take a closer look at categorical variables by examining a real-world dataset. One commonly used measure of democracy is V-Dem’s Regimes of the World classification. This variable categorizes countries into four types:\n\nClosed Autocracy\nElectoral Autocracy\nElectoral Democracy\nLiberal Democracy\n\nThese categories are mutually exclusive and ordered from least to most democratic, making this an ordinal categorical variable.\nLet’s use the data from the prework section of this module to explore the distribution of these regime types across the world in 2022. We can start by just glimpsing the data to see what we have:\n\nglimpse(vdem2022)\n\nRows: 179\nColumns: 9\n$ country           &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghan…\n$ country_text_id   &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MM…\n$ country_id        &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19,…\n$ year              &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n$ polyarchy         &lt;dbl&gt; 0.571, 0.775, 0.896, 0.897, 0.664, 0.711, 0.817, 0.0…\n$ women_empowerment &lt;dbl&gt; 0.773, 0.834, 0.932, 0.942, 0.823, 0.839, 0.794, 0.3…\n$ polarization      &lt;dbl&gt; 1.586, -0.625, -1.486, -1.268, -0.615, 0.411, -1.715…\n$ regime            &lt;chr&gt; \"Electoral Democracy\", \"Electoral Democracy\", \"Liber…\n$ region            &lt;chr&gt; \"Latin America\", \"Latin America\", \"The West\", \"The W…\n\n\nTo begin summarizing the distribution of regimes, we can use the count() function:\n\nvdem2022 |&gt;\n  count(regime)\n\n               regime  n\n1    Closed Autocracy 33\n2 Electoral Autocracy 54\n3 Electoral Democracy 58\n4   Liberal Democracy 34\n\n\nThis gives us a frequency table showing how many countries fall into each regime category. But sometimes, a table doesn’t give us the full picture. Visualizations can help us better see patterns and communicate them clearly. Let’s create a bar plot using the geom_bar() geom:\n\nvdem2022 |&gt;\n  ggplot(aes(x = regime)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    x = \"Regime\",\n    y = \"Frequency\",\n    title = \"Regimes of the World in 2022\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nThis simple bar plot provides an immediate visual summary of the global distribution of regime types in 2022. We see that electoral democracies are the most common regime type, followed by electoral autocracies, with closed autocracies being the least common.\nThe plot highlights the usefulness of geom_bar() as a straightforward way to create bar plots in ggplot2 when working with categorical variables. Unlike geom_col(), which requires both x and y aesthetics (typically used when you already have counts or proportions computed), geom_bar() automatically calculates the heights of the bars for you. It counts the number of occurrences of each category in the variable you supply to the x aesthetic.\nThis makes geom_bar() particularly convenient for quick summaries. It’s similar in spirit to geom_histogram(), which bins and counts continuous data, except that here, the binning is categorical. You only need to specify the variable for the x-axis—ggplot2 handles the y-axis internally based on the counts.\nFrom here, we can begin asking deeper questions such as whether certain regime types are more common in some regions than others, or how these distributions change over time. We’ll explore these questions as we move forward.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nExpore the distribution of regimes for a different year\nPreprocess your data to include only the year you are interested in\nVisualize the distribution of regimes using geom_bar()\n\nUse the labs() function to change title\nWhat is different about the year that you chose relative to 2022?",
    "crumbs": [
      "Course Modules",
      "Module 5.1"
    ]
  },
  {
    "objectID": "modules/module-5.1.html#regimes-by-region",
    "href": "modules/module-5.1.html#regimes-by-region",
    "title": "Module 5.1",
    "section": "Regimes by Region",
    "text": "Regimes by Region\nNow that we’ve examined the overall distribution of regime types, let’s take things a step further and explore how these regimes vary across different parts of the world.\nTo do this, we can use a grouped bar plot by using geom_bar() with two categorical variables: region and regime type. Region will go on the x-axis, and regime type will be mapped to the fill aesthetic. This allows us to visualize how the distribution of political regimes differs from one world region to another.\n\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)\n\n\n\n\n\n\n\nIt’s worth noting the types of categorical variables we’re using here. Regime type is an ordinal categorical variable—its categories follow a meaningful order, from closed autocracy to liberal democracy. Region, on the other hand, is a nominal categorical variable—its categories (like Asia, Latin America, or Africa) don’t have an inherent order.\nBy combining a nominal categorical variable with an ordinal one in a single plot, we can explore important patterns in the data, such as where the most liberal democracies are located or where autocratic regimes are most common.\nThis chart gives us a useful breakdown of regime types by region. However, one challenge immediately becomes apparent: the number of countries varies across regions. Africa, for example, has far more countries than Eastern Europe or the Middle East. So, even if every region had the same proportions of regime types, the bars would be taller in regions with more countries.\nTo address this, we can switch from raw counts to proportions using position = \"fill\" inside geom_bar(). This stacks the bars so that they all have the same height, and each segment reflects the proportion of countries in each regime category:\n\nvdem2022 %&gt;%\n  ggplot(., aes(x = region, fill = regime)) +\n      geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Proportion\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)\n\n\n\n\n\n\n\nNow we can better compare regime distributions across regions, without having to worry about differences in the number of countries. For example, we might notice that liberal democracies are more prevalent in “The West,” while electoral democracies concentrate in Eastern Europe and electoral autocracies dominate in Africa.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nExplore the distribution of regimes by region for a different year:\n\nUse fetchdem to load data on regimes for a year that you are interested\nVisualize the distribution of regimes using geom_bar() and position = \"fill\"\n\nUse the labs() function to change the plot title\nWhat’s different about the year you chose compared to 2022?",
    "crumbs": [
      "Course Modules",
      "Module 5.1"
    ]
  },
  {
    "objectID": "modules/module-3.2.html",
    "href": "modules/module-3.2.html",
    "title": "Module 3.2",
    "section": "",
    "text": "Now that we have completed some wrangling, let’s do something with our data. A common sequence in data science is group by(), summarize() and arrange(). First, we group the data by certain value or category. Then we summarize it by applying a function like min(), max(), mean(), median() or sd(). Finally, we order the data according to column values.\nLet’s go ahead and apply our three new verbs to a data frame and store the resulting new data frame in an object.\nYou can download the raw data from here and read it into your environment with the read_csv() function from the readr package. Store it in an object called dem_women.\nWe will group the data by region, take the mean of each variable, and sort the data in descending order based on the regions’ polyarchy scores. Then we will print the object to view its contents. Along the way, we can also export the data to a .csv file for future use.\n\n\n\n\n\n\nNote\n\n\n\nRemember to check your file path when reading in a CSV file. If the file is not in the same directory as your R script, you will need to specify the path to the file.\n\n\n\n# Load packages\n\nlibrary(dplyr)\nlibrary(readr)\n\ndem_women &lt;- read_csv(\"data/dem_women.csv\") # if the path is different, adjust this code\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- dem_women |&gt; # save result as new object\n  group_by(region)  |&gt; # group dem_women data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    flfp = mean(flfp, na.rm = TRUE), \n    women_rep = mean(women_rep, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Save as .csv for future use\nwrite_csv(dem_summary, \"data/dem_summary.csv\")\n\n# View the data\nglimpse(dem_summary)\n\nRows: 6\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\", \"Afri…\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319\n$ flfp      &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.21568",
    "crumbs": [
      "Course Modules",
      "Module 3.2"
    ]
  },
  {
    "objectID": "modules/module-3.2.html#group-summarize-and-arrange",
    "href": "modules/module-3.2.html#group-summarize-and-arrange",
    "title": "Module 3.2",
    "section": "",
    "text": "Now that we have completed some wrangling, let’s do something with our data. A common sequence in data science is group by(), summarize() and arrange(). First, we group the data by certain value or category. Then we summarize it by applying a function like min(), max(), mean(), median() or sd(). Finally, we order the data according to column values.\nLet’s go ahead and apply our three new verbs to a data frame and store the resulting new data frame in an object.\nYou can download the raw data from here and read it into your environment with the read_csv() function from the readr package. Store it in an object called dem_women.\nWe will group the data by region, take the mean of each variable, and sort the data in descending order based on the regions’ polyarchy scores. Then we will print the object to view its contents. Along the way, we can also export the data to a .csv file for future use.\n\n\n\n\n\n\nNote\n\n\n\nRemember to check your file path when reading in a CSV file. If the file is not in the same directory as your R script, you will need to specify the path to the file.\n\n\n\n# Load packages\n\nlibrary(dplyr)\nlibrary(readr)\n\ndem_women &lt;- read_csv(\"data/dem_women.csv\") # if the path is different, adjust this code\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- dem_women |&gt; # save result as new object\n  group_by(region)  |&gt; # group dem_women data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    flfp = mean(flfp, na.rm = TRUE), \n    women_rep = mean(women_rep, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Save as .csv for future use\nwrite_csv(dem_summary, \"data/dem_summary.csv\")\n\n# View the data\nglimpse(dem_summary)\n\nRows: 6\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\", \"Afri…\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319\n$ flfp      &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.21568",
    "crumbs": [
      "Course Modules",
      "Module 3.2"
    ]
  },
  {
    "objectID": "modules/module-3.1.html",
    "href": "modules/module-3.1.html",
    "title": "Module 3.1",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the devtools package. Type install.packages(\"devtools\") in your console. You will need this to install the vdemdata package because it is not on the CRAN Network.\nInstall the vdemdata package from GitHub. Type devtools::install_github(\"vdeminstitute/vdemdata\") in your console.\nGenerate a quarto document named “module-1.2.qmd” in your modules project folder so that you can code along with me",
    "crumbs": [
      "Course Modules",
      "Module 3.1"
    ]
  },
  {
    "objectID": "modules/module-3.1.html#overview",
    "href": "modules/module-3.1.html#overview",
    "title": "Module 3.1",
    "section": "Overview",
    "text": "Overview\nIn this module working, we are going to be working with data from an API. We are going to be working with the vdemdata in this lesson and some others later in the course.\nAlong the way, we are going to continue to extend our data wrangling skills. We will learn some new functions in dplyr that will help us get our data into a usable form for analysis. We are also going to cover in depth some common data science workflows, including filtering observations, selecting variables, summarizing data for different groups, and sorting data based on column values.",
    "crumbs": [
      "Course Modules",
      "Module 3.1"
    ]
  },
  {
    "objectID": "modules/module-3.1.html#downloading-data-from-the-v-dem-api",
    "href": "modules/module-3.1.html#downloading-data-from-the-v-dem-api",
    "title": "Module 3.1",
    "section": "Downloading data from the V-Dem API",
    "text": "Downloading data from the V-Dem API\n\nThe first thing we want to talk about is how to filter observations and to select variables for analysis. We will also delve into the topic of how to create new variables. To illustrate these concepts, we are going to be working with the V-Dem Dataset. The V-Dem offers an R package for downloading its data called vdemdata.\nvdemdata is perfect for illustrating the filter() and select() verbs because its main function for downloading the data (vdem) does not take any arguments (it simply downloads the whole dataset). So you have to use R functions to narrow down the variables and years you want to work with.\nWhile V-Dem has wealth of indicators related to democracy, we are going to focus on the most famous one called the “polyarchy” score. We are also going to download data on per capita GDP and create some indicator variables for region that we will use later on when we summarize the data. Along with those variables, we also want to retain country_name, year and country_id for the purposes of merging these data with our World Bank data.\n\n\n\n\n\n\nNote\n\n\n\nWhile V-Dem has a variable look-up tool (find_var), it does not provide very much information on the variables that the search function returns. Therefore, if you want to use this package for your own research, I highly recommend just going to the V-Dem codebook and manually grabbing the codes for the indicators that you want to use in your analysis.\n\n\nIn addition to filtering out years and selecting variables, let’s also create a region coding to facilitate our analysis later on. We will do this by piping in a mutate() call where we use the case_match() function to change the region from a numeric variable to a string. This will come in handy when we go to visualize the data in future lessons.\nWe will store our new data as an object called democracy.\n\n# Load packages\nlibrary(dplyr)\nlibrary(vdemdata) # to download V-Dem data\n\n# Download the data\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year &gt;= 1990)  |&gt; # filter out years less than 1990\n  select(                  # select (and rename) these variables\n    country = country_name,     # the name before the = sign is the new name  \n    vdem_ctry_id = country_id,  # the name after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n                    # number on the left of the ~ is the V-Dem region code\n                    # we are changing the number to the country name on the right\n                    # of the equals sign\n  )\n\n# View the data\nglimpse(democracy)\n\nRows: 6,204\nColumns: 6\n$ country      &lt;chr&gt; \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico…\n$ vdem_ctry_id &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ year         &lt;dbl&gt; 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 199…\n$ polyarchy    &lt;dbl&gt; 0.389, 0.412, 0.437, 0.447, 0.470, 0.480, 0.508, 0.556, 0…\n$ gdp_pc       &lt;dbl&gt; 24.396, 25.077, 25.561, 25.967, 26.101, 25.434, 25.851, 2…\n$ region       &lt;chr&gt; \"Latin America\", \"Latin America\", \"Latin America\", \"Latin…",
    "crumbs": [
      "Course Modules",
      "Module 3.1"
    ]
  },
  {
    "objectID": "modules/module-4.11.html",
    "href": "modules/module-4.11.html",
    "title": "Module 4.1",
    "section": "",
    "text": "Prework\n\n\n\n\nGet a U.S. Census api key\n\nInstall tidycensus to retrieve the data we will use\nInstall kableExtra and gt, out packages for making tables\ninstall webshot2 for exporting gt tables\n\ninstall.packages(c(\"tidycensus\", \"kableExtra\", \"gt\", \"webshot2\"))\n\nWe will be using the stringr package, which is part of the Tidyverse, so you probably already have it installed. But spend some time reading about its usage and features.\nInstall webshot2 for the purposes of saving a .png of your table (install.packages(\"webshot2\"))"
  },
  {
    "objectID": "modules/module-4.11.html#overview",
    "href": "modules/module-4.11.html#overview",
    "title": "Module 4.1",
    "section": "Overview",
    "text": "Overview\nThis week we are going to be talking about making tables in R. Tables can be a great way to summarize data for your audience. While there are no hard and fast rules about when to use a table versus a plot, typically we use tables when we want to present summary statistics that we want to compare across groups or when we want to show the precise values for individual data points. This can be true when we have a small number of cases that we want to discuss.\nIn this module we are going to be working with the tidycensus package to download income data from the American Community Survey (ACS) and the kableExtra and gt packages to visualize it. Along the way, we discuss “the grammar of tables” and some situations where a table would be less appropriate than other methods of visualizing our data."
  },
  {
    "objectID": "modules/module-4.11.html#working-with-tidycensus",
    "href": "modules/module-4.11.html#working-with-tidycensus",
    "title": "Module 4.1",
    "section": "Working with tidycensus",
    "text": "Working with tidycensus\n\nWe are going to start by using tidycensus to download some income data. To use tidycensus you need a Census API key, which you can get here.\n\nlibrary(tidycensus)\ncensus_api_key(\"YOUR API KEY\") # enter your census api key here in quotes\n\nUse the load_variables() function to import data from the census or ACS for a particular year. There is a cache = TRUE option if you want to cache the data for faster retrieval in the future. We can save our data in an object called v21 and then click on it or use View() and the search function in the data frame viewer to see what data are available.\n\nv21 &lt;- load_variables(2021, \"acs5\", cache = TRUE)\n\n#View(v21)\n\nWe want data on income quintiles, so let’s search for “quintile” in the search field. From there we use get_acs() to retrieve the data based on the codes for the five quintiles and the top five percent of earners.\nIn our call, we specify “state” as the geography and “2021” as the year. This will ensure that data from the 2017-2021 ACS is retrieved. Note that by default, tidycensus returns data such that rows represent a unit-variable combination. To get the data with census variables in the columns, we have to specify wide form with output = \"wide\". We will select all of the variables except for the margin of error and GEOID. Let’s rename NAME so it is in lower case. And for some reason, tidycensus puts an “E” suffix at the end of all of our variables when we specify wide format, so let’s use rename_with() and str_remove from the [stringr] package to get rid of that suffix. We will save the data frame in an object called quintiles.\n\nlibrary(stringr)\nlibrary(dplyr)\n\nquintiles &lt;- get_acs(geography = \"state\", \n                      variables = c(q1 = \"B19081_001\",\n                                    q2 = \"B19081_002\",\n                                    q3 = \"B19081_003\",\n                                    q4 = \"B19081_004\",\n                                    q5 = \"B19081_005\",\n                                    top5 = \"B19081_006\"),\n                      year = 2021,\n                      output = \"wide\") |&gt;\n                      select(\n                        !ends_with(\"M\"), # eliminate margin of error\n                        -GEOID) |&gt; # eliminate geo id\n                      rename(name = NAME) |&gt;\n                      rename_with(~str_remove(., 'E'))\n    \n\nglimpse(quintiles)\n\nRows: 52\nColumns: 7\n$ name &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colora…\n$ q1   &lt;dbl&gt; 11602, 19344, 15486, 12076, 17433, 18963, 17417, 17052, 12971, 14…\n$ q2   &lt;dbl&gt; 31928, 51030, 40774, 31051, 49234, 49711, 48870, 44638, 51060, 37…\n$ q3   &lt;dbl&gt; 55270, 81169, 66384, 52280, 84658, 80679, 84042, 73187, 94478, 62…\n$ q4   &lt;dbl&gt; 88640, 122652, 102299, 82811, 134560, 123359, 133488, 111915, 157…\n$ q5   &lt;dbl&gt; 193311, 242097, 223521, 188510, 309857, 264516, 319533, 238612, 3…\n$ top5 &lt;dbl&gt; 336788, 394694, 395620, 344470, 555007, 466181, 602707, 420859, 6…"
  },
  {
    "objectID": "modules/module-4.11.html#explore-the-data-with-kableextra",
    "href": "modules/module-4.11.html#explore-the-data-with-kableextra",
    "title": "Module 4.1",
    "section": "Explore the data with kableExtra",
    "text": "Explore the data with kableExtra\n\nNow let’s use some dplyr slice functions and kableExtra to subset and explore the data. First let’s see which are the wealthiest states. To do this, we can apply slice_max() to identify the states with the highest incomes among the top 5 percent of wage earners. We will save that list as an object called top_10 and then call kable() to view it.\n\nlibrary(kableExtra)\n\ntop_10 &lt;- quintiles |&gt;\n  slice_max(top5, n = 10)\n\nkable(top_10)\n\n\n\nname\nq1\nq2\nq3\nq4\nq5\ntop5\n\n\n\nDistrict of Columbia\n12971\n51060\n94478\n157803\n375792\n670768\n\n\nConnecticut\n17417\n48870\n84042\n133488\n319533\n602707\n\n\nNew York\n14054\n42220\n75647\n123318\n302676\n574063\n\n\nNew Jersey\n18458\n52339\n90337\n142858\n319140\n562886\n\n\nMassachusetts\n16812\n50519\n89602\n142491\n316447\n558616\n\n\nCalifornia\n17433\n49234\n84658\n134560\n309857\n555007\n\n\nMaryland\n19946\n55165\n91725\n140353\n293979\n503597\n\n\nWashington\n19367\n50803\n82817\n127003\n277165\n487950\n\n\nVirginia\n17922\n48359\n81072\n127411\n280299\n486006\n\n\nIllinois\n15102\n42688\n72900\n114531\n258373\n466713\n\n\n\n\n\nNow let’s do the same thing but searching for the poorest states instead. We will use the slice_min() function to identify the states with the lowest incomes in the first quintile of earners.\n\nbottom_10 &lt;- quintiles |&gt;\n  slice_min(q1, n = 10)\n\nkable(bottom_10)\n\n\n\nname\nq1\nq2\nq3\nq4\nq5\ntop5\n\n\n\nPuerto Rico\n2906\n12144\n22163\n38397\n99043\n187234\n\n\nMississippi\n10096\n27963\n49418\n80125\n175581\n308523\n\n\nLouisiana\n10371\n29781\n53925\n89536\n201514\n357026\n\n\nNew Mexico\n11058\n31274\n54295\n86905\n188282\n323568\n\n\nWest Virginia\n11120\n29606\n51038\n81393\n174019\n299882\n\n\nAlabama\n11602\n31928\n55270\n88640\n193311\n336788\n\n\nKentucky\n11846\n32616\n55838\n88089\n194168\n350411\n\n\nArkansas\n12076\n31051\n52280\n82811\n188510\n344470\n\n\nSouth Carolina\n12680\n34881\n58665\n92118\n207367\n374427\n\n\nDistrict of Columbia\n12971\n51060\n94478\n157803\n375792\n670768\n\n\n\n\n\nOK now let’s make a table with a selection of states that reflects the full range of household incomes. So first, we will use slice_min() and slice_max() without arguments to select the state with the poorest households in q1 and the state with the wealthiest households in top5. The we will use slice_sample() to take a random sample of five additional states.\nWe will store these selections in three objects and then combine them into a single data frame called states using the dplyr function bind_rows. bind_rows() appends data frames with different observations for the same set of columns. You can think of its a kind of “verticle merging” of data frames.\nAfter we have done the append, we can view the new data by calling kable(states).\n\n# lowest \nstate_min &lt;- quintiles |&gt; \n  slice_min(q1) \n\n# highest\nstate_max &lt;- quintiles |&gt; \n  slice_max(top5) \n\n# randomly select five more\nfive_more &lt;- quintiles |&gt;\n   slice_sample(n = 5) \n\nstates &lt;- bind_rows(state_min, state_max, five_more) |&gt;\n  arrange(desc(top5))\n\nkable(states)\n\n\n\nname\nq1\nq2\nq3\nq4\nq5\ntop5\n\n\n\nDistrict of Columbia\n12971\n51060\n94478\n157803\n375792\n670768\n\n\nConnecticut\n17417\n48870\n84042\n133488\n319533\n602707\n\n\nUtah\n21105\n51272\n79517\n116016\n239149\n421379\n\n\nOregon\n16014\n42554\n70418\n108483\n232703\n404468\n\n\nLouisiana\n10371\n29781\n53925\n89536\n201514\n357026\n\n\nNew Mexico\n11058\n31274\n54295\n86905\n188282\n323568\n\n\nPuerto Rico\n2906\n12144\n22163\n38397\n99043\n187234"
  },
  {
    "objectID": "modules/module-4.11.html#display-the-data-with-a-gt-table",
    "href": "modules/module-4.11.html#display-the-data-with-a-gt-table",
    "title": "Module 4.1",
    "section": "Display the data with a gt table",
    "text": "Display the data with a gt table\n\nNow that we have some good data for a table, let’s make a really beautiful table with the gt package. “gt” stands for “grammar of tables.” So just as we talked about a “grammar of graphics” when we were studying plots, we can talk about the “grammar of tables” and break a table down into its component parts.\ngt envisions six main parts of a table that can be customized in various ways. The table header includes the title and possibly a subtitle. Next, we have the stub section that contains our row labels and, above that, a stubhead label, which we could use to provide more information about what is in the rows. Then, we have column labels that tell us about what is in each column and the table body which contains the actual data that we want to present. Finally, we have the table footer, which would contain any notes that we have about information contained in the table as well as information about sources. Check out the gt function reference to get a sense of all the customizations available.\nLet’s go ahead and start out by making a basic gt table with a title, subtitle, column labels, source note and format the numbers as dollar figures.\nMake a good gt table\n\nlibrary(gt)\n\ngoodtable &lt;- gt(states) |&gt; \n  tab_header(\n    title = \"Mean Household Income of Quintiles, 2021\",\n    subtitle = \"Seven Representative U.S. States\"\n  ) |&gt; \n  cols_label(\n    name = \"\",\n    q1 = \"lowest\",\n    q2 = \"second\",\n    q3 = \"third\",\n    q4 = \"fourth\",\n    q5 = \"highest\",\n    top5 = \"top 5%\"\n  ) |&gt; \n  fmt_currency(\n    columns = c(q1:top5),\n    currency = \"USD\", \n    use_subunits = FALSE\n  ) |&gt;\n  # note that you can use markdown (md) to format the source note for html documents\n  tab_source_note(source_note = md(\"**Source**: US Census Bureau, American Community Survey\"))\n\ngoodtable\n\n\n\n\n\n\nMean Household Income of Quintiles, 2021\n\n\nSeven Representative U.S. States\n\n\n\nlowest\nsecond\nthird\nfourth\nhighest\ntop 5%\n\n\n\n\nDistrict of Columbia\n$12,971\n$51,060\n$94,478\n$157,803\n$375,792\n$670,768\n\n\nConnecticut\n$17,417\n$48,870\n$84,042\n$133,488\n$319,533\n$602,707\n\n\nUtah\n$21,105\n$51,272\n$79,517\n$116,016\n$239,149\n$421,379\n\n\nOregon\n$16,014\n$42,554\n$70,418\n$108,483\n$232,703\n$404,468\n\n\nLouisiana\n$10,371\n$29,781\n$53,925\n$89,536\n$201,514\n$357,026\n\n\nNew Mexico\n$11,058\n$31,274\n$54,295\n$86,905\n$188,282\n$323,568\n\n\nPuerto Rico\n$2,906\n$12,144\n$22,163\n$38,397\n$99,043\n$187,234\n\n\n\n\nSource: US Census Bureau, American Community Survey\n\n\n\n\n\nChange column width\nNow let’s add some further customization. One thing to pay attention to is the column width. Too narrow of columns can make it difficult to read the information, while too wide of columns can cause the table not to fit on the page. We can adjust the column width of our table with the cols_width() function.\n\nvgoodtable &lt;- goodtable |&gt;\n  cols_width(c(q1:top5) ~ px(90))\n\nvgoodtable\n\n\n\n\n\n\nMean Household Income of Quintiles, 2021\n\n\nSeven Representative U.S. States\n\n\n\nlowest\nsecond\nthird\nfourth\nhighest\ntop 5%\n\n\n\n\nDistrict of Columbia\n$12,971\n$51,060\n$94,478\n$157,803\n$375,792\n$670,768\n\n\nConnecticut\n$17,417\n$48,870\n$84,042\n$133,488\n$319,533\n$602,707\n\n\nUtah\n$21,105\n$51,272\n$79,517\n$116,016\n$239,149\n$421,379\n\n\nOregon\n$16,014\n$42,554\n$70,418\n$108,483\n$232,703\n$404,468\n\n\nLouisiana\n$10,371\n$29,781\n$53,925\n$89,536\n$201,514\n$357,026\n\n\nNew Mexico\n$11,058\n$31,274\n$54,295\n$86,905\n$188,282\n$323,568\n\n\nPuerto Rico\n$2,906\n$12,144\n$22,163\n$38,397\n$99,043\n$187,234\n\n\n\n\nSource: US Census Bureau, American Community Survey\n\n\n\n\n\nChange font\nNext we want to make sure that the fonts that we use are legible and accessible, just like we did for our charts. We can do this with opt_table_font().\n\ngreattable &lt;- vgoodtable |&gt;\n  opt_table_font(font = \"verdana\")\n\ngreattable\n\n\n\n\n\n\nMean Household Income of Quintiles, 2021\n\n\nSeven Representative U.S. States\n\n\n\nlowest\nsecond\nthird\nfourth\nhighest\ntop 5%\n\n\n\n\nDistrict of Columbia\n$12,971\n$51,060\n$94,478\n$157,803\n$375,792\n$670,768\n\n\nConnecticut\n$17,417\n$48,870\n$84,042\n$133,488\n$319,533\n$602,707\n\n\nUtah\n$21,105\n$51,272\n$79,517\n$116,016\n$239,149\n$421,379\n\n\nOregon\n$16,014\n$42,554\n$70,418\n$108,483\n$232,703\n$404,468\n\n\nLouisiana\n$10,371\n$29,781\n$53,925\n$89,536\n$201,514\n$357,026\n\n\nNew Mexico\n$11,058\n$31,274\n$54,295\n$86,905\n$188,282\n$323,568\n\n\nPuerto Rico\n$2,906\n$12,144\n$22,163\n$38,397\n$99,043\n$187,234\n\n\n\n\nSource: US Census Bureau, American Community Survey\n\n\n\n\n\nCenter\nThen we want to check to make sure that information is aligned properly in the table. We can use right, left or center justify our text, depending on its purpose, by calling cols_align().\n\nvgreattable &lt;- greattable |&gt;\n  cols_align(\n  align = \"center\",\n  columns = q1:top5\n)\n\nvgreattable\n\n\n\n\n\n\nMean Household Income of Quintiles, 2021\n\n\nSeven Representative U.S. States\n\n\n\nlowest\nsecond\nthird\nfourth\nhighest\ntop 5%\n\n\n\n\nDistrict of Columbia\n$12,971\n$51,060\n$94,478\n$157,803\n$375,792\n$670,768\n\n\nConnecticut\n$17,417\n$48,870\n$84,042\n$133,488\n$319,533\n$602,707\n\n\nUtah\n$21,105\n$51,272\n$79,517\n$116,016\n$239,149\n$421,379\n\n\nOregon\n$16,014\n$42,554\n$70,418\n$108,483\n$232,703\n$404,468\n\n\nLouisiana\n$10,371\n$29,781\n$53,925\n$89,536\n$201,514\n$357,026\n\n\nNew Mexico\n$11,058\n$31,274\n$54,295\n$86,905\n$188,282\n$323,568\n\n\nPuerto Rico\n$2,906\n$12,144\n$22,163\n$38,397\n$99,043\n$187,234\n\n\n\n\nSource: US Census Bureau, American Community Survey\n\n\n\n\n\nAdd borders and lines\nFinally, we want to think about how to use borders and lines to separate and identify different elements of the table using tab_options() like this.\n\nawesometable &lt;- vgreattable |&gt;\n  tab_options(\n    table.border.top.color = \"black\", \n    table.border.bottom.color = \"black\",\n    heading.border.bottom.color = \"black\", \n    column_labels.border.bottom.color = \"black\", \n    table_body.border.bottom.color = \"black\"\n  )\n\nawesometable\n\n\n\n\n\n\nMean Household Income of Quintiles, 2021\n\n\nSeven Representative U.S. States\n\n\n\nlowest\nsecond\nthird\nfourth\nhighest\ntop 5%\n\n\n\n\nDistrict of Columbia\n$12,971\n$51,060\n$94,478\n$157,803\n$375,792\n$670,768\n\n\nConnecticut\n$17,417\n$48,870\n$84,042\n$133,488\n$319,533\n$602,707\n\n\nUtah\n$21,105\n$51,272\n$79,517\n$116,016\n$239,149\n$421,379\n\n\nOregon\n$16,014\n$42,554\n$70,418\n$108,483\n$232,703\n$404,468\n\n\nLouisiana\n$10,371\n$29,781\n$53,925\n$89,536\n$201,514\n$357,026\n\n\nNew Mexico\n$11,058\n$31,274\n$54,295\n$86,905\n$188,282\n$323,568\n\n\nPuerto Rico\n$2,906\n$12,144\n$22,163\n$38,397\n$99,043\n$187,234\n\n\n\n\nSource: US Census Bureau, American Community Survey\n\n\n\n\n\nExport your table\nNow try exporting your table as .png file with the gtsave() function.\n\ngtsave(awesometable, \"awesometable.png\")"
  },
  {
    "objectID": "modules/module-4.11.html#when-a-plot-is-better-than-a-table",
    "href": "modules/module-4.11.html#when-a-plot-is-better-than-a-table",
    "title": "Module 4.1",
    "section": "When a plot is better than a table",
    "text": "When a plot is better than a table\n\nBe judicious with your use of tables. You would not want to use tables where a plot is more appropriate. For example you would not want to use a table to show a trend over time (a line chart would be more appropriate) or to display the relationship between two variables (where a scatter plot would be more appropriate).\nAnother case where a table would be less effective than a plot is in showing estimates, margins of error and confidence intervals. Let’s do an example with median income estimates. We can start by searching for “median income” and discover that the code for median income is B06011_001. Let’s use that to extract the median income for counties in the state of Massachusetts.\n\nlibrary(janitor)\n\nmass_med_inc &lt;- get_acs(\n  geography = \"county\", \n  variables = c(median_income = \"B06011_001\"), \n  state = \"MA\", \n  year = 2021\n  ) |&gt;\n  mutate(\n    lower_90 = estimate - moe,\n    upper_90 = estimate + moe \n  ) |&gt;\n  clean_names() |&gt;\n  mutate(name = str_replace_all(name, \" County, Massachusetts\", \"\")) |&gt;\n  select(name, estimate, lower_90, upper_90)\n\nglimpse(mass_med_inc)\n\nRows: 14\nColumns: 4\n$ name     &lt;chr&gt; \"Barnstable\", \"Berkshire\", \"Bristol\", \"Dukes\", \"Essex\", \"Fran…\n$ estimate &lt;dbl&gt; 40442, 33040, 36910, 40119, 39756, 34775, 32262, 30795, 51808…\n$ lower_90 &lt;dbl&gt; 39554, 32074, 36347, 34791, 39174, 33712, 31707, 29868, 51337…\n$ upper_90 &lt;dbl&gt; 41330, 34006, 37473, 45447, 40338, 35838, 32817, 31722, 52279…\n\n\nWe can select the county name, median income estimate and the upper and lower confidence intervals and put those in a table.\n\nkable(mass_med_inc)\n\n\n\nname\nestimate\nlower_90\nupper_90\n\n\n\nBarnstable\n40442\n39554\n41330\n\n\nBerkshire\n33040\n32074\n34006\n\n\nBristol\n36910\n36347\n37473\n\n\nDukes\n40119\n34791\n45447\n\n\nEssex\n39756\n39174\n40338\n\n\nFranklin\n34775\n33712\n35838\n\n\nHampden\n32262\n31707\n32817\n\n\nHampshire\n30795\n29868\n31722\n\n\nMiddlesex\n51808\n51337\n52279\n\n\nNantucket\n45717\n38260\n53174\n\n\nNorfolk\n52591\n51991\n53191\n\n\nPlymouth\n43684\n43014\n44354\n\n\nSuffolk\n39200\n38624\n39776\n\n\nWorcester\n39009\n38454\n39564\n\n\n\n\n\nBut this is not very compelling. So instead, we can plot confidence intervals with ggplot using a combination of geom_errorbar() and geom_point().\n\nlibrary(ggplot2)\n\nmass_med_inc |&gt;\n  ggplot(aes(x = estimate, y = reorder(name, estimate))) +\n  geom_errorbar(aes(xmin = lower_90, xmax = upper_90)) +\n  geom_point(color = \"red\", size = 2) +\n  labs(title = \"Household income by county in Massachusetts\",\n       subtitle = \"2017-2021 American Community Survey\",\n       y = \"\",\n       x = \"Median Income\", \n       caption = \"ACS estimate (bars represent 90% confidence intervals)\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThis conveys a lot more information relative to a table."
  },
  {
    "objectID": "modules/module-6.1.html",
    "href": "modules/module-6.1.html",
    "title": "Module 6.1",
    "section": "",
    "text": "Prework\n\n\n\nClick on Code toggle below to unfold the setup code chunk. Then, copy and run the code in your Quarto notebook to load the necessary packages and create the data frame for this lesson.\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\n    \"v2x_polyarchy\",\n    \"v2x_gender\",\n    \"v2cacamps\",\n    \"v2x_regime\",\n    \"e_regionpol_6C\"\n    ),\n    start_year = 2022, \n    end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    polyarchy = v2x_polyarchy, \n    women_empowerment = v2x_gender,\n    polarization = v2cacamps,\n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~ \"Electoral Democracy\",\n                    3 ~ \"Liberal Democracy\")\n  )\n\n#glimpse(vdem2022)",
    "crumbs": [
      "Course Modules",
      "Module 6.1"
    ]
  },
  {
    "objectID": "modules/module-6.1.html#overview",
    "href": "modules/module-6.1.html#overview",
    "title": "Module 6.1",
    "section": "Overview",
    "text": "Overview\nIn this module, we go beyond the shape of a distribution to describe it more precisely using summary statistics. We begin by exploring measures of central tendency, such as the mean and median, which tell us where the center of a distribution lies. Then we turn to measures of spread, which help us understand how tightly or loosely the values are clustered around that center.\nWe’ll learn how to calculate and interpret the range, interquartile range (IQR), variance, and standard deviation, and see how each tells a different part of the story about our data. Along the way, we’ll visualize these concepts using histograms, box plots, and density plots. We’ll also compare distributions across groups, using summary statistics and ridge plots to uncover patterns that aren’t visible from center alone.\nBy the end of this module, you’ll have a well-rounded set of tools for describing and comparing continuous variables — and a better understanding of when the mean can be misleading, and why spread matters just as much as center.",
    "crumbs": [
      "Course Modules",
      "Module 6.1"
    ]
  },
  {
    "objectID": "modules/module-6.1.html#measures-of-central-tendency",
    "href": "modules/module-6.1.html#measures-of-central-tendency",
    "title": "Module 6.1",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\nWhen we work with continuous variables, one of our first goals is to describe the center of the data — a typical or representative value that captures where most observations tend to fall. This is what we mean by measures of central tendency.\nThe two most common measures are the mean and the median:\n\nThe mean is the arithmetic average: add up all the values and divide by the number of observations.\nThe median is the middle value: half the observations fall below it, and half above.\n\nWe already know how to calculate both of these using the summarize() function from the dplyr package. Here is an example using the data frame that we created in the setup chunk:\n\nvdem2022 |&gt; \n  summarize(\n    mean_polarization = mean(polarization, na.rm = TRUE),\n    median_polarization = median(polarization, na.rm = TRUE)\n  )\n\n  mean_polarization median_polarization\n1         0.2347416              0.2265\n\n\nIn some datasets, the mean and median will be very close. But in others — particularly those with skewed distributions — they can diverge. The mean is pulled in the direction of extreme values, while the median resists the influence of outliers.\nThat’s why the mean, though widely used, is not always the most informative measure of central tendency. Its usefulness depends on the shape of the distribution. In a symmetric distribution, the mean and median will align. But in a skewed distribution — like GDP per capita, where a few wealthy countries drive up the average — the median may offer a better sense of the “typical” case.\n\n\n\n\n\n\nWhat About the Mode?\n\n\n\nThe mode is another measure of central tendency — it refers to the most frequently occurring value in a dataset. While it can be useful for categorical or discrete data (e.g., identifying the most common regime type or income bracket), it is rarely used with continuous variables. That’s because continuous data often don’t have exact repeat values, especially when measured with precision (e.g., GDP per capita like $10,542.87).\n\n\nLet’s look at two examples: one symmetric, one skewed. We’ll overlay vertical lines for the mean and median so you can see how they behave in each case.\n\n\n\n\n\n\n\n\nIn a symmetric distribution, the mean and median are nearly identical. In this example, both are around 50. This is why the mean is often a reliable summary when data are normally distributed.\n\n\n\n\n\n\n\n\nIn the right-skewed case, we can see that the mean is pulled to the right by a few large values, while the median stays closer to the bulk of the data. This is why the median can sometimes offer a more realistic picture of central tendency, especially when distributions are skewed.\nThe lesson here is to always look at how your data are distributed before analyzing them. When reading or in a presentation, you should ask yourself whether the mean make sense given the distribution of the measure. Could extreme values in a skewed distribution make the mean not as useful? Have the analysts shown you the distribution? If not, ask about it!\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nCalculate the mean and median for the women_empowerment variable in the vdem2022 data frame.\nNow overlay the mean and median on a density plot of women_empowerment using geom_vline().\nWhat do you notice about the relationship between the mean and median in this case?\nTry the same visualization with the polarization variable. Are the mean and median for polarization close together or far apart? How does this distance compare to the women_empowerment variable?",
    "crumbs": [
      "Course Modules",
      "Module 6.1"
    ]
  },
  {
    "objectID": "modules/module-6.1.html#measures-of-dispersion",
    "href": "modules/module-6.1.html#measures-of-dispersion",
    "title": "Module 6.1",
    "section": "Measures of Dispersion",
    "text": "Measures of Dispersion\nWe have seen how an important thing that we want to know about our data is how much variability there is, e.g. how tightly or loosely the values are clustered around the center. This variability is often referred to as the spread of the distribution.\nWhy should we be concerned with spread? Let’s start by comparing two distributions. Both have the same mean — zero — but one has values tightly clustered around that mean, while the other spreads out much more broadly.\n\n\n\n\n\n\n\n\nEven without doing any math, we can see that the second distribution is more dispersed. But to describe and compare distributions in a consistent way, we need to summarize that spread with numbers.\nRange: A Starting Point\nOne of the simplest ways to measure spread is the range, which is just the difference between the minimum and maximum values.\n\nvdem2022 |&gt;\n  summarize(min = min(polarization, na.rm = TRUE),\n            max = max(polarization, na.rm = TRUE))\n\n     min   max\n1 -3.115 3.538\n\n\nWhile easy to calculate, the range only tells us about the extremes. It doesn’t tell us where most values lie, and it’s highly sensitive to outliers. We need something more robust.\nThe Interquartile Range (IQR)\nA more useful measure of spread is the interquartile range (IQR) — the range of the middle 50% of the data. It spans from the 25th percentile (Q1) to the 75th percentile (Q3).\n\n\n\n\n\n\n\n\nThis gives us a much better sense of where the “typical” values are. In this example, most countries had political polarization scores between -0.73 and 1.15 in 2022.\n\nvdem2022 |&gt;\n  summarize(\n    IQRlow =  quantile(polarization, .25, na.rm = TRUE),\n    IQRhigh = quantile(polarization, .75, na.rm = TRUE),\n    IQRlength = IQR(polarization, na.rm = TRUE)\n  )\n\n   IQRlow IQRhigh IQRlength\n1 -0.7265 1.14975   1.87625\n\n\nVisualizing the IQR with a Box Plot\nThe box plot is a powerful tool for visually summarizing the spread of a distribution. It provides a standardized way to display key features of a dataset using what’s known as the five-number summary: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. In R we can make a boxplot with the geom_boxplot() function from the ggplot2 package.\n\nggplot(vdem2022, aes(x = \"\", y = polarization)) +\n  geom_boxplot(fill = \"steelblue\") + \n   labs(\n    x = \"\", \n    y = \"Polarization\", \n    title = \"Distribution of Political Polarization Scores, 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nAt its core, a box plot helps us quickly see where most values lie and how they are distributed across the range. The “box” itself shows the interquartile range — the middle 50% of the data — while the line inside the box marks the median. The “whiskers” extend to the smallest and largest values that fall within 1.5 times the interquartile range, and any values beyond that are plotted individually as potential outliers.\nThis makes box plots especially useful for comparing distributions across different groups. They allow us to see not just differences in center (like the median), but also differences in spread, skewness, and the presence of outliers.\nStandard Deviation: The Classic Measure of Spread\nThe standard deviation is a widely used summary of spread. It tells us, on average, how far each observation lies from the mean. A small standard deviation means values are tightly clustered; a large one means they are more spread out. Here is an example of how to calculate the mean and standard deviation for the polarization variable in the vdem2022 data frame:\n\nvdem2022 |&gt;\n  summarize(mean = mean(polarization, na.rm = TRUE),\n            stdDev = sd(polarization, na.rm = TRUE))\n\n       mean   stdDev\n1 0.2347416 1.408674\n\n\nStandard deviation is derived from the variance, which is the average of the squared deviations from the mean. The standard deviation is simply the square root of the variance — bringing the result back to the original scale of the data.\nUnder the hood, R is calculating the standard deviation with the following formula:\n\\[\ns = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\n\\]\nA Step-by-Step Breakdown of Standard Deviation\nTo better understand how standard deviation works, let’s walk through a toy example by breaking down the formula. We’ll follow the standard formula for sample standard deviation:\n\\[\ns = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\n\\]\nThis formula tells us to:\n\nSubtract the mean from each value (to get deviations),\nSquare those deviations,\nSum them,\nDivide by \\(n - 1\\),\nAnd take the square root.\n\nLet’s try this using a simple vector of evenly spaced numbers from 0 to 10.\n\n\n\n\n\n\nImportant\n\n\n\nPlay the interactive code chunks below to see how each step works. You can also change the numbers in the initial vector to see how the standard deviation changes with different data.\n\n\nFirst, we create the vector. Next, we calculate the mean (\\(\\bar{X}\\)) of the dataset and subtract it from each data point (\\(X_i\\)) to calculate its deviation from the mean: \\(e_i = X_i - \\bar{X}\\). We store that vector of deviations in a new variable called e:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow we square each deviation: \\(e_i^2 = (X_i - \\bar{X})^2\\). This removes negative signs and prepares the values for averaging:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNext, we sum up all of the squared deviations: \\(\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\). This represents the total squared deviation from the mean or the sum of squares.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nDivide the total squared deviation by \\((n-1)\\) get the sample variance: \\(\\text{Variance} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\). Using \\((n-1)\\) ensures an unbiased estimate of the population variance when calculating from a sample.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFinally, we take the square root of the variance to get the standard deviation: \\(s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\). Taking the square root converts the variance back to the units of the original data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow let’s compare our manual calculation with R’s built-in sd() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nCalculate the range and interquartile range (IQR) for the women_empowerment variable in the vdem2022 data frame.\nCreate a box plot for women_empowerment and overlay the mean and median.\nCalculate the mean and standard deviation for women_empowerment and polarization in the vdem2022 data frame using R’s built-in sd() function.\nCompare your results with what we found for the polarization variable earlier in this module. What do you notice about the spread of women_empowerment compared to polarization?",
    "crumbs": [
      "Course Modules",
      "Module 6.1"
    ]
  },
  {
    "objectID": "modules/module-2.2.html",
    "href": "modules/module-2.2.html",
    "title": "Module 2.2",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the devtools package. Type install.packages(\"devtools\") in your console. You will need this to install the vdemdata package because it is not on the CRAN Network.\nInstall the vdemdata package from GitHub. Type devtools::install_github(\"vdeminstitute/vdemdata\") in your console.\n\nIn previous lessons, we have loaded the Tidyverse packages individually. However, it is also possible to load them by simply loading the tidyverse package. Let’s try doing it that way this time.\n\nlibrary(tidyverse)\n\nNow we have all of the tidyverse packages we need including readr, dplyr, ggplot2 and scales.\nWe are also going to be using the vdemdata package to download data from the Varieties of Democracy project. We will load that package a little later, but be sure you have it installed before proceeding.\nIn this module we are going to learn how to make line charts and scatter plots. We will also learn how to add a additional dimensions to our visualizations by using color, size and linetype. For scatter plots, we will learn how to “decompose” our data by using facets. Finally, we will learn how to change the scale of our axes and how to label our points.",
    "crumbs": [
      "Course Modules",
      "Module 2.2"
    ]
  },
  {
    "objectID": "modules/module-2.2.html#line-charts",
    "href": "modules/module-2.2.html#line-charts",
    "title": "Module 2.2",
    "section": "Line charts",
    "text": "Line charts\n\nNow let’s create a line chart. Line charts are usually the best option when we want to illustrate trends in our data. For this visualization, we will try to illustrate Samuel Huntington’s waves of democracy by showing how countries representing each of the three waves. The U.S. represents the first wave, Japan the second wave starting with the allied victory in WWII, and Portugal represents the first country to transition in the third wave.\nFirst, let’s grab the relevant data using vdemdata and dplyr. We are going to be downloading the polyarchy measure for the U.S., Japan and Portugal as far back as the data are available. So first we will select country name, year and the polyarchy schore and then we will filter the data based on the three country names. We are saving these data in an object called dem_waves_ctrs.\n\nlibrary(vdemdata)\n\ndem_waves_ctrs &lt;- vdem |&gt;\n  select(\n    country = country_name,     \n    year, \n    polyarchy = v2x_polyarchy, \n  ) |&gt;\n  filter( \n    country %in% c(\"United States of America\", # select countries in this list\n                   \"Japan\", \n                   \"Portugal\")\n    )\n\nwrite_csv(dem_waves_ctrs, \"data/dem_waves_ctrs.csv\")\n\nNext, we are going to do our ggplot() call. The data will be the dem_waves_ctrs object that we just created. You can should have the data saved but you can also download it here. For the aesthetics mapping, we will put the year on the x-axis and the polyarchy score on the y-axis. We will also specify color in the aes() call so that we can color the lines by region.\nTo get a line chart, we have to specify geom_line(). Then within the geom_line() function we will set the linewidth equal to `1’ so that the lines are a bit more visible.\nFinally, we will add a labs() call as with the previous visualizations. But in addition to title, axis labels and a caption, we will also add color = \"Country\" to change the label of the legend to “Country” with a capital “C.”\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )",
    "crumbs": [
      "Course Modules",
      "Module 2.2"
    ]
  },
  {
    "objectID": "modules/module-2.2.html#scatter-plots",
    "href": "modules/module-2.2.html#scatter-plots",
    "title": "Module 2.2",
    "section": "Scatter plots",
    "text": "Scatter plots\n\nThe last thing we are going to do in this lesson is to create a scatter plot. We use scatter plots in order to illustrate how two variables relate to each other (or not). In this example, we are going to illustrating modernization theory, which predicts a positive relationship between wealth and democracy, while also incorporating levels of women’s representation into our analysis.\nWe are going to start with the dem_women.csv file that you can find here. We will then group the data by country and calculate the mean for each variable. Note that in the group_by() call we also include region because we will want to keep it so that we can color our points by region.\n\ndem_summary_ctry &lt;- read_csv(\"data/dem_women.csv\") |&gt;\n  group_by(country, region) |&gt; # group by country, keep region\n  summarize(\n    polyarchy = mean(polyarchy, na.rm = TRUE),\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    flfp = mean(flfp, na.rm = TRUE), \n    women_rep = mean(women_rep, na.rm = TRUE)\n  )\n\nNow let’s create our first scatter plot. Our ggplot() call looks similar to previous ones except for a few things. First we are calling geom_point() for our geom. But also notice that our aesthetics mapping includes four dimenstions: x, y, color and size. So here we are telling ggplot2 that we want wealth on the x-axis, the polyarchy score on the y-axis, to color the points based on region, and to vary the size of the points in relation to the level of women’s representation.\nOne last thing we want to do is to put our x-axis on a log scale and change the labels to reflect their dollar values. For the log scale, we can use the scale_x_log10() function and for the labels we can use the label_number() function from the scales package. We set the prefix to “$” and the suffix to “k” so that each number on the x-axis starts with a dollar sign and ends with “k” denoting “thousands.”\n\n\n\n\n\n\nNote\n\n\n\nWe will encounter other useful scales functions including label_dollar() and label_percent() in future lessons.\nNotice that in this example we introduce the scales package by including it as a prefix to the label_number() function, e.g. scales::label_number(prefix = \"$\", suffix = \"k\"). This allows us to use the package without having to load it, e.g. library(scales). It also has the benefit of generating a list of auto-complete suggestions for the many available functions in the scales package.\n\n\n\n# in this ggplot() call we have four dimensions\n# x, y, color, and size\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )\n\n\n\n\n\n\n\nThe plot does a good job of illustrating the basic point of modernization theory in that we do see the positive correlation between wealth and democracy. But we also see that there are some outliers and that a lot of the outlier countries are concentrated in the Middle East.\nWe also see that the distribution of women’s representation is somewhat orthogonal to wealth and democracy. Most wealthy western countries have high levels of women’s representation, but so do a lot of low- and middle-income countries in Africa, Asia and Latin America.\nAdding a trend line\nWe can definitely see a relationship between wealth and democracy in the scatter plot, but how strong is it? One way to find out is to add a trend line. Let’s do this by adding another geom, geom_smooth(), and specifying a linear model with the argument method = \"lm\" We acn also set the linewidth of the trend line to 1 so that the line is more visible.\nIf we want to add a single trend while also maintaining the coloring by region, then we have to reconfigure the ggplot() call a bit. Specifically, we will want to move color = region to a separate aes() call in the geom_point() function, e.g. geom_point(aes(color = region)). If we don’t do this we will get separate trend lines for each region (try it and see!).\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    )\n\n\n\n\n\n\n\nFacet wrapping\nNow let’s imagine that we really interested in drilling down into the “heterogeneous effects” of wealth on democracy by region. In other words, we want to see more clearly how wealth is related to democracy in some regions but not others. For this, we can use facet_wrap() to get a separate chart for each region rather than just shading the points by region. Inside of facet_wrap() we identify region as the variable that we want to use to separate the plots, e.g. facet_wrap(~region). Notice how we have to include a tilde (~) here.\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\nHere we can clearly see a relationship between wealth and democracy in all of the countries except for the Middle East and Africa. We could speculate that the lack of a relationship in the Middle East could be evidence of an oil curse dynamic whereas perhaps the lack of a relationship in Africa is due to weak institutions.\nThe relationship between wealth and democracy in the West would be apparent, but it is obscured by the fact that western countries because the high wealth and polyarchy values result in extreme bunching in the northwest quadrant of the graph. To deal with this issue, we could add the scales = \"free\" argument to our plot.\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region, scales = \"free\") +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\nBut notice there is a bit of a tradeoff here. With the scales = free option set, we now have separate axes for each of the plots. This is less of a clean look than having common x and y axes.\nLabeling points\nNow let’s try drilling down into one of the regions to get a better sense of what countries are driving the relationship. To do this, we can filter our data set for a region that we are interested in and then add country labels to the points in the scatter plot. Here we are going to filter for “Asia” and we will ad a geom_text() call to add country labels. In the geom_text() call we include arguments for size and vjust to adjust the size and vertical location of the labels relative to the points.\n\ndem_summary_ctry |&gt; \n  filter(region == \"Asia\") |&gt;\n  ggplot(aes(x = gdp_pc, y = polyarchy)) + \n    geom_point() + \n    geom_text(aes(label = country), size = 2, vjust = 2) +\n    geom_smooth(method = \"lm\", linewidth = 1) +\n    scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n      labs(\n        x= \"GDP Per Capita\", \n        y = \"Polyarchy Score\",\n        title = \"Wealth and democracy in Asia, 1990 - present\", \n        caption = \"Source: V-Dem Institute\"\n        )",
    "crumbs": [
      "Course Modules",
      "Module 2.2"
    ]
  },
  {
    "objectID": "modules/module-12.1.html",
    "href": "modules/module-12.1.html",
    "title": "Module 12.1",
    "section": "",
    "text": "Building on our bivariate interpretation skills, this module extends logistic regression to realistic research scenarios involving multiple predictors and interaction effects. Real-world research rarely involves simple bivariate relationships. Instead, we need to control for multiple confounding variables and often discover that the effect of one variable depends on the level of another variable.\nIn this module, we will explore how controlling for additional variables changes our interpretation of individual predictors, understand when and why to include interaction terms in our models, learn to interpret complex conditional relationships step-by-step, and master both manual calculation and automated tools for understanding interactions. We will also discover how to visualize complex relationships using predicted probabilities and recognize when continuous versus dichotomous measures affect our analytical conclusions.\nBy the end of this module, you’ll understand how multiple predictors work together in logistic regression, be able to interpret interaction effects manually and using specialized R tools, know how to visualize complex conditional relationships, and recognize the theoretical and practical considerations that guide interaction modeling decisions.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#overview",
    "href": "modules/module-12.1.html#overview",
    "title": "Module 12.1",
    "section": "",
    "text": "Building on our bivariate interpretation skills, this module extends logistic regression to realistic research scenarios involving multiple predictors and interaction effects. Real-world research rarely involves simple bivariate relationships. Instead, we need to control for multiple confounding variables and often discover that the effect of one variable depends on the level of another variable.\nIn this module, we will explore how controlling for additional variables changes our interpretation of individual predictors, understand when and why to include interaction terms in our models, learn to interpret complex conditional relationships step-by-step, and master both manual calculation and automated tools for understanding interactions. We will also discover how to visualize complex relationships using predicted probabilities and recognize when continuous versus dichotomous measures affect our analytical conclusions.\nBy the end of this module, you’ll understand how multiple predictors work together in logistic regression, be able to interpret interaction effects manually and using specialized R tools, know how to visualize complex conditional relationships, and recognize the theoretical and practical considerations that guide interaction modeling decisions.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#models-with-multiple-predictors",
    "href": "modules/module-12.1.html#models-with-multiple-predictors",
    "title": "Module 12.1",
    "section": "Models with Multiple Predictors",
    "text": "Models with Multiple Predictors\nIn our previous modules, we focused on bivariate relationships to build solid interpretive foundations. However, real research requires us to control for multiple potential confounding variables simultaneously. One of the most influential examples of multiple predictor logistic regression in political science comes from Fearon and Laitin’s (2003) groundbreaking study of civil war onset.\nFearon and Laitin argued that civil wars are not primarily caused by ethnic or religious grievances, but rather by conditions that favor insurgency: weak state capacity, difficult terrain, and economic underdevelopment. Their comprehensive model included multiple predictors to control for alternative explanations while testing their core theoretical arguments.\nHere we will loosely replicate their approach using the peacesciencer package and examine how our interpretation changes when we move from bivariate to multiple predictor models. First we will use the peacesciencer package to create a dataset similar to Fearon and Laitin’s. Notice how you can add multiple predictors using the add_* functions, which automatically handle data cleaning and merging. Notice also that we are creating a dichotomous democracy measure (democracy) based on the continuous V-Dem v2x_polyarchy score, which we will use in our interaction models later.\n\nlibrary(peacesciencer)\nlibrary(tidyverse)\n\n# Recreate the conflict dataset, adding a dichotomous democracy measure\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain() |&gt;\n  mutate(democracy = ifelse(v2x_polyarchy &gt; 0.5, 1, 0)) |&gt; # binary democracy measure\n  select(-ucdpongoing, -maxintensity, -conflict_ids, -sdpest) |&gt;\n  drop_na() # need to drop NAs for logistic regression\n\nglimpse(conflict_df)\n\nRows: 5,964\nColumns: 19\n$ gwcode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ gw_name        &lt;chr&gt; \"United States of America\", \"United States of America\",…\n$ microstate     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ year           &lt;dbl&gt; 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1…\n$ ucdponset      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ euds           &lt;dbl&gt; 1.293985, 1.308359, 1.343539, 1.330836, 1.354015, 1.350…\n$ aeuds          &lt;dbl&gt; 0.4862558, 0.5006298, 0.5358093, 0.5231064, 0.5462858, …\n$ polity2        &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…\n$ v2x_polyarchy  &lt;dbl&gt; 0.603, 0.607, 0.599, 0.580, 0.585, 0.611, 0.611, 0.612,…\n$ ethfrac        &lt;dbl&gt; 0.2226323, 0.2248701, 0.2271561, 0.2294918, 0.2318781, …\n$ ethpol         &lt;dbl&gt; 0.4152487, 0.4186156, 0.4220368, 0.4255134, 0.4290458, …\n$ relfrac        &lt;dbl&gt; 0.4980802, 0.5009111, 0.5037278, 0.5065309, 0.5093204, …\n$ relpol         &lt;dbl&gt; 0.7769888, 0.7770017, 0.7770303, 0.7770729, 0.7771274, …\n$ wbgdp2011est   &lt;dbl&gt; 28.539, 28.519, 28.545, 28.534, 28.572, 28.635, 28.669,…\n$ wbpopest       &lt;dbl&gt; 18.744, 18.756, 18.781, 18.804, 18.821, 18.832, 18.848,…\n$ wbgdppc2011est &lt;dbl&gt; 9.794, 9.762, 9.764, 9.730, 9.752, 9.803, 9.821, 9.857,…\n$ rugged         &lt;dbl&gt; 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073,…\n$ newlmtnest     &lt;dbl&gt; 3.214868, 3.214868, 3.214868, 3.214868, 3.214868, 3.214…\n$ democracy      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nLet’s now start out with a baseline model that includes multiple predictors. This model will allow us to see how controlling for other factors changes our interpretation of the relationship between democracy and conflict onset.\n\n# Fit the multiple predictor baseline model\nbaseline_model &lt;- glm(ucdponset ~ v2x_polyarchy + newlmtnest + wbpopest + \n                     wbgdppc2011est + ethfrac + relfrac,\n                     data = conflict_df,\n                     family = \"binomial\")\n\n# Display the model summary\nsummary(baseline_model)\n\n\nCall:\nglm(formula = ucdponset ~ v2x_polyarchy + newlmtnest + wbpopest + \n    wbgdppc2011est + ethfrac + relfrac, family = \"binomial\", \n    data = conflict_df)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -5.22286    1.48750  -3.511 0.000446 ***\nv2x_polyarchy  -0.59263    0.53326  -1.111 0.266422    \nnewlmtnest      0.14488    0.07219   2.007 0.044739 *  \nwbpopest        0.27454    0.07489   3.666 0.000246 ***\nwbgdppc2011est -0.41839    0.12675  -3.301 0.000963 ***\nethfrac         0.65051    0.37871   1.718 0.085853 .  \nrelfrac        -0.23301    0.42125  -0.553 0.580160    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1167.2  on 5963  degrees of freedom\nResidual deviance: 1105.3  on 5957  degrees of freedom\nAIC: 1119.3\n\nNumber of Fisher Scoring iterations: 7\n\n\nThis baseline model reveals several important patterns. First, notice how some variables that might have been significant in bivariate models may lose significance when we control for other factors. This is a classic result in multiple regression: apparent relationships can disappear when we account for confounding variables.\nThe model tells us that population size (wbpopest) and GDP per capita (wbgdppc2011est) remain significant predictors even after controlling for other factors. Ethnic fractionalization (ethfrac) shows some evidence of increasing conflict risk, while the continuous democracy measure (v2x_polyarchy) is not statistically significant in this specification.\nThe key insight for interpretation is that each coefficient now represents the effect of that variable holding all other variables constant. When we say that the coefficient for GDP per capita is -0.365, we mean that a one-unit increase in log GDP per capita decreases the log-odds of conflict onset by 0.365 (or about 31% when we exponentiate the coefficient) controlling for democracy levels, terrain, population, ethnic fractionalization, and religious fractionalization*.\n\n\n\n\n\n\nYour Turn!\n\n\n\nThe peacesciencer package provides multiple different measures of democracy, terrain and ethnic and religious divisions. Try running the baseline model using different combinations of these variables. How do the coefficients and significance levels change? What does this tell you about the importance of controlling for confounding variables in regression analysis?",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#why-interaction-effects-matter-in-conflict-research",
    "href": "modules/module-12.1.html#why-interaction-effects-matter-in-conflict-research",
    "title": "Module 12.1",
    "section": "Why Interaction Effects Matter in Conflict Research",
    "text": "Why Interaction Effects Matter in Conflict Research\n\nThe baseline model assumes that the effect of each predictor is the same regardless of the values of other predictors. This is called an additive model because the effects simply add up. But what if the effect of one variable depends on the level of another variable? What if mountainous terrain only increases conflict risk in non-democratic countries? What if economic development has a stronger peace-promoting effect in democratic societies?\nThese are questions about interaction effects, where the impact of one variable is conditional on the value of another variable. In conflict research, there are compelling theoretical reasons to expect such interactions. Democracy might provide institutional mechanisms for resolving grievances that would otherwise lead to violence. Democratic institutions might also change how other risk factors operate.\nConsider two theoretical scenarios:\nDemocracy and Terrain: In non-democratic countries, mountainous terrain might facilitate insurgency because remote populations have no voice in government and rebels can hide in difficult terrain. In democratic countries, these same remote populations might have political representation and peaceful means of addressing grievances, reducing the conflict-promoting effect of terrain.\nDemocracy and Economic Development: Economic growth might reduce conflict risk in all countries, but this effect might be amplified in democracies where the benefits of growth are more widely shared and people have more say about how their tax dollars are spent.\nTesting these theoretical expectations requires interaction terms in our logistic regression models. An interaction term allows the effect of one variable to vary depending on the level of another variable.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#interaction-effect-1-democracy-and-mountainous-terrain",
    "href": "modules/module-12.1.html#interaction-effect-1-democracy-and-mountainous-terrain",
    "title": "Module 12.1",
    "section": "Interaction Effect #1: Democracy and Mountainous Terrain",
    "text": "Interaction Effect #1: Democracy and Mountainous Terrain\nLet’s start with the democracy-terrain interaction using our dichotomous democracy measure. This provides an excellent opportunity to understand interaction effects through manual calculation before moving to more complex tools.\n\n# Fit model with democracy-terrain interaction\nterrain_interaction_model &lt;- glm(ucdponset ~ democracy * newlmtnest + wbpopest + \n                                wbgdppc2011est + ethfrac + relfrac,\n                                data = conflict_df,\n                                family = \"binomial\")\n\n# Display the model summary\nsummary(terrain_interaction_model)\n\n\nCall:\nglm(formula = ucdponset ~ democracy * newlmtnest + wbpopest + \n    wbgdppc2011est + ethfrac + relfrac, family = \"binomial\", \n    data = conflict_df)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -6.33596    1.51652  -4.178 2.94e-05 ***\ndemocracy            -0.10491    0.52646  -0.199 0.842046    \nnewlmtnest            0.18978    0.07724   2.457 0.014013 *  \nwbpopest              0.28259    0.07554   3.741 0.000183 ***\nwbgdppc2011est       -0.31535    0.12312  -2.561 0.010427 *  \nethfrac               0.60149    0.37820   1.590 0.111739    \nrelfrac              -0.15443    0.41842  -0.369 0.712068    \ndemocracy:newlmtnest -0.41668    0.22677  -1.837 0.066141 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1167.2  on 5963  degrees of freedom\nResidual deviance: 1095.4  on 5956  degrees of freedom\nAIC: 1111.4\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe interaction term democracy:newlmtnest has a coefficient of -0.415 and is statistically significant (p = 0.041). But what does this mean substantively?\nWith a dichotomous interaction variable like our democracy measure, we can calculate the conditional effects manually. The model equation is:\n\\[\\text{logit}(p) = \\beta_0 + \\beta_1(\\text{democracy}) + \\beta_2(\\text{terrain}) + \\beta_3(\\text{democracy} \\times \\text{terrain}) + \\text{other controls}\\]\nThis gives us two different equations depending on the democracy level:\nFor non-democratic countries (democracy = 0) the democracy coefficient gets zeroed out, the interaction term drops out and the model simplifies to:\n\\[\\text{logit}(p) = \\beta_0 + \\beta_2(\\text{terrain}) + \\text{other controls}\\]\nIn other words, the effect of terrain is simply the value of the terrain coefficient \\(\\beta_2 = 0.187\\).\nFor democratic countries (democracy = 1) we include the democracy coefficient and the interaction term (1 + terrain):\n\\[\\text{logit}(p) = \\beta_0 + \\beta_1(1) + \\beta_2(\\text{terrain}) + \\beta_3(\\text{democracy} \\times \\text{terrain}) + \\text{other controls}\\] This simplifies to:\n\\[\\text{logit}(p) = \\beta_0 + \\beta_1 + (\\beta_2 + \\beta_3)(\\text{terrain}) + \\text{other controls}\\]\nSo then the marginal effect of terrain is \\(\\beta_2 + \\beta_3 = 0.187 + (-0.415) = -0.228\\).\nHere is how it would look if we did that with R code.\n\nlibrary(marginaleffects)\nlibrary(broom)\n\nconditional_effects_terrain &lt;- slopes(terrain_interaction_model, \n                                      variables = \"newlmtnest\",\n                                      by = \"democracy\",\n                                      type = \"link\") |&gt;\n  tidy() |&gt; # convert to tidy format\n  mutate(odds.ratio = exp(estimate)) |&gt; # add column with odds ratios\n  select(\n    term:std.error, \n    conf.low:odds.ratio\n  )\n\nconditional_effects_terrain\n\n# A tibble: 2 × 8\n  term       contrast democracy estimate std.error conf.low conf.high odds.ratio\n  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 newlmtnest dY/dX            0    0.190    0.0772   0.0384     0.341      1.21 \n2 newlmtnest dY/dX            1   -0.227    0.215   -0.648      0.194      0.797\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nThe slopes() function in marginaleffects can calculate marginal effects on different scales. By default, it uses type = \"response\", which gives us marginal effects on the probability scale that show how much the probability of conflict changes per unit increase in terrain. These effects are typically very small because probabilities are constrained between 0 and 1.\nWhen we specify type = \"link\", we get marginal effects on the log-odds scale instead. This matches the scale of our model coefficients and our manual calculations above. Using type = “link” is particularly helpful for understanding interactions because it directly corresponds to the coefficients we see in our model summary, making it easier to connect the automated results to the underlying mathematical relationships.\nAfter we run the slopes() function, we use tidy() to convert the results into a tidy format that is easier to read and manipulate. We then calculate the odds ratios by exponentiating the estimates, which allows us to interpret the effects in terms of odds rather than log-odds. Then we select only the relevant columns for clarity.\n\n\nThese results reveal a fascinating pattern! In non-democratic countries, mountainous terrain increases the odds of conflict onset by about 21% for each unit increase in terrain roughness (odds ratio = 1.206). However, in democratic countries, mountainous terrain actually decreases the odds of conflict onset by about 20% for each unit increase (odds ratio = 0.796).\nThis interaction suggests that democracy fundamentally changes how terrain affects conflict risk. In authoritarian systems, difficult terrain may indeed facilitate insurgency by providing rebels with safe havens and making government control difficult. In democratic systems, perhaps the same geographical isolation is less problematic because remote populations have political voice and peaceful means of addressing grievances.\n\n\n\n\n\n\nYour Turn!\n\n\n\nNow practice interpreting interaction effects by examining the democracy main effect:\n\nCalculate the effect of democracy when terrain = 0 using the model coefficients\nCalculate the effect of democracy when terrain = 1 (one standard deviation above mean)\nConvert both to odds ratios and interpret the results\nWhat does this tell us about when democracy has stronger or weaker effects on conflict risk?\n\nRemember: the interaction works both ways! If terrain’s effect depends on democracy, then democracy’s effect also depends on terrain.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#interaction-effect-2-democracy-and-economic-development",
    "href": "modules/module-12.1.html#interaction-effect-2-democracy-and-economic-development",
    "title": "Module 12.1",
    "section": "Interaction Effect #2: Democracy and Economic Development",
    "text": "Interaction Effect #2: Democracy and Economic Development\nOur second interaction examines how democracy moderates the relationship between economic development and conflict risk. For this analysis, we’ll use the continuous democracy measure (v2x_polyarchy) to illustrate why we need more sophisticated tools when dealing with continuous interaction variables.\n\n# Fit model with continuous democracy-wealth interaction\nwealth_interaction_model &lt;- glm(ucdponset ~ v2x_polyarchy * wbgdppc2011est + newlmtnest + \n                               wbpopest + ethfrac + relfrac,\n                               data = conflict_df,\n                               family = \"binomial\")\n\n# Display the model summary\nsummary(wealth_interaction_model)\n\n\nCall:\nglm(formula = ucdponset ~ v2x_polyarchy * wbgdppc2011est + newlmtnest + \n    wbpopest + ethfrac + relfrac, family = \"binomial\", data = conflict_df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  -8.03235    1.90548  -4.215 2.49e-05 ***\nv2x_polyarchy                 8.92762    4.09824   2.178 0.029376 *  \nwbgdppc2011est               -0.11530    0.17640  -0.654 0.513349    \nnewlmtnest                    0.17653    0.07425   2.377 0.017437 *  \nwbpopest                      0.28365    0.07485   3.790 0.000151 ***\nethfrac                       0.59923    0.37436   1.601 0.109444    \nrelfrac                      -0.07663    0.42745  -0.179 0.857727    \nv2x_polyarchy:wbgdppc2011est -1.08638    0.46974  -2.313 0.020739 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1167.2  on 5963  degrees of freedom\nResidual deviance: 1099.6  on 5956  degrees of freedom\nAIC: 1115.6\n\nNumber of Fisher Scoring iterations: 8\n\n\nThe interaction term v2x_polyarchy:wbgdppc2011est has a coefficient of -1.130 and is statistically significant (p = 0.015). This negative coefficient suggests that democracy amplifies the conflict-reducing effect of economic development. But with a continuous moderator variable, we can’t simply calculate “democracy = 0” and “democracy = 1” effects as we did before.\nThis is where the marginaleffects package becomes invaluable. We can specify at = list() to calculate the conditional effect of economic development at different levels of democracy, taking into account the continuous nature of our democracy measure:\n\nconditional_effects_wealth &lt;- slopes(wealth_interaction_model, \n                       variables = \"wbgdppc2011est\",\n                       newdata = datagrid(v2x_polyarchy = c(0.1, 0.3, 0.5, 0.7, 0.9)),\n                       type = \"link\") |&gt;  \n  tidy() |&gt; \n  mutate(odds.ratio = exp(estimate)) |&gt; \n  select(v2x_polyarchy, estimate, std.error, conf.low, conf.high, odds.ratio)\n\n\nconditional_effects_wealth\n\n# A tibble: 5 × 6\n  v2x_polyarchy estimate std.error conf.low conf.high odds.ratio\n          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1           0.1   -0.224     0.146   -0.510    0.0616      0.799\n2           0.3   -0.441     0.122   -0.680   -0.202       0.643\n3           0.5   -0.658     0.162   -0.975   -0.342       0.518\n4           0.7   -0.876     0.235   -1.34    -0.416       0.417\n5           0.9   -1.09      0.319   -1.72    -0.468       0.335\n\n\nThese results reveal how democracy progressively amplifies the peace-promoting effects of economic development. In highly authoritarian countries (polyarchy = 0.1), economic development has a modest negative effect on conflict risk. As democracy levels increase, this peace-promoting effect becomes much stronger. In highly democratic countries (polyarchy = 0.9), the conflict-reducing effect of economic development is substantially amplified.\nThe theoretical interpretation is compelling: economic development may reduce conflict risk in all political systems, but democratic institutions help ensure that the benefits of growth are more widely shared and that economic grievances can be addressed through peaceful political processes rather than violence.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#model-comparison-and-selection-considerations",
    "href": "modules/module-12.1.html#model-comparison-and-selection-considerations",
    "title": "Module 12.1",
    "section": "Model Comparison and Selection Considerations",
    "text": "Model Comparison and Selection Considerations\nWhen should we include interaction terms in our models? This decision should be guided by both theoretical considerations and empirical evidence. Interactions should be theoretically motivated rather than the result of fishing expeditions through possible variable combinations.\nIn our conflict example, both interactions have strong theoretical foundations. The democracy-terrain interaction tests whether democratic institutions provide alternative channels for addressing grievances that might otherwise lead to insurgency in remote areas. The democracy-wealth interaction examines whether democratic institutions help translate economic development into political stability more effectively than authoritarian institutions.\nLet’s compare our models to see how interactions affect overall model fit:\n\n# Compare model fit using AIC\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Baseline\", \"Democracy × Terrain\", \"Democracy × Wealth\"),\n  AIC = c(AIC(baseline_model), AIC(terrain_interaction_model), AIC(wealth_interaction_model)),\n  Deviance = c(deviance(baseline_model), deviance(terrain_interaction_model), deviance(wealth_interaction_model))\n)\n\nmodels_comparison\n\n                Model      AIC Deviance\n1            Baseline 1119.287 1105.287\n2 Democracy × Terrain 1111.433 1095.433\n3  Democracy × Wealth 1115.608 1099.608\n\n\nBoth models reduce the AIC compared to the baseline model. We can confirm this using likelihood ratio tests, which compare nested models to see if the additional parameters significantly improve fit:\n\n# Test significance of interaction terms using likelihood ratio tests\nlibrary(lmtest)\n\n# Test terrain interaction\nlrtest(baseline_model, terrain_interaction_model)\n\nLikelihood ratio test\n\nModel 1: ucdponset ~ v2x_polyarchy + newlmtnest + wbpopest + wbgdppc2011est + \n    ethfrac + relfrac\nModel 2: ucdponset ~ democracy * newlmtnest + wbpopest + wbgdppc2011est + \n    ethfrac + relfrac\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)   \n1   7 -552.64                       \n2   8 -547.72  1 9.854   0.001695 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# For wealth interaction, need to adjust baseline model for fair comparison\nbaseline_continuous &lt;- glm(ucdponset ~ v2x_polyarchy + newlmtnest + wbpopest + \n                          wbgdppc2011est + ethfrac + relfrac,\n                          data = conflict_df, family = \"binomial\")\n\nlrtest(baseline_continuous, wealth_interaction_model)\n\nLikelihood ratio test\n\nModel 1: ucdponset ~ v2x_polyarchy + newlmtnest + wbpopest + wbgdppc2011est + \n    ethfrac + relfrac\nModel 2: ucdponset ~ v2x_polyarchy * wbgdppc2011est + newlmtnest + wbpopest + \n    ethfrac + relfrac\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)  \n1   7 -552.64                      \n2   8 -549.80  1 5.679    0.01717 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe way we read this results is that if the p-value is less than 0.05, then we can conclude that the interaction term significantly improves model fit compared to the baseline model. If the p-value is greater than 0.05, then we cannot conclude that the interaction term significantly improves model fit.\nBased on these tests, we can conclude that both interaction terms significantly improve model fit. However, remember that model fit alone should not drive interaction decisions. The theoretical motivation for expecting conditional effects should be the primary consideration.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-12.1.html#summary",
    "href": "modules/module-12.1.html#summary",
    "title": "Module 12.1",
    "section": "Summary",
    "text": "Summary\nThis module has equipped you with essential skills for handling realistic logistic regression analyses involving multiple predictors and interaction effects. We discovered how moving from bivariate to multiple predictor models changes coefficient interpretation by controlling for confounding variables. We learned to identify when interaction effects are theoretically justified and how they allow the effect of one variable to depend on another. We mastered both manual calculation techniques for simple interactions and automated tools for complex conditional relationships. Most importantly, we developed skills in visualizing and communicating complex interaction patterns to diverse audiences.\nThe Fearon-Laitin replication demonstrated how democracy can fundamentally alter the relationships between other variables and conflict risk. Mountainous terrain appears to promote conflict only in non-democratic settings, while economic development has stronger peace-promoting effects in democratic contexts. These findings illustrate why interaction effects are crucial for understanding real-world political phenomena.\nOur progression from manual calculation with dichotomous variables to automated tools with continuous variables provides a solid foundation for tackling any interaction scenario. The visualization techniques we learned help communicate complex statistical relationships to policy audiences who need to understand not just whether interactions exist, but what they mean in practice.",
    "crumbs": [
      "Course Modules",
      "Module 12.1"
    ]
  },
  {
    "objectID": "modules/module-10.1.html",
    "href": "modules/module-10.1.html",
    "title": "Module 10.1",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the vdemdata package (install.packages(\"vdemdata\")). Note that this is different from the vdemlite package that we have been using this semester. vdemdata is comprised of one function, vdem, which loads the entire V-Dem dataset into R. We are using vdemdata instead of vdemlite because it has some indicators that are not yet available in vdemlite.\nRestart your R session by going to Session &gt; Restart R in RStudio. The reason is that packages we used in the last model may result in conflicts in this one.\nRun the following code chunk to set up the data and packages required for this module. Then take a look to explore what is in the data frame.\n\n\nCodelibrary(tidyverse)\nlibrary(vdemdata)\n\nmodel_data &lt;- vdem |&gt;\n  filter(year == 2006) |&gt; \n  select(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc, \n    oil_rents = e_total_oil_income_pc,\n    polarization = v2cacamps, \n    corruption = v2x_corr, \n    judicial_review = v2jureview_ord, \n    region = e_regionpol_6C, \n    regime = v2x_regime) \n\n#glimpse(model_data)",
    "crumbs": [
      "Course Modules",
      "Module 10.1"
    ]
  },
  {
    "objectID": "modules/module-10.1.html#overview",
    "href": "modules/module-10.1.html#overview",
    "title": "Module 10.1",
    "section": "Overview",
    "text": "Overview\nIn our previous work with linear regression, we explored relationships between two continuous variables like the relationship between a country’s wealth and its level of democracy. But the real world is more complex than simple two-variable relationships. Countries differ not just in wealth, but also in their political institutions, regional contexts, historical experiences, and cultural factors. How do we account for these multiple influences simultaneously?\nMultiple linear regression allows us to model these complex relationships by including multiple predictor variables in a single model. This powerful technique helps us understand not just whether variables are related, but how they relate to each other while controlling for other factors.\nBefore we dive into the technical details, let’s hear from Andrew Ng, one of the leading experts in machine learning and statistical modeling, as he explains the fundamentals of multiple linear regression:\n\nBy the end of this module, you’ll be able to use categorical variables as predictors in linear regression models, interpret dummy variables and understand reference categories, build and interpret multiple regression models with both categorical and continuous predictors, understand the concept of “controlling for” other variables, and make informed decisions about which variables to include in your models.",
    "crumbs": [
      "Course Modules",
      "Module 10.1"
    ]
  },
  {
    "objectID": "modules/module-10.1.html#categorical-predictors-in-regression",
    "href": "modules/module-10.1.html#categorical-predictors-in-regression",
    "title": "Module 10.1",
    "section": "Categorical Predictors in Regression",
    "text": "Categorical Predictors in Regression\nSo far, we have worked with continuous predictors like GDP per capita and democracy scores. But many important variables in political science are categorical, representing distinct groups or categories rather than numerical measurements. In our democracy research, examples include regime type (democratic, autocratic, hybrid), world region (Europe, Asia, Americas, Africa), institutional features such as the presence or absence of judicial review, and historical experiences like colonial history or a communist past.\nAs we learned in a previous module, categorical variables can take different forms. Nominal variables have categories with no natural order, like world region. Ordinal variables have categories with a meaningful order, like education levels from primary through tertiary. Binary variables have just two categories, like the presence or absence of an institution.\nLet’s have a quick look at our data for this module to see whether we can spot our categorical variables:\n\nglimpse(model_data)\n\nRows: 177\nColumns: 9\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ lib_dem         &lt;dbl&gt; 0.479, 0.658, 0.877, 0.843, 0.639, 0.648, 0.777, 0.017…\n$ wealth          &lt;dbl&gt; 31.873, 22.316, 98.394, 105.485, 6.471, 23.398, 84.983…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; 0.072, -1.803, -2.284, -1.770, -0.472, 0.026, -2.250, …\n$ corruption      &lt;dbl&gt; 0.607, 0.205, 0.004, 0.022, 0.631, 0.436, 0.109, 0.887…\n$ judicial_review &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …\n$ region          &lt;dbl&gt; 2, 2, 5, 5, 4, 4, 6, 6, 1, 1, 3, 3, 2, 1, 2, 5, 5, 2, …\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n\n\nHopefully you can see that we have several categorical variables in our dataset: judicial_review, region and regime. judicial_review is a binary indicator (or “dummy”) variable indicating whether a country has judicial review, region is a nominal variable representing the world region, and regime is an ordinal variable representing different types of political regimes.\nFactors in R\nWhen we include categorical variables in regression models, we need to convert them into a format that the model can understand. In R, this is done using factors. Factors are a special data type that tells R to treat the variable as categorical, even if the categories are represented by numbers.\nRight now, if we look at our data types, we can see that judicial_review, region, and regime are in the dbl (double) format, which means they are treated as continuous numeric variables. To use them as categorical predictors, we need to convert them to factors.\nLet’s start with converting our binary judicial_review variable into a factor:\n\nmodel_data &lt;- model_data |&gt; \n  mutate(judicial_review = factor(judicial_review, \n                                   labels = c(\"No\", \"Yes\")))\n\nglimpse(model_data)\n\nRows: 177\nColumns: 9\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ lib_dem         &lt;dbl&gt; 0.479, 0.658, 0.877, 0.843, 0.639, 0.648, 0.777, 0.017…\n$ wealth          &lt;dbl&gt; 31.873, 22.316, 98.394, 105.485, 6.471, 23.398, 84.983…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; 0.072, -1.803, -2.284, -1.770, -0.472, 0.026, -2.250, …\n$ corruption      &lt;dbl&gt; 0.607, 0.205, 0.004, 0.022, 0.631, 0.436, 0.109, 0.887…\n$ judicial_review &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, …\n$ region          &lt;dbl&gt; 2, 2, 5, 5, 4, 4, 6, 6, 1, 1, 3, 3, 2, 1, 2, 5, 5, 2, …\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n\n\nNow judicial_review is a factor with two levels: “No” and “Yes”. This tells R to treat it as a categorical variable in our regression models.\nLinear Regression with a Categorical Predictor\nNow that we have our categorical variable set up, we can include it in a linear regression model. Our judicial review variable is based on the following question:\n\nDo high courts (Supreme Court, Constitutional Court, etc) have the power to rule on whether laws or policies are constitutional/legal? (Yes or No)\n\nWe can use this variable to explore whether countries with judicial review tend to have higher levels of democracy. Let’s start by visualizing the relationship between judicial review and democracy levels.\n\nCodeggplot(model_data, aes(x = wealth, y = lib_dem, \n                       color=judicial_review)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  theme_bw() +\n  scale_x_log10(labels = scales::dollar_format(suffix = \"k\")) +\n  scale_color_manual(name = \"Judicial Review\", \n                     values = c(\"steelblue3\", \"coral\"), \n                     labels = c(\"No\", \"Yes\")) \n\n\n\n\n\n\n\nHere we can see that at every level of wealth, countries with judicial review (in coral) tend to have higher democracy levels than those without (in blue).\nWe can more precisely quantify this relationship between judical review and democracy using a linear regression model. Let’s fit a model predicting democracy (lib_dem) with judicial review (judicial_review):\n\njudicial_model &lt;- lm(lib_dem ~ judicial_review, data = model_data)\n\nsummary(judicial_model)\n\n\nCall:\nlm(formula = lib_dem ~ judicial_review, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41867 -0.18967 -0.03267  0.19133  0.70804 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.12896    0.04871   2.648  0.00884 ** \njudicial_reviewYes  0.32271    0.05256   6.140 5.38e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2435 on 175 degrees of freedom\nMultiple R-squared:  0.1772,    Adjusted R-squared:  0.1725 \nF-statistic:  37.7 on 1 and 175 DF,  p-value: 5.384e-09\n\n\nThe output shows us the coefficients for the model. The intercept represents the average democracy level for countries without judicial review (the reference category), and the coefficient for judicial_reviewYes represents the difference in average democracy level between countries with judicial review and those without.\n\n\n\n\n\n\nUnderstanding Reference Categories\n\n\n\nWhen we include a categorical variable in a regression model, R automatically creates dummy variables for each category except one. The category that does not get its own dummy variable is called the reference category or baseline category. In this example, the reference category is “No” for judicial review.\nWhen we interpret a coefficient for a categorical variable, we are comparing that category to the reference category. So the coefficient for judicial_reviewYes tells us how much higher the average democracy level is for countries with judicial review compared to those without.\n\n\nTo better understand the model, let’s have a look at the model equation:\n\\[ \\widehat{Democracy_{i}} = 0.18 + 0.27*JudicialReview(yes) \\]\nHere the slope of the coefficient for \\(JudicialReview(yes)\\) indicates that. countries with judicial review are expected, on average, to be 0.27 units more democratic on the liberal democracy index. The intercept of 0.18 represents the average democracy score for countries without judicial review.",
    "crumbs": [
      "Course Modules",
      "Module 10.1"
    ]
  },
  {
    "objectID": "modules/module-10.1.html#multiple-categories-and-reference-groups",
    "href": "modules/module-10.1.html#multiple-categories-and-reference-groups",
    "title": "Module 10.1",
    "section": "Multiple Categories and Reference Groups",
    "text": "Multiple Categories and Reference Groups\nBinary categorical variables are straightforward because they create a single dummy variable. But what happens when we have categorical variables with more than two categories, like world regions or regime types?\nWhen a categorical variable has multiple levels, R creates multiple dummy variables, specifically one for each category except for the reference category. For example, if we have a region variable with 6 categories, R will create 5 dummy variables: one for Latin America (1 if Latin America, 0 otherwise), one for MENA (1 if MENA, 0 otherwise), one for Sub-Saharan Africa (1 if SSA, 0 otherwise), one for Western Europe & North America (1 if WENA, 0 otherwise), and one for Asia & Pacific (1 if Asia, 0 otherwise). There is no dummy variable for Eastern Europe, which becomes our reference category.\nThe reference category is crucial because all other categories are interpreted relative to this baseline. Without it, we’d have perfect multicollinearity where the dummy variables would be perfectly correlated with each other and the intercept, making the model impossible to estimate.\nLet’s see this in action with world regions and democracy. Let’s start by converting the region variable into a factor with labels for each region:\n\nmodel_data &lt;- model_data |&gt; \n  mutate(region = factor(region, \n                         labels = c(\"Eastern Europe\", \n                                    \"Latin America\", \n                                    \"MENA\", \n                                    \"Sub-Saharan Africa\", \n                                    \"Western Europe & North America\", \n                                    \"Asia & Pacific\")))\n\nglimpse(model_data)\n\nRows: 177\nColumns: 9\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ lib_dem         &lt;dbl&gt; 0.479, 0.658, 0.877, 0.843, 0.639, 0.648, 0.777, 0.017…\n$ wealth          &lt;dbl&gt; 31.873, 22.316, 98.394, 105.485, 6.471, 23.398, 84.983…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; 0.072, -1.803, -2.284, -1.770, -0.472, 0.026, -2.250, …\n$ corruption      &lt;dbl&gt; 0.607, 0.205, 0.004, 0.022, 0.631, 0.436, 0.109, 0.887…\n$ judicial_review &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, …\n$ region          &lt;fct&gt; Latin America, Latin America, Western Europe & North A…\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n\n\nNow our region variable is a factor with six levels, and Eastern Europe will be our reference category. We can check this by using the base R levels() function:\n\n# Check the levels of our region variable\nlevels(model_data$region)\n\n[1] \"Eastern Europe\"                 \"Latin America\"                 \n[3] \"MENA\"                           \"Sub-Saharan Africa\"            \n[5] \"Western Europe & North America\" \"Asia & Pacific\"                \n\n\nWhen we fit a regression model with this region variable, R will create dummy variables for each region except Eastern Europe:\n\nregion_model &lt;- lm(lib_dem ~ region, data = model_data)\n\nsummary(region_model)\n\n\nCall:\nlm(formula = lib_dem ~ region, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45808 -0.13136 -0.01907  0.11464  0.49405 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           0.43407    0.03610  12.024  &lt; 2e-16 ***\nregionLatin America                   0.06501    0.05354   1.214  0.22634    \nregionMENA                           -0.23712    0.05708  -4.154 5.14e-05 ***\nregionSub-Saharan Africa             -0.14043    0.04566  -3.075  0.00245 ** \nregionWestern Europe & North America  0.37327    0.05415   6.893 1.01e-10 ***\nregionAsia & Pacific                 -0.13471    0.05196  -2.593  0.01034 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1977 on 171 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4546 \nF-statistic: 30.34 on 5 and 171 DF,  p-value: &lt; 2.2e-16\n\n\nThe equation for this model looks like this:\n\\[\\hat{Democracy} = \\beta_0 + \\beta_1 \\cdot LatinAmerica + \\beta_2 \\cdot MENA + \\beta_3 \\cdot SSAfrica + \\beta_4 \\cdot TheWest + \\beta_5 \\cdot Asia\\]\nIn this equation, \\(\\beta_0\\) (the intercept) represents the average democracy level in Eastern Europe (the reference category). Each of the other coefficients represents the difference in average democracy between that region and Eastern Europe. So \\(\\beta_1\\) is the difference in average democracy between Latin America and Eastern Europe, \\(\\beta_2\\) is the difference between MENA and Eastern Europe, and so on.\nSometimes you may want a different reference category. The relevel() function allows you to change which category serves as the baseline. Let’s change the reference category for our analysis to Sub-Saharan Africa (SSA):\n\n# Make Sub-Saharan Africa the reference category\nmodel_data &lt;- model_data |&gt; \n  mutate(region2 = relevel(region, ref = \"Sub-Saharan Africa\"))\n\nlevels(model_data$region2)\n\n[1] \"Sub-Saharan Africa\"             \"Eastern Europe\"                \n[3] \"Latin America\"                  \"MENA\"                          \n[5] \"Western Europe & North America\" \"Asia & Pacific\"                \n\n\nNow when we fit the model again, Sub-Saharan Africa will be the reference category:\n\nregion_model_ssa &lt;- lm(lib_dem ~ region2, data = model_data)\n\nsummary(region_model_ssa)\n\n\nCall:\nlm(formula = lib_dem ~ region2, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45808 -0.13136 -0.01907  0.11464  0.49405 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                            0.293640   0.027962  10.501  &lt; 2e-16 ***\nregion2Eastern Europe                  0.140427   0.045662   3.075  0.00245 ** \nregion2Latin America                   0.205440   0.048432   4.242 3.62e-05 ***\nregion2MENA                           -0.096690   0.052312  -1.848  0.06628 .  \nregion2Western Europe & North America  0.513693   0.049100  10.462  &lt; 2e-16 ***\nregion2Asia & Pacific                  0.005717   0.046670   0.123  0.90265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1977 on 171 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4546 \nF-statistic: 30.34 on 5 and 171 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\nWhich regimes have more corruption?\n\nThere is one more categorical variable in our dataset: regime, which represents different types of political regimes. Convert this variable into a factor.\nCheck the levels of the regime variable and identify which category will be the reference category (you can tell because it is the first level). It should be “Closed Autocracy.”\nNow visualize the differences in corruption levels across regime types using a bar plot. Use ggplot() to create a bar plot with regime on the x-axis and average corruption (corruption) on the y-axis.\nNow fit a regression model predicting corruption (corruption) from regime type (regime). What does the model tell you about the relationship between regime type and corruption levels? How would you interpret the coefficients for each regime type (relative to the baseline category)?\nFinally, siwtch the reference category to “Electoral Democracy” and refit the model. How do the coefficients change?",
    "crumbs": [
      "Course Modules",
      "Module 10.1"
    ]
  },
  {
    "objectID": "modules/module-10.1.html#multiple-predictors",
    "href": "modules/module-10.1.html#multiple-predictors",
    "title": "Module 10.1",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\nNow we are ready to discuss models with multiple predictors. This is an important step because this is where the real power of multiple regression lies because it allows us to control for confounding variables, isolate the effect of specific predictors, build more accurate predictions, and test complex theories.\nThe fundamental logic of multiple regression centers on asking: “What is the relationship between each predictor and the outcome, holding all other predictors constant?” Or sometimes you will hear this idea of holding everything else constant phrased as ceteris paribus which is Latin for “all things being equal.”\nAdding Continuous Predictors\nLet’s start by adding a second predictor (polarization) to the wealth and democracy model that we worked with in the last module. We’ve seen that polarization is negatively related to democracy. But what happens when we also include wealth? We can compare a model with just polarization to a model with both polarization and wealth:\n\nm1_polarization_democracy &lt;- lm(lib_dem ~ polarization + log(wealth), data = model_data)\n\nsummary(m1_polarization_democracy)\n\n\nCall:\nlm(formula = lib_dem ~ polarization + log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66293 -0.15068  0.04725  0.18170  0.41785 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.09967    0.04458   2.236   0.0267 *  \npolarization -0.05712    0.01386  -4.120 5.90e-05 ***\nlog(wealth)   0.09972    0.01507   6.619 4.55e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2171 on 170 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.3612,    Adjusted R-squared:  0.3537 \nF-statistic: 48.07 on 2 and 170 DF,  p-value: &lt; 2.2e-16\n\n\nThe model equation with multiple predictors becomes:\n\\[\\hat{Democracy} = \\beta_0 + \\beta_1 \\cdot Polarization + \\beta_2 \\cdot \\log(Wealth)\\]\nIn this equation, \\(\\beta_0\\) represents the predicted democracy level when both polarization and log(wealth) equal zero. \\(\\beta_1\\) represents the change in democracy for a 1-unit increase in polarization, holding wealth constant. \\(\\beta_2\\) represents the change in democracy for a 1-unit increase in log(wealth), holding polarization constant. The key phrase is “holding other variables constant.”\nIn this case, we see that polarization is associated with a .058 unit deacrease in the democracy score, while log(wealth) is associated with a .098 unit increase in the democracy score. We can tell the direction of the relationship (positive or negative) based on the sign of the coefficient. For polarization the sign is negative and for wealth it is positive.\nWe can continue adding predictors to build even more complex models. Let’s try adding oil rents per capita (oil_rents) to our model. Oil rents are the income a country receives from oil extraction and, as we have already noticed in our scatter plots, they can have significant effects on democracy.\n\nm2_three_predictors  &lt;- lm(lib_dem ~ polarization + log(wealth) + oil_rents, data = model_data)\n\nsummary(m2_three_predictors)\n\n\nCall:\nlm(formula = lib_dem ~ polarization + log(wealth) + oil_rents, \n    data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52558 -0.14413  0.03809  0.12850  0.67700 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.516e-02  4.190e-02   0.600    0.549    \npolarization -5.531e-02  1.283e-02  -4.312 2.83e-05 ***\nlog(wealth)   1.372e-01  1.484e-02   9.243  &lt; 2e-16 ***\noil_rents    -4.158e-05  5.956e-06  -6.980 7.69e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1936 on 158 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.5043,    Adjusted R-squared:  0.4948 \nF-statistic: 53.57 on 3 and 158 DF,  p-value: &lt; 2.2e-16\n\n\nThe model equation now becomes:\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc + b_3*OilRents \\]\nWhere \\(Y_i\\) is the predicted democracy level for country \\(i\\), \\(a\\) is the intercept, and \\(b_1\\), \\(b_2\\), and \\(b_3\\) are the coefficients for polarization, log(wealth), and oil rents, respectively. Now, the coefficient for \\(OilRents\\) tells us the predicted change in democracy for a 1-unit increase in oil rents, holding polarization and log(wealth) constant. Similarly, the coefficients for polarization and log(wealth) tell us the predicted change in democracy for a 1-unit increase in those variables, while controlling for the other two predictors.\n\n\n\n\n\n\nNote\n\n\n\nNotice how the coefficients might change when we add predictors. This happens for two main reasons. First, confounding occurs when our predictors are correlated with each other. If polarization and wealth are correlated, the simple regression coefficient for polarization includes both the direct effect of polarization and the indirect effect through its correlation with wealth. Second, when we control for other variables, the multiple regression coefficient for polarization shows only the direct effect, after removing the part that’s explained by wealth.\n\n\nCombining Variable Types\nIt is also entirely possible to combine different types of predictors. We can include both categorical variables (like region or regime type) and continuous variables (like wealth or polarization) in the same model.\nWhen we build mixed models that include both types of predictors, the interpretation becomes even richer. Consider a model that includes all of the predictors we have discussed so far: polarization, wealth, oil rents and world regions:\n\nm3_mixed_model &lt;- lm(lib_dem ~ polarization + log(wealth) + oil_rents + region, data = model_data)\n\nsummary(m3_mixed_model)\n\n\nCall:\nlm(formula = lib_dem ~ polarization + log(wealth) + oil_rents + \n    region, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46015 -0.09635 -0.00940  0.10693  0.38539 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           2.594e-02  7.082e-02   0.366 0.714627    \npolarization                         -3.920e-02  1.162e-02  -3.375 0.000936 ***\nlog(wealth)                           1.249e-01  2.006e-02   6.224 4.43e-09 ***\noil_rents                            -2.941e-05  6.032e-06  -4.875 2.70e-06 ***\nregionLatin America                   8.940e-02  4.710e-02   1.898 0.059569 .  \nregionMENA                           -1.682e-01  5.514e-02  -3.051 0.002687 ** \nregionSub-Saharan Africa              6.684e-02  5.166e-02   1.294 0.197672    \nregionWestern Europe & North America  2.009e-01  5.529e-02   3.633 0.000383 ***\nregionAsia & Pacific                 -5.752e-02  4.946e-02  -1.163 0.246661    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1702 on 153 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.6291,    Adjusted R-squared:  0.6097 \nF-statistic: 32.43 on 8 and 153 DF,  p-value: &lt; 2.2e-16\n\n\nThis model allows us to explore how democracy varies by region while controlling for wealth, polarization, and oil rents.The regional coefficients now represent regional differences in democracy among countries with the same wealth level, polarization and oil rents. This is a much more precise comparison than simply comparing regional averages, because it accounts for the fact that regions differ systematically in their wealth levels.\nMaking Predictions\nOnce we have estimated a multiple regression model, we can use it to make predictions for new cases. To see how this works, let’s return to our model that includes polarization, log-transformed wealth, oil rents, and region as predictors. Suppose we want to predict the level of democracy for a hypothetical country that has a polarization score of 0.5, a GDP per capita of $8.1k (which corresponds to a log wealth of 2.09), oil rents of 2, and is located in Latin America.\nThe regression model gives us an equation that looks like this:\n\\[\n\\hat{Democracy} = a + b_1 \\cdot Polarization + b_2 \\cdot \\log(Wealth) + b_3 \\cdot OilRents + b_4 \\cdot Region\n\\]\nEach coefficient—\\(b_1\\), \\(b_2\\), \\(b_3\\), and so on—represents the effect of a one-unit change in that variable, holding the others constant. To make a prediction, we simply plug in the values: 0.5 for polarization, 9 for log(wealth), 2 for oil rents, and the appropriate regional adjustment for Latin America. The result is the model’s best guess for the democracy score of a country with those characteristics.\nTo do this, we can use R’s predict() function. This function takes a fitted model and a new set of predictor values and returns the predicted outcome:\n\nnew_data &lt;- tibble(\n  polarization = 0.5,\n  wealth = 8.1,  \n  oil_rents = 2,\n  region = \"Latin America\"\n)\n\npredict(m3_mixed_model, newdata = new_data)\n\n        1 \n0.3568916 \n\n\nSo in other words, with these inputs, the model predicts a democracy score of approximately 0.46 for this hypothetical country. This prediction reflects what the model expects, given the relationships it has learned from the original data. It doesn’t guarantee what will happen in the real world, but it gives us a principled estimate based on the variables we think matter.\n\nIf you’re curious about uncertainty, predict() can also provide a confidence interval for the prediction by adding the argument interval = \"confidence\".\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nTry running a multiple linear regression with corruption as the dependent variable.\nThink about what variables you would want to include as predictors. Use a mix of continuous and categorical variables, such as wealth, oil rents, region, and regime type.\nFit the model and interpret the coefficients. What do they tell you about the relationship between these predictors and corruption levels?\nNow try making some predictions with your model. Create a new data frame with hypothetical values for each predictor (e.g., a country with a wealth of $10k, oil rents of 3, and located in Asia). Use the predict() function to estimate the corruption level for this hypothetical country.",
    "crumbs": [
      "Course Modules",
      "Module 10.1"
    ]
  },
  {
    "objectID": "modules/module-10.1.html#conclusion",
    "href": "modules/module-10.1.html#conclusion",
    "title": "Module 10.1",
    "section": "Conclusion",
    "text": "Conclusion\nMultiple linear regression allows us to model complex relationships while controlling for multiple factors at once. In this module, you learned how to include both continuous and categorical predictors, interpret coefficients while holding other variables constant, and use regression to make informed predictions. You also saw how adding variables can change the interpretation of coefficients and why reference categories matter when working with categorical predictors.\nKey lessons include the importance of controlling for confounders, the role of theory in guiding model building, and the need for careful interpretation. More variables don’t always mean a better model—clarity and purpose matter more. In the next modules, we’ll explore interaction effects, check model assumptions, and distinguish between models built for explanation versus those built for prediction.",
    "crumbs": [
      "Course Modules",
      "Module 10.1"
    ]
  },
  {
    "objectID": "modules/module-4.22.html",
    "href": "modules/module-4.22.html",
    "title": "Module 4.2",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall peacesciencer, broom and modelsummary. Familiarize yourself with the basic purpose and usage of these packages,\n\ninstall.packages(c(\"peacesciencer\", \"broom\", \"modelsummary\"))"
  },
  {
    "objectID": "modules/module-4.22.html#overview",
    "href": "modules/module-4.22.html#overview",
    "title": "Module 4.2",
    "section": "Overview",
    "text": "Overview\nA very common use of tables in the social sciences is to present regression results. There are numerous packages available for presenting regression output. In this lesson, we are going to focus on one of them that I think is particularly good for both pdf and html output: modelsummary. modelsummary also includes a function (modelplot()) for plotting point estimates and confidence intervals. So part of the objective of this lesson is going to be to learn when you should use a table to present your regression results versus when you should use a plot.\nWe will be taking our example from the peace studies literature. We are going to download data using the peacesciencer package and use it to partially reproduce a famous analysis of conflict onset by Fearon and Laitin."
  },
  {
    "objectID": "modules/module-4.22.html#run-a-regression-model-and-display-results-with-broom",
    "href": "modules/module-4.22.html#run-a-regression-model-and-display-results-with-broom",
    "title": "Module 4.2",
    "section": "Run a regression model and display results with broom",
    "text": "Run a regression model and display results with broom\n\nWe will start off by building a data frame for our analysis with the peacesciencer package. peacesciencer is designed to make standard analysis for conflict studies more convenient and includes many of the control variables that you would use to estimate the likelihood of conflict onset or duration.\nTo start, we call create_stateyears() to create a time-series data set for all available countries. We will specify system = 'gw' to denote the Gleditsch-Ward country coding system. Then we will filter for roughly the same years of Fearon and Laitin’s original analysis (1945-99) with the caveat that peacesciencer only has conflict data starting in 1946.\nThen we add a bunch of data to the data frame using the various add_X functions available in the package. Here we add UCDP conflict data which includes our dependent variable for this analysis–conflict onset. Then we add measures of democracy, ethno-religious fractionalization, GDP and terrain.\n\nlibrary(peacesciencer)\nlibrary(dplyr)\n\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()\n\nglimpse(conflict_df)\n\nRows: 7,624\nColumns: 22\n$ gwcode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ gw_name        &lt;chr&gt; \"United States of America\", \"United States of America\",…\n$ microstate     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ year           &lt;dbl&gt; 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1…\n$ ucdpongoing    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ucdponset      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ maxintensity   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ conflict_ids   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ euds           &lt;dbl&gt; 1.293985, 1.308359, 1.343539, 1.330836, 1.354015, 1.350…\n$ aeuds          &lt;dbl&gt; 0.4862558, 0.5006298, 0.5358093, 0.5231064, 0.5462858, …\n$ polity2        &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…\n$ v2x_polyarchy  &lt;dbl&gt; 0.603, 0.607, 0.599, 0.580, 0.585, 0.611, 0.611, 0.612,…\n$ ethfrac        &lt;dbl&gt; 0.2226323, 0.2248701, 0.2271561, 0.2294918, 0.2318781, …\n$ ethpol         &lt;dbl&gt; 0.4152487, 0.4186156, 0.4220368, 0.4255134, 0.4290458, …\n$ relfrac        &lt;dbl&gt; 0.4980802, 0.5009111, 0.5037278, 0.5065309, 0.5093204, …\n$ relpol         &lt;dbl&gt; 0.7769888, 0.7770017, 0.7770303, 0.7770729, 0.7771274, …\n$ wbgdp2011est   &lt;dbl&gt; 28.539, 28.519, 28.545, 28.534, 28.572, 28.635, 28.669,…\n$ wbpopest       &lt;dbl&gt; 18.744, 18.756, 18.781, 18.804, 18.821, 18.832, 18.848,…\n$ sdpest         &lt;dbl&gt; 28.478, 28.456, 28.483, 28.469, 28.510, 28.576, 28.611,…\n$ wbgdppc2011est &lt;dbl&gt; 9.794, 9.762, 9.764, 9.730, 9.752, 9.803, 9.821, 9.857,…\n$ rugged         &lt;dbl&gt; 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073,…\n$ newlmtnest     &lt;dbl&gt; 3.214868, 3.214868, 3.214868, 3.214868, 3.214868, 3.214…\n\n\nNow let’s go ahead and run the analysis. We will specify a logit model using the glm() function and specifying family = binomial(link = \"logit\"). We will store our model in an object called conflict_model. And from there we can use the tidy() function from the broom package to view the results.\nBefore doing this, though, try calling summary() from base R on the model. This provides us with a basic regression table and it is great insofar as we don’t want to do anything else with these estimates. Next, go ahead and call View() on the model object of just click on it to see what it looks like. You will notice that the results are stored in a complicated list format.\nThe tidy() function enables us to take the results from this list and store them in a “tibble”, which is the Tidyverse equivalent of a data frame. Once the results are stored like this, we can easily access the estimates for anything that we might want to do with them including combining the results of different models or displaying particular estimates in our document using all of the tools that we have learned in this course. We can also set conf.int = TRUE) as an argument in tidy() to create and store confidence intervals.\nBy default, tidy() returns p-values with large numbers of digits following the decimal point, making hard to tell whether the variables are significant. To fix this, we can pipe our tidy() output into a mutate_if() call and specify that we want numeric output to round to five decimal places.\n\nlibrary(broom)\n\nconflict_model &lt;- glm(ucdponset ~ ethfrac + relfrac + v2x_polyarchy + \n                        rugged + wbgdppc2011est + wbpopest,\n                  data= conflict_df,\n                  family = binomial(link=\"logit\"))\n\n# summary(conflict_model)\n\ntidy_model &lt;- conflict_model |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  mutate_if(is.numeric, round, 5)\n\ntidy_model\n\n# A tibble: 7 × 7\n  term           estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -5.38      1.41      -3.81  0.00014 -8.13       -2.60 \n2 ethfrac          0.709     0.372      1.91  0.0566  -0.00829     1.45 \n3 relfrac         -0.288     0.409     -0.702 0.482   -1.10        0.511\n4 v2x_polyarchy   -0.652     0.513     -1.27  0.204   -1.67        0.344\n5 rugged           0.0836    0.0756     1.11  0.269   -0.0716      0.226\n6 wbgdppc2011est  -0.392     0.120     -3.26  0.0011  -0.632      -0.161\n7 wbpopest         0.285     0.0677     4.20  0.00003  0.153       0.418\n\n\nHow did we do relative to Fearon and Latin’s original analysis. Well, one thing that F&L were pretty certain about is that ethnic and religious fractionalization do not matter for conflict onset. But here we find a statistically significant relationship between these variables and conflict onset. But one thing we do find in common with their analysis is the importance of wealth and population. Both of these variables are significant in the expected direction. Wealthier countries experience less risk of conflict onset while more populous ones have a higher risk."
  },
  {
    "objectID": "modules/module-4.22.html#run-many-regressions-and-display-with-modelsummary",
    "href": "modules/module-4.22.html#run-many-regressions-and-display-with-modelsummary",
    "title": "Module 4.2",
    "section": "Run many regressions and display with modelsummary",
    "text": "Run many regressions and display with modelsummary\n\nbroom is really great if we want to just run one regression and see the results in the context of a working document. But what if we want to display our results to other researchers? For this, we need to use a different package. One package that is really good at producing professional-looking tabels is modelsummary and one of its strongest features is the ability to combine multiple models into the same table while still allowing for substantial customization.\nLet’s go ahead and store four models. In the first three, we will feature sets of predictors (ethnicity, democracy and terrain) and then a final model that includes all of our predictors. In each model, we will include wealth (GDP) and population as controls because we have a feeling that these are robust predictors of conflict onset.\n\nethnicity &lt;- glm(ucdponset ~ ethfrac + relfrac + wbgdppc2011est + wbpopest, # store each model in an object\n                  data = conflict_df,\n                  family = binomial(link=\"logit\"))\n\ndemocracy &lt;- glm(ucdponset ~ v2x_polyarchy + wbgdppc2011est +  wbpopest,\n                  data = conflict_df,\n                  family = binomial(link=\"logit\"))\n\nterrain &lt;- glm(ucdponset ~ rugged + wbgdppc2011est + wbpopest ,\n                  data = conflict_df,\n                  family = binomial(link=\"logit\"))\n\nfull_model &lt;- glm(ucdponset ~ ethfrac + relfrac + v2x_polyarchy + rugged +\n                        wbgdppc2011est + wbpopest,\n                  data = conflict_df,\n                  family = binomial(link=\"logit\"))\n\nNext we will store our models as a list with intuitive names that we can display as column headers in our regression table. Then we will store a new coefficient mapping where we rename our variables and change the order that they will appear in the table. We will also store a title and a reference.\n\nmodels &lt;- list(\"Ethnicity\" = ethnicity,  # store list of models in an object\n               \"Democracy\" = democracy, \n               \"Terrain\" = terrain, \n               \"Full Model\" = full_model)\n\ncoef_map &lt;- c(\"ethfrac\" = \"Ethnic Frac\",  # map coefficients\n        \"relfrac\" = \"Religions Frac\",     #(change names and order)\n        \"v2x_polyarchy\" = \"Polyarchy\",\n        \"rugged\" = \"Terrain\",\n        \"wbgdppc2011est\" = \"Per capita GDP\",\n        \"wbpopest\" = \"Population\",\n        \"(Intercept)\" = \"Intercept\")\n\ncaption = \"Table 1: Predictors of Conflict Onset\" # store caption\nreference = \"See appendix for data sources.\"      # store reference notes\n\nNow we can call modelsummary(). The first argument is the list of models we want to display. Next we tell modelsummary that we want it to show stars for statistical significance (stars = TRUE). And in gof_map, we say that, of the many goodness of fit statistics available to us, we only want to include the number of observations. Finally we plug our title into title = and a source note into notes =.\n\nlibrary(modelsummary)\n\nmodelsummary(models,                      # display the table\n             stars = TRUE,                # include stars for significance\n             gof_map = c(\"nobs\"),         # goodness of fit stats to include   \n             coef_map = coef_map,         # coefficient mapping\n             title = caption,             # title\n             notes = reference)           # source note\n\n\n\n    \n\n\n      \nTable 1: Predictors of Conflict Onset\n        \n \n                Ethnicity\n                Democracy\n                Terrain\n                Full Model\n              \n\n+ p \nSee appendix for data sources.\n\n\n\nEthnic Frac\n                  0.733*\n                  \n                  \n                  0.709+\n                \n\n\n                  (0.365)\n                  \n                  \n                  (0.372)\n                \n\nReligions Frac\n                  -0.429\n                  \n                  \n                  -0.288\n                \n\n\n                  (0.409)\n                  \n                  \n                  (0.409)\n                \n\nPolyarchy\n                  \n                  -0.801*\n                  \n                  -0.652\n                \n\n\n                  \n                  (0.390)\n                  \n                  (0.513)\n                \n\nTerrain\n                  \n                  \n                  0.035\n                  0.084\n                \n\n\n                  \n                  \n                  (0.075)\n                  (0.076)\n                \n\nPer capita GDP\n                  -0.485***\n                  -0.275***\n                  -0.553***\n                  -0.392**\n                \n\n\n                  (0.104)\n                  (0.059)\n                  (0.092)\n                  (0.120)\n                \n\nPopulation\n                  0.278***\n                  0.300***\n                  0.296***\n                  0.285***\n                \n\n\n                  (0.067)\n                  (0.051)\n                  (0.050)\n                  (0.068)\n                \n\nIntercept\n                  -4.571***\n                  -6.184***\n                  -4.166***\n                  -5.376***\n                \n\n\n                  (1.323)\n                  (0.967)\n                  (1.140)\n                  (1.410)\n                \n\nNum.Obs.\n                  6364\n                  6955\n                  6840\n                  6129"
  },
  {
    "objectID": "modules/module-4.22.html#when-a-coefficient-plot-is-better",
    "href": "modules/module-4.22.html#when-a-coefficient-plot-is-better",
    "title": "Module 4.2",
    "section": "When a coefficient plot is better",
    "text": "When a coefficient plot is better\n\nA regression table is great when we have many models that we want to display. But what happens when we have just one model? We could present something like our earlier tidy() output which has the beta coefficient, the standard error, t-statistic and p-values in separate columns, but this would be unconventional and would take up a lot of space. Another option is just to present a table with one column like this:\n\nmodelsummary(conflict_model, \n             stars = TRUE,  \n             gof_map = c(\"nobs\"),\n             coef_map = coef_map,\n             title = caption, \n             notes = reference)\n\n\n\n    \n\n\n      \nTable 1: Predictors of Conflict Onset\n        \n \n                (1)\n              \n\n+ p \nSee appendix for data sources.\n\n\n\nEthnic Frac\n                  0.709+\n                \n\n\n                  (0.372)\n                \n\nReligions Frac\n                  -0.288\n                \n\n\n                  (0.409)\n                \n\nPolyarchy\n                  -0.652\n                \n\n\n                  (0.513)\n                \n\nTerrain\n                  0.084\n                \n\n\n                  (0.076)\n                \n\nPer capita GDP\n                  -0.392**\n                \n\n\n                  (0.120)\n                \n\nPopulation\n                  0.285***\n                \n\n\n                  (0.068)\n                \n\nIntercept\n                  -5.376***\n                \n\n\n                  (1.410)\n                \n\nNum.Obs.\n                  6129\n                \n\n\n\n\n\n\nBut this is also somewhat unconventional and makes our regression output look a little bit lonely. A better option could be to display a coefficient plot that shows point estimates and confidence intervals. This is often preferable with one model because it makes our results so much easier to interpret.\nLet’s try making a coefficient plot with modelplot() from the modelsummary package. The syntax for modelplot() is very similar to that of modelsummary() but there are a few small differences.\nFirst, it puts maps the coefficients from the bottom up, so if you to maintain the order of the coefficients you need to reverse the mapping. We do this with the rev() function.\nSecond, we want to omit the intercept with coef_omit = \"Intercept\" because the emphasis in a coefficient plot is less on the exact regression equation and more on the magnitude and significance of the coefficients.\nThird we can customize various things. We can specify the color of the points and confidence intervals and we can load ggplot2 for further customization. Now we can add geoms and labels just like any other ggplot object. Here we add a red vertical intercept line at zero to make it clearer which variables are significant (e.g. the confidence interval does not overlap with zero). And we add a title and a caption using the labs() function.\n\nlibrary(ggplot2)\n\n\nmodelplot(conflict_model, \n          coef_map = rev(coef_map), # rev() reverses list order\n          coef_omit = \"Intercept\", \n          color = \"blue\") + # use plus to add customizations like any ggplot object\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = .75) + # red 0 line\n  labs(\n    title = \"Figure 1: Predictors of Conflict Onset\",\n    caption = \"See appendix for data sources.\"\n  )"
  },
  {
    "objectID": "modules/module-11.2.html",
    "href": "modules/module-11.2.html",
    "title": "Module 11.2",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the marginaleffects package if you haven’t already (install.packages(\"marginaleffects\")) and revew the documentation\n\nInstall the tidymodels package if you haven’t already (install.packages(\"tidymodels\"))\nFamiliarize yourself with the broom package for cleaning up model output",
    "crumbs": [
      "Course Modules",
      "Module 11.2"
    ]
  },
  {
    "objectID": "modules/module-11.2.html#overview",
    "href": "modules/module-11.2.html#overview",
    "title": "Module 11.2",
    "section": "Overview",
    "text": "Overview\nBuilding on the mathematical foundations we established in Module 5.2, this module focuses on the practical interpretation of logistic regression results. While we now understand how the logit function and sigmoid transformation work mathematically, the real challenge lies in translating model output into meaningful insights about our research questions.\nIn this module, we will explore three complementary approaches to interpreting bivariate logistic regression results. We will learn to move beyond the raw log-odds coefficients to understand odds ratios, which provide more intuitive measures of effect size. We will discover how to calculate predicted probabilities both manually and using specialized R packages, enabling us to make concrete predictions about specific scenarios. Finally, we will synthesize these approaches to conduct complete analyses that effectively communicate our findings.\nBy the end of this module, you will be able to interpret odds ratios and explain their practical meaning, calculate predicted probabilities for specific cases both by hand and using R tools, and integrate multiple interpretation approaches to tell a complete analytical story. We will continue using our conflict onset example to maintain continuity with previous modules, focusing exclusively on bivariate relationships to solidify these foundational interpretation skills before moving to more complex models.",
    "crumbs": [
      "Course Modules",
      "Module 11.2"
    ]
  },
  {
    "objectID": "modules/module-11.2.html#what-do-the-logistic-regression-coefficients-mean",
    "href": "modules/module-11.2.html#what-do-the-logistic-regression-coefficients-mean",
    "title": "Module 11.2",
    "section": "What Do the Logistic Regression Coefficients Mean?",
    "text": "What Do the Logistic Regression Coefficients Mean?\nWhen we run a logistic regression model, the coefficients we obtain are expressed on the log-odds scale. This mathematical necessity allows us to model probabilities using linear relationships, but it creates a significant challenge for interpretation. Consider a coefficient of -0.33 for log GDP per capita in predicting conflict onset. While we can determine that this negative coefficient suggests that higher GDP per capita is associated with lower probability of conflict onset, the magnitude of -0.33 is difficult to interpret in practical terms.\nThis interpretive challenge arises because most of us do not think naturally in terms of log-odds. When we want to understand the relationship between economic development and conflict risk, we are more interested in questions like “How much does doubling GDP per capita change the odds of conflict?” or “What is the predicted probability of conflict for a country with $8,000 GDP per capita?” These are the types of questions that policymakers, researchers, and citizens actually care about, yet they require us to transform our model results into more intuitive metrics.\nLet’s begin with the bivariate logistic regression model from Module 5.2 that examines the relationship between GDP per capita and conflict onset. This model provides our foundation for exploring different interpretation approaches:\n\nlibrary(peacesciencer)\nlibrary(dplyr)\nlibrary(broom)\n\n# Recreate the conflict dataset from Module 5.2\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()\n\n# Fit the bivariate logistic regression model\nconflict_model &lt;- glm(ucdponset ~ wbgdppc2011est,\n                      data = conflict_df,\n                      family = \"binomial\")\n\n# Display the model summary\nsummary(conflict_model)\n\n\nCall:\nglm(formula = ucdponset ~ wbgdppc2011est, family = \"binomial\", \n    data = conflict_df)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -1.00735    0.41297  -2.439   0.0147 *  \nwbgdppc2011est -0.35695    0.05089  -7.015 2.31e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1420.5  on 7623  degrees of freedom\nResidual deviance: 1381.9  on 7622  degrees of freedom\nAIC: 1385.9\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\n\n\n\n\nNote\n\n\n\nBe sure to read the documentation for create_stateyears() in peacesciencer before interpreting your results. The GDP variables in this dataset are in constant 2011 US dollars, and the model uses the natural logarithm of GDP per capita.\n\n\nThe model equation can be written as:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16 - 0.33 \\times \\text{log GDP per capita}\\]\nWhile the negative coefficient tells us the direction of the relationship, we need additional tools to understand the practical magnitude and significance of this effect.",
    "crumbs": [
      "Course Modules",
      "Module 11.2"
    ]
  },
  {
    "objectID": "modules/module-11.2.html#understanding-and-interpreting-odds-ratios",
    "href": "modules/module-11.2.html#understanding-and-interpreting-odds-ratios",
    "title": "Module 11.2",
    "section": "Understanding and Interpreting Odds Ratios",
    "text": "Understanding and Interpreting Odds Ratios\n\nOne common approach to making logistic regression coefficients more interpretable involves converting them to odds ratios. An odds ratio provides a standardized way to express how much the odds of an outcome change with a one-unit increase in a predictor variable.\nTo convert logistic regression coefficients to odds ratios, we simply exponentiate them. This mathematical transformation takes advantage of the logarithmic properties that underlie the logit function. When we exponentiate a log-odds coefficient, we obtain the multiplicative factor by which the odds change for each one-unit increase in the predictor variable.\nFor our conflict model, we can calculate the odds ratio for GDP per capita:\n\n# Extract coefficients and convert to odds ratios\ncoefficients &lt;- coef(conflict_model)\nodds_ratios &lt;- exp(coefficients)\n\n# Display both coefficients and odds ratios\nresults_table &lt;- data.frame(\n  Variable = names(coefficients),\n  Coefficient = round(coefficients, 3),\n  Odds_Ratio = round(odds_ratios, 3)\n)\n\nresults_table\n\n                     Variable Coefficient Odds_Ratio\n(Intercept)       (Intercept)      -1.007      0.365\nwbgdppc2011est wbgdppc2011est      -0.357      0.700\n\n\nThe odds ratio for log GDP per capita is approximately 0.718. This means that for each one-unit increase in log GDP per capita, the odds of conflict onset are multiplied by 0.718. Since this value is less than 1, it indicates that higher GDP per capita is associated with decreased odds of conflict onset.\nTo interpret this more intuitively, we can say that the odds of conflict onset decrease by approximately 28.2% (calculated as (1 - 0.718) × 100%) for each one-unit increase in log GDP per capita. This interpretation is much more meaningful than simply stating that the log-odds decrease by 0.33 units.\nOdds ratios follow consistent interpretation rules that make them particularly useful for communication. When the odds ratio equals 1, there is no association between the predictor and outcome. When the odds ratio is greater than 1, increases in the predictor are associated with increased odds of the outcome occurring. When the odds ratio is less than 1, increases in the predictor are associated with decreased odds of the outcome occurring. The further the odds ratio is from 1 in either direction, the stronger the association.\n\n\n\n\n\n\nYour Turn!\n\n\n\nNow it’s your turn to practice interpreting odds ratios with a different predictor variable. Using the conflict_df dataset, run a bivariate logistic regression model with ucdponset as the outcome variable and choose a different predictor from the dataset. Some options include:\n\n\nv2x_polyarchy (the V-Dem measure of democracy)\n\nethfrac (ethnic fractionalization)\n\nrelfrac (religious fractionalization)\n\nwbpopest (population estimate)\n\nAfter fitting your model, calculate the odds ratio for your chosen predictor and interpret the results. What does the odds ratio tell you about the relationship between your predictor and conflict onset? How would you explain this relationship to someone unfamiliar with statistical analysis?",
    "crumbs": [
      "Course Modules",
      "Module 11.2"
    ]
  },
  {
    "objectID": "modules/module-11.2.html#calculating-predicted-probabilities-and-synthesis",
    "href": "modules/module-11.2.html#calculating-predicted-probabilities-and-synthesis",
    "title": "Module 11.2",
    "section": "Calculating Predicted Probabilities and Synthesis",
    "text": "Calculating Predicted Probabilities and Synthesis\nWhile odds ratios help us understand relative effects, predicted probabilities allow us to make concrete predictions about specific scenarios. Let’s work through calculating predicted probabilities manually, then use automated tools to examine multiple scenarios and synthesize our complete understanding.\nFor a country with log GDP per capita of 9 (which corresponds to approximately $8,100 in actual GDP per capita, since e^9 ≈ 8,103), we can calculate the predicted probability step by step. Starting with our model equation:\n\\(\\log\\left(\\frac{p}{1-p}\\right) = -1.16 - 0.33 \\times 9 = -4.13\\)\nConverting to probability using the sigmoid function:\n\nlinear_predictor &lt;- -1.16 + (-0.33 * 9)\nprobability &lt;- 1 / (1 + exp(-linear_predictor))\nprint(paste(\"Probability for log GDP = 9:\", round(probability, 4)))\n\n[1] \"Probability for log GDP = 9: 0.0158\"\n\n\nThis manual approach becomes cumbersome for multiple scenarios, so we can use marginaleffects to examine how conflict probability changes across different GDP levels:\n\nlibrary(marginaleffects)\n\n# Calculate predictions for different log GDP levels\nprediction_data &lt;- data.frame(wbgdppc2011est = c(6, 7, 8, 9, 10))\npredictions &lt;- predictions(conflict_model, newdata = prediction_data)\n\n# Display results\ntidy(predictions) |&gt;\n  select(wbgdppc2011est, estimate, conf.low, conf.high) |&gt;\n  mutate(\n    Log_GDP = wbgdppc2011est,\n    GDP_Level = paste(\"~$\", round(exp(wbgdppc2011est)), sep = \"\"),\n    Probability = round(estimate, 4)\n  ) |&gt;\n  select(Log_GDP, GDP_Level, Probability)\n\n# A tibble: 5 × 3\n  Log_GDP GDP_Level Probability\n    &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n1       6 ~$403          0.0411\n2       7 ~$1097         0.0291\n3       8 ~$2981         0.0206\n4       9 ~$8103         0.0145\n5      10 ~$22026        0.0102\n\n\nThese results powerfully complement our odds ratio interpretation. The odds ratio of 0.718 told us that when log GDP per capita increases one unit, the odds of conflict decrease by 28.2%. The predicted probabilities show us the practical impact across different economic development levels: conflict probability is much higher for countries with lower log GDP values and decreases substantially as economic development increases. Together, these interpretations provide both standardized effect measures and concrete insights about how economic development relates to political stability.\n\n\n\n\n\n\nYour Turn!\n\n\n\nUsing the model you fitted in the previous exercise, calculate predicted probabilities for three different levels of your chosen predictor variable. You can work through the manual calculation or use marginaleffects.\nConsider how your predicted probabilities complement your odds ratio interpretation. What story do they tell together about the relationship between your predictor and conflict onset?",
    "crumbs": [
      "Course Modules",
      "Module 11.2"
    ]
  },
  {
    "objectID": "modules/module-11.2.html#summary-and-looking-ahead",
    "href": "modules/module-11.2.html#summary-and-looking-ahead",
    "title": "Module 11.2",
    "section": "Summary and Looking Ahead",
    "text": "Summary and Looking Ahead\nThis module has equipped you with essential tools for interpreting logistic regression results through three complementary approaches. We learned to convert log-odds coefficients to odds ratios, providing intuitive measures of relative effects that can be easily communicated to diverse audiences. We mastered the calculation of predicted probabilities both manually and using automated tools, enabling concrete predictions about specific scenarios. Most importantly, we discovered how to integrate these approaches into comprehensive analyses that effectively communicate our findings.\nThe bivariate focus of this module has allowed us to develop these interpretation skills thoroughly using clear, unambiguous examples. The conflict onset case study demonstrates how economic development relates to political stability, but the interpretation techniques apply broadly across disciplines and research questions.\nIn our next module, we will extend these interpretation skills to multiple predictor models, where we will encounter additional complexities such as controlling for confounding variables and modeling interaction effects. We will also explore how the presence of multiple predictors affects our interpretation of individual effects and learn to construct more nuanced analytical narratives. The solid foundation in bivariate interpretation that we have built here will prove invaluable as we tackle these more sophisticated modeling challenges.",
    "crumbs": [
      "Course Modules",
      "Module 11.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html",
    "href": "modules/module-9.2.html",
    "title": "Module 9.2",
    "section": "",
    "text": "Prework\n\n\n\nRun this code chunk to load the necessary packages and data for this module:\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(\n  indicators = c(\n  \"v2x_libdem\", \n  \"e_gdppc\", \n  \"v2cacamps\"),\n  start_year = 2019, \n  end_year = 2019\n  ) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps\n    ) |&gt;\n  filter(!is.na(lib_dem), !is.na(wealth))",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#overview",
    "href": "modules/module-9.2.html#overview",
    "title": "Module 9.2",
    "section": "Overview",
    "text": "Overview\nIn Module 4.1, you learned how to fit regression lines and interpret them. But how does R actually find the “best” line among all possible lines? This module dives into the mathematical optimization behind least squares regression. You’ll understand why it’s called “least squares” and develop intuition for the cost function that R minimizes when fitting your models.\nBy the end of this module, you’ll be able to: - Explain why we minimize the sum of squared residuals - Calculate and interpret a cost function - Understand the optimization process behind lm() - Connect mathematical theory to practical regression output",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#the-optimization-problem",
    "href": "modules/module-9.2.html#the-optimization-problem",
    "title": "Module 9.2",
    "section": "The Optimization Problem",
    "text": "The Optimization Problem\nWhen we fit a regression line in Module 4.1, we used R’s lm() function and got specific values for our intercept and slope. But think about it - there are infinitely many possible lines we could draw through any set of points. How does the computer choose which one is “best”?\nLet’s return to our democracy and GDP example from Module 4.1. We found that the relationship between log GDP per capita and democracy scores could be modeled as:\n\\[\\widehat{Democracy}_i = 0.13 + 0.12 \\times \\log(GDP)_i\\]\nBut why these specific numbers? Why not \\(\\widehat{Democracy}_i = 0.10 + 0.15 \\times \\log(GDP)_i\\) or any other combination?\n\n\n\n\n\n\n\n\nThe answer lies in a mathematical optimization problem. We want to find the line that makes the “best” predictions - the line that minimizes our prediction errors across all the data points.",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#understanding-the-cost-function",
    "href": "modules/module-9.2.html#understanding-the-cost-function",
    "title": "Module 9.2",
    "section": "Understanding the Cost Function",
    "text": "Understanding the Cost Function\nTo understand how we measure “best,” let’s watch Andrew Ng explain the intuition behind the cost function:\n\nAs Andrew explains, we need a way to measure how well our line fits the data. Remember from Module 4.1 that a residual is the difference between an actual value and our predicted value:\n\\[\\text{residual}_i = y_i - \\hat{y}_i\\]\nThe cost function (also called the loss function) measures the total error across all our predictions. For least squares regression, we use the sum of squared residuals (SSR):\n\\[\\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nBut why do we square the residuals instead of just adding them up? There are several good reasons.First, squaring the residuals ensures that the cost function is always positive, which is important for optimization. Squaring ensures that a prediction that’s too high (+2) is penalized the same as a prediction that’s too low (-2). Second, larger errors get bigger penalties. An error of 4 contributes 16 to the cost, while an error of 2 contributes only 4. Finally, for mathematical convenience. Squared functions have nice mathematical properties that make optimization easier.\n\n\n\n\n\n\nNote\n\n\n\nSome definitions of the cost function divide the SSR by the number of observations \\(n\\) (or two times the number of observations \\(2n\\)), yielding the mean squared error (MSE). This doesn’t change which line is best—it just rescales the cost.\n\n\nThe line that minimizes this cost function is our “best” line - the least squares regression line.\nLet’s build intuition with a very simple example. Consider these three data points: - (1, 1) - (2, 2) - (3, 3)\nWhat line would you draw through these points? Let’s test different lines and see which has the lowest cost.\n\n# Create our simple dataset\nsimple_data &lt;- tibble(\n  x = c(1, 2, 3),\n  y = c(1, 2, 3)\n)\n\nsimple_data\n\n# A tibble: 3 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n2     2     2\n3     3     3\n\n\n\n\n\n\n\n\n\n\nTest Line 1: \\(\\hat{y} = 0 + 1 \\times x\\) (intercept = 0, slope = 1)\nFor each point, let’s calculate the predicted value and residual: - Point (1,1): \\(\\hat{y} = 0 + 1(1) = 1\\), residual = \\(1 - 1 = 0\\) - Point (2,2): \\(\\hat{y} = 0 + 1(2) = 2\\), residual = \\(2 - 2 = 0\\)\n- Point (3,3): \\(\\hat{y} = 0 + 1(3) = 3\\), residual = \\(3 - 3 = 0\\)\nSum of squared residuals = \\(0^2 + 0^2 + 0^2 = 0\\)\nPerfect! This line goes exactly through all points.\n\n\n\n\n\n\n\n\nTest Line 2: \\(\\hat{y} = 0 + 2 \\times x\\) (intercept = 0, slope = 2)\nThis is a steeper line with a slope of 2. Let’s calculate the residuals:\n\nPoint (1,1): \\(\\hat{y} = 0 + 0(1) = 0\\), residual = \\(1 - 2 = -1\\)\n\nPoint (2,2): \\(\\hat{y} = 0 + 0(2) = 0\\), residual = \\(2 - 4 = -2\\)\n\nPoint (3,3): \\(\\hat{y} = 0 + 0(3) = 0\\), residual = \\(3 - 6 = -3\\)\n\n\nSum of squared residuals = \\(-1^2 + -2^2 + -3^2 = 1 + 4 + 9 = 14\\)\nMuch worse!\n\n\n\n\n\n\nYour Turn!!\n\n\n\nAssuming the same data points as in the above example, calculate the sum of squared residuals for the following lines:\n\n\n\\(\\hat{y} = 0 + 3 \\times x\\) (intercept = 0, slope = 3)\n\n\\(\\hat{y} = 0 + 0 \\times x\\) (intercept = 0, slope = 0)\n\n\\(\\hat{y} = 0 - 1 \\times x\\) (intercept = 0, slope = -1)\n\nChange the values in this interactive code chunk to perform your calculations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCheck your answers below when you are finished:\n\nCode# Calculate the sum of squared residuals for the line ŷ = 0 + 3x\n#ssr1 &lt;- (1-3)^2 + (2-6)^2 + (3-9)^2\n#ssr1\n# Answer: 56\n\n# Calculate the sum of squared residuals for the line ŷ = 0 + 0x\n#ssr2 &lt;- (1-0)^2 + (2-0)^2 + (3-0)^2\n#ssr2\n# Answer: 14\n\n# Calculate the sum of squared residuals for the line ŷ = 0 -1x\n#ssr3 &lt;- (1+1)^2 + (2+2)^2 + (3+3)^2\n#ssr3\n# Answer: 56",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#visualizing-the-optimization",
    "href": "modules/module-9.2.html#visualizing-the-optimization",
    "title": "Module 9.2",
    "section": "Visualizing the Optimization",
    "text": "Visualizing the Optimization\nLet’s see how the cost function behaves as we change the slope parameter. Andrew Ng provides excellent visualization of this concept:\n\nFor our simple three-point example, let’s use this Shiny app to plot the cost function for different slope values (keeping intercept = 0). Move the slide to choose a different slope. See how this changes the fit of the line relative to the points on the left, and how it affects the cost function on the right.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndata_points &lt;- data.frame(x = c(1, 2, 3), y = c(1, 2, 3))\n\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Cost Function Explorer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"slope\", \n                  \"Slope Parameter:\", \n                  min = -2, max = 4, value = 1, step = 0.1),\n      br(),\n      h4(\"Current Values:\"),\n      textOutput(\"current_slope\"),\n      textOutput(\"current_equation\"),\n      textOutput(\"current_ssr\"),\n      br(),\n      p(\"Move the slider to see how the slope affects:\"),\n      tags$ul(\n        tags$li(\"The regression line (left plot)\"),\n        tags$li(\"Your position on the cost function (right plot)\")\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"combined_plot\", height = \"400px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Fixed: Calculate predictions and residuals properly\n  current_calculations &lt;- reactive({\n    predictions &lt;- input$slope * data_points$x\n    residuals &lt;- data_points$y - predictions\n    ssr &lt;- sum(residuals^2)\n    list(predictions = predictions, residuals = residuals, ssr = ssr)\n  })\n  \n  cost_data &lt;- reactive({\n    slopes &lt;- seq(-2, 4, by = 0.1)\n    ssr_values &lt;- sapply(slopes, function(b) {\n      preds &lt;- b * data_points$x\n      resids &lt;- data_points$y - preds\n      sum(resids^2)\n    })\n    list(slopes = slopes, ssr = ssr_values)\n  })\n  \n  output$current_slope &lt;- renderText({\n    paste(\"Slope:\", round(input$slope, 2))\n  })\n  \n  output$current_equation &lt;- renderText({\n    paste0(\"Equation: Ŷ = 0 + \", round(input$slope, 2), \" * X\")\n  })\n  \n  # Fixed: Use the corrected reactive calculations\n  output$current_ssr &lt;- renderText({\n    calc &lt;- current_calculations()\n    ssr_terms &lt;- paste0(\"(\", data_points$y, \" - \", round(calc$predictions, 2), \")^2\", collapse = \" + \")\n    ssr_value &lt;- round(calc$ssr, 2)\n    paste0(\"SSR: \", ssr_terms, \" = \", ssr_value)\n  })\n  \n  output$combined_plot &lt;- renderPlot({\n    par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n    \n    # Left: Regression plot\n    plot(data_points$x, data_points$y, pch = 19, col = \"blue\",\n         xlim = c(0, 4), ylim = c(-2, 6),\n         xlab = \"x\", ylab = \"y\", main = \"Regression Line\")\n    abline(0, input$slope, col = \"red\", lwd = 2)\n    \n    # Add residual lines for visualization\n    calc &lt;- current_calculations()\n    segments(data_points$x, data_points$y, data_points$x, calc$predictions, \n             col = \"gray\", lty = 2)\n    \n    # Right: Cost function\n    cost &lt;- cost_data()\n    plot(cost$slopes, cost$ssr, type = \"l\", col = \"darkred\", lwd = 2,\n         xlab = \"Slope Parameter\", ylab = \"Sum of Squared Residuals\",\n         main = \"Cost Function\")\n    points(input$slope, calc$ssr, col = \"red\", pch = 19, cex = 1.5)\n  })\n}\n\nshinyApp(ui = ui, server = server)\nNotice that the cost function forms a parabola with its minimum at slope = 1. This is exactly where we found SSR = 0! The optimization problem is to find the slope (and intercept) that minimizes this cost function.",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#worked-example-democracy-and-gdp",
    "href": "modules/module-9.2.html#worked-example-democracy-and-gdp",
    "title": "Module 9.2",
    "section": "Worked Example: Democracy and GDP",
    "text": "Worked Example: Democracy and GDP\nLet’s apply this same thinking to our democracy and GDP data. We’ll manually test a few different potential regression lines and calculate their costs.\nFirst, let’s fit the actual least squares line to remind ourselves what R found:\n\n# Fit the model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Show the summary\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nThe least squares solution is approximately: \\(\\widehat{Democracy} = 0.13 + 0.12 \\times \\log(GDP)\\)\nLet’s test some alternative lines and see how they compare:\n\n\n\n\n\n\n\n\nNow let’s calculate the sum of squared residuals (SSR) for each of these lines to see which one has the lowest cost:\n\n\nLeast squares line (intercept = 0.13, slope = 0.12): SSR = 8.57\n\nSteeper slope line (intercept = 0.13, slope = 0.15): SSR = 9.58\n\nGentler slope line (intercept = 0.13, slope = 0.08): SSR = 10.49\n\nDifferent intercept line (intercept = 0.00, slope = 0.12): SSR = 11.58\n\nNotice how the least squares line has the lowest SSR! Any other combination of intercept and slope results in a higher cost, confirming that our optimization algorithm found the truly optimal solution.\nNotice how the least squares line has the lowest SSR! Any other combination of intercept and slope will result in a higher cost.",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#from-math-to-r-output",
    "href": "modules/module-9.2.html#from-math-to-r-output",
    "title": "Module 9.2",
    "section": "From Math to R Output",
    "text": "From Math to R Output\nWhen you run lm() in R, the computer is solving this optimization problem automatically. It searches through all possible combinations of intercept and slope values to find the ones that minimize the sum of squared residuals.\nFor simple linear regression, there’s actually a mathematical formula to find the optimal parameters directly (no searching required). But the key insight is that R is giving you the parameter values that make your predictions as accurate as possible, on average, across all your data points.\nThis is why we can trust the output from lm() - it’s not arbitrary, it’s the result of a principled mathematical optimization.\n\n# Our model from before\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n# The SSR for this model\nactual_ssr &lt;- sum(residuals(democracy_model)^2)\ncat(\"Sum of squared residuals for least squares line:\", round(actual_ssr, 3))\n\nSum of squared residuals for least squares line: 8.574\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nIn the last line of this code chunk we apply the cat() function to print the sum of squared residuals (SSR) for the least squares line. The cat() function is used to concatenate and print text and variables together in a single output. The round() function is applied to format the SSR value to three decimal places for better readability.\n\n\n\n\n\n\n\n\nYour Turn!\n\n\n\n\nTry running a regression with the polarization variable as the predictor of liberal democracy instead of wealth.\nCalculate the sum of squared residuals (SSR) for this new model.",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "modules/module-9.2.html#summary",
    "href": "modules/module-9.2.html#summary",
    "title": "Module 9.2",
    "section": "Summary",
    "text": "Summary\nThe “least squares” in least squares regression refers to the optimization principle: find the line that minimizes the sum of squared residuals. This mathematical framework ensures that: 1) your predictions are as accurate as possible on average across all data points; 2) the solution is unique in that there only one best line for any dataset; the method is principled (not arbitrary) because it is based on mathematical optimization; and 4) R’s output is trustworthy because lm() is finding the genuinely best-fitting line.\nUnderstanding this optimization principle helps you appreciate why regression works and gives you confidence in interpreting the results. When you see regression coefficients, you now know they represent the solution to a well-defined mathematical problem: finding the line that makes the best predictions for your data.",
    "crumbs": [
      "Course Modules",
      "Module 9.2"
    ]
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "index.qmd is the main schedule landing page, with supporting pages like course-syllabus.qmd and course-support.qmd.\nCore content lives in three parallel trees:\n\nmodules/ for detailed learning modules (module-X.Y.qmd).\nslides/ for Reveal.js decks (week-X.Y.qmd) and slides/custom.scss for styling.\nweeks/ for weekly overview pages.\n\nAssignments and final project materials are in assignments/ and project/.\nData lives in modules/data/ and slides/data/. Shared R helpers are in modules/functions/ and helper.R.\nGenerated output is in _site/ and cached renders in _freeze/ (do not hand-edit).\n\n\n\n\n\nquarto preview launches the live-reload dev server.\nquarto render builds the full site; quarto render path/to/file.qmd renders a single file.\nquarto render file.qmd --to html|pdf targets specific formats.\nquarto publish deploys to the configured destination.\n\n\n\n\n\nFollow existing .qmd structure and YAML headers; use the native R pipe (|&gt;) over %&gt;%.\nKeep R chunks quiet by default (message: false, warning: false) and prefer theme_minimal() in plots.\nNaming: module-X.Y.qmd, week-X.Y.qmd, and homework-N.qmd for assignments.\nNo formal formatter/linter is configured; match surrounding style and spacing.\n\n\n\n\n\nNo automated test framework is present. Validate changes by rendering affected files.\nFor slides, confirm Reveal.js decks render correctly and links resolve (quarto render slides/week-X.Y.qmd).\n\n\n\n\n\nRecent commit messages are short, imperative summaries (e.g., “Update syllabus”, “spring 2026 update”).\nKeep commits scoped to a single content change or update set.\nPRs should include a brief description, linked issues (if any), and screenshots or rendered output notes when visual changes are made.\n\n\n\n\n\n_quarto.yml defines site navigation and theming; update it when adding new pages.\nhelper.R controls semester start dates for schedule logic—update it when rolling to a new term."
  },
  {
    "objectID": "AGENTS.html#project-structure-module-organization",
    "href": "AGENTS.html#project-structure-module-organization",
    "title": "Repository Guidelines",
    "section": "",
    "text": "index.qmd is the main schedule landing page, with supporting pages like course-syllabus.qmd and course-support.qmd.\nCore content lives in three parallel trees:\n\nmodules/ for detailed learning modules (module-X.Y.qmd).\nslides/ for Reveal.js decks (week-X.Y.qmd) and slides/custom.scss for styling.\nweeks/ for weekly overview pages.\n\nAssignments and final project materials are in assignments/ and project/.\nData lives in modules/data/ and slides/data/. Shared R helpers are in modules/functions/ and helper.R.\nGenerated output is in _site/ and cached renders in _freeze/ (do not hand-edit)."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "quarto preview launches the live-reload dev server.\nquarto render builds the full site; quarto render path/to/file.qmd renders a single file.\nquarto render file.qmd --to html|pdf targets specific formats.\nquarto publish deploys to the configured destination."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Follow existing .qmd structure and YAML headers; use the native R pipe (|&gt;) over %&gt;%.\nKeep R chunks quiet by default (message: false, warning: false) and prefer theme_minimal() in plots.\nNaming: module-X.Y.qmd, week-X.Y.qmd, and homework-N.qmd for assignments.\nNo formal formatter/linter is configured; match surrounding style and spacing."
  },
  {
    "objectID": "AGENTS.html#testing-guidelines",
    "href": "AGENTS.html#testing-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "No automated test framework is present. Validate changes by rendering affected files.\nFor slides, confirm Reveal.js decks render correctly and links resolve (quarto render slides/week-X.Y.qmd)."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Recent commit messages are short, imperative summaries (e.g., “Update syllabus”, “spring 2026 update”).\nKeep commits scoped to a single content change or update set.\nPRs should include a brief description, linked issues (if any), and screenshots or rendered output notes when visual changes are made."
  },
  {
    "objectID": "AGENTS.html#configuration-content-tips",
    "href": "AGENTS.html#configuration-content-tips",
    "title": "Repository Guidelines",
    "section": "",
    "text": "_quarto.yml defines site navigation and theming; update it when adding new pages.\nhelper.R controls semester start dates for schedule logic—update it when rolling to a new term."
  },
  {
    "objectID": "project/project-datasets.html",
    "href": "project/project-datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Here are some datasets that you might consider using for your final project:\n\nfredr is an R package that provides access to the Federal Reserve Economic Data (FRED) API. FRED is a comprehensive database of economic data maintained by the Federal Reserve Bank of St. Louis. The package allows you to search for and download data from FRED directly into R.\nGoogle Public Data Explorer contains information about dozens of databases related to governance and the economy. You cannot download the raw data from Google, but you can use the site to visualize the data and then follow the link to the original source.\nILOSTAT is the statistical database of the International Labour Organisation. It has data pertaining to labor, working conditions, industrial relations, poverty and inequality.\nKaggle has data on just about anything you can think of. Very usable, clean data. Just stick to the social science stuff for your project. You can easily download CSV files from Kaggle but you can also access the data through the Kaggle API.\nOECD DATA provides data related to the performance of high income countries.\nOur World in Data is a good general resource for political economy data. The site is centered around blog posts but you can also search for a topic, view a visualization related to that topic and then download the data used to create it.\npeacesciencer is an R package maintained by Steve Miller that compiles data from a number of sources that are useful for peace and conflict studies analysis\nStatista is a good place to look for data on more niche topics.\nUNCTADstat is the United Nations Conference on Trade and Development statistical database. It provides harmonized data on a range of topics related to economic performance, trade and statistics.\nThe UN Human Development Reports include a number of important indicators related to human development, gender and sustainable development goals (SDGs).\nThe unvotes package provides data on United Nations General Assembly voting patterns.\nVarieties of Democracy (V-DEM) provides original measures of the quality of democracy for every country dating back to the 18th century. You can access vdem data through the vdemdata package.\nWorld Bank Development Indicators (WDI) is the primary World Bank database for development data from officially-recognized international sources. You can access WB Development Indicators through the WDI package or the wbstats package.\nThe World Bank DataBank provides access to dozens of additional World Bank databases on topics such as regional development, governance, education, gender and the environment. You can access world bank data through the wbstats package.\n\nFor information on more specific resources available, see this page on the Gelman Library website.",
    "crumbs": [
      "Final Project",
      "Datasets"
    ]
  },
  {
    "objectID": "slides/week-6.1.html#sampling",
    "href": "slides/week-6.1.html#sampling",
    "title": "Sampling",
    "section": "Sampling",
    "text": "Sampling\n\nSampling the act of selecting a subset of individuals, items, or data points from a larger population to estimate characteristics or metrics of the entire population\nVersus a census, which involves gathering information on every individual in the population\nWhy would you want to use a sample?\n\nToday’s code is HERE"
  },
  {
    "objectID": "slides/week-6.1.html#what-proportion-of-all-milk-chocolate-mms-are-blue",
    "href": "slides/week-6.1.html#what-proportion-of-all-milk-chocolate-mms-are-blue",
    "title": "Sampling",
    "section": "What proportion of all milk chocolate M&Ms are blue?",
    "text": "What proportion of all milk chocolate M&Ms are blue?\n\nM&Ms has a precise distribution of colors that it produces in its factories\nM&Ms are sorted into bags in factories in a fairly random process\nEach bag represents a sample from the full population of M&Ms"
  },
  {
    "objectID": "slides/week-6.1.html#activity",
    "href": "slides/week-6.1.html#activity",
    "title": "Sampling",
    "section": "Activity",
    "text": "Activity\n\nGet in groups of 2. Each group will have 4-5 bags of M&Ms.\nKeep the contents of each bag separate, and do not eat (yet!)\nOpen up your first bag of M&Ms: calculate the proportion of the M&Ms that are blue. Write this down. What is your best guess (your estimate) for the proportion of all M&Ms that are blue?"
  },
  {
    "objectID": "slides/week-6.1.html#activity-1",
    "href": "slides/week-6.1.html#activity-1",
    "title": "Sampling",
    "section": "Activity",
    "text": "Activity\n\nDo the same as above for the rest of your bags (you should have 4-5 estimates written down)\nDraw a histogram of your estimates (by hand)\nAdd your estimates to this Google Sheet\nAdd your estimates to the class histogram on the whiteboard"
  },
  {
    "objectID": "slides/week-6.1.html#lets-analyze-the-data",
    "href": "slides/week-6.1.html#lets-analyze-the-data",
    "title": "Sampling",
    "section": "Let’s Analyze the Data",
    "text": "Let’s Analyze the Data\n\nWe will use the googlesheets4 package to pull the data into R so be sure to install it.\n\n#install.packages(\"googlesheets4\")\n\nlibrary(tidyverse)\nlibrary(googlesheets4)\n\ngs4_deauth() # to signify no authorization required\n\nmnm_data &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/136wGKZOnwOdo3O-4bUfF_TWKBCAWQUCyiVKSY4HyYAw/edit#gid=0\")\n\nglimpse(mnm_data)"
  },
  {
    "objectID": "slides/week-6.1.html#calculate-some-summary-stats",
    "href": "slides/week-6.1.html#calculate-some-summary-stats",
    "title": "Sampling",
    "section": "Calculate Some Summary Stats",
    "text": "Calculate Some Summary Stats\n\n\nmnm_data |&gt;\n  summarize(\n    mean_blue = mean(proportion_blue),\n    median_blue = median(proportion_blue),\n    sd_blue = sd(proportion_blue)\n  )"
  },
  {
    "objectID": "slides/week-6.1.html#now-lets-make-a-histogram",
    "href": "slides/week-6.1.html#now-lets-make-a-histogram",
    "title": "Sampling",
    "section": "Now Let’s Make a Histogram",
    "text": "Now Let’s Make a Histogram\n\n\nggplot(mnm_data, aes(x = proportion_blue)) +\n  geom_histogram(fill = \"steelblue\") +\n  labs(\n    title = \"Percentage of Blue M&Ms\",\n    x = \"Proportion Blue\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "slides/week-6.1.html#discuss-with-neighbor",
    "href": "slides/week-6.1.html#discuss-with-neighbor",
    "title": "Sampling",
    "section": "Discuss with Neighbor",
    "text": "Discuss with Neighbor\n\nWhat is the histogram/distribution showing?\nBased on the histogram on the board, what is your answer to the question of what proportion of all milk chocolate M&Ms are blue? Why do you give that answer?\nWhy do some bag of M&Ms have proportions of blues that are higher and lower than the number you gave above?\nHow do our estimates relate to the actual percentage of blue M&Ms manufactured (ask Google or ChatGPT)"
  },
  {
    "objectID": "slides/week-6.1.html#what-did-we-just-do",
    "href": "slides/week-6.1.html#what-did-we-just-do",
    "title": "Sampling",
    "section": "What did we just do?",
    "text": "What did we just do?\n\nWe wanted to say something about the population of M&Ms\nThe parameter we care about is the proportion of M&Ms that are blue\nIt would be impossible to conduct a census and to calculate the parameter\nWe took a sample from the population and calculated a sample statistic\nstatistical inference: act of making a guess about a population using information from a sample"
  },
  {
    "objectID": "slides/week-6.1.html#what-have-we-just-done",
    "href": "slides/week-6.1.html#what-have-we-just-done",
    "title": "Sampling",
    "section": "What have we just done?",
    "text": "What have we just done?\n\nWe completed this task many times\nThis produced a sampling distribution of our estimates\nThere is a distribution of estimates because of sampling variability\nDue to random chance, one estimate from one sample can differ from another\nThese are foundational ideas for statistical inference that we are going to keep building on"
  },
  {
    "objectID": "slides/week-6.1.html#target-population",
    "href": "slides/week-6.1.html#target-population",
    "title": "Sampling",
    "section": "Target Population",
    "text": "Target Population\nIn data analysis, we are usually interested in saying something about a Target Population.\n\nWhat proportion of adult Russians support the war in Ukraine?\n\nTarget population: adult Russians (age 18+)\n\nHow many US college students check social media during their classes?\n\nTarget population: US college students\n\nWhat percentage of M&Ms are blue?\n\nTarget population: all of the M&Ms"
  },
  {
    "objectID": "slides/week-6.1.html#sample",
    "href": "slides/week-6.1.html#sample",
    "title": "Sampling",
    "section": "Sample",
    "text": "Sample\n\nIn many instances, we have a Sample\n\nWe cannot talk to every Russian\nWe cannot talk to all college students\nWe cannot count all of the M&Ms"
  },
  {
    "objectID": "slides/week-6.1.html#parameters-vs-statistics",
    "href": "slides/week-6.1.html#parameters-vs-statistics",
    "title": "Sampling",
    "section": "Parameters vs Statistics",
    "text": "Parameters vs Statistics\n\n\nThe parameter is the value of a calculation for the entire target population\nThe statistic is what we calculate on our sample\n\nWe calculate a statistic in order to say something about the parameter"
  },
  {
    "objectID": "slides/week-6.1.html#inference",
    "href": "slides/week-6.1.html#inference",
    "title": "Sampling",
    "section": "Inference",
    "text": "Inference\n\n\nInference–The act of “making a guess” about some unknown\nStatistical inference–Making a good guess about a population from a sample\nCausal inference–Did X cause Y? [topic for later classes]"
  },
  {
    "objectID": "slides/week-4.2.html#publish-html-to-quarto-pub",
    "href": "slides/week-4.2.html#publish-html-to-quarto-pub",
    "title": "Exercise 1",
    "section": "Publish HTML to Quarto Pub",
    "text": "Publish HTML to Quarto Pub\n\nGo to quartopub.com\nSet up Quarto pub account\nLook at guide for pubishing on Quarto pub\nWith your Quarto pub account open:\n\nGo to terminal in RStudio\nType quarto publish quarto pub"
  },
  {
    "objectID": "slides/week-4.2.html#making-pdfs",
    "href": "slides/week-4.2.html#making-pdfs",
    "title": "Exercise 1",
    "section": "Making PDFs",
    "text": "Making PDFs\n\nInstall tinytex Type quarto install tinytex in terminal\nChange document type in YAML header to pdf\nLook at Quarto Reference and play with options"
  },
  {
    "objectID": "slides/week-14.2.html#what-is-reveal.js",
    "href": "slides/week-14.2.html#what-is-reveal.js",
    "title": "Presenting in Quarto",
    "section": "What is Reveal.js?",
    "text": "What is Reveal.js?\n\nWeb-based presentation framework that enables the creation of interactive presentations using HTML\nSupports dynamic transitions, embedded media, and interactive elements\nWorks across devices and platforms, ensuring your presentation looks great everywhere\nOffers a wide range of plugins and extensions to enhance your presentations, from analytics to themes\nShare your presentations as a URL, making it accessible to anyone, anywhere"
  },
  {
    "objectID": "slides/week-14.2.html#why-revealjs",
    "href": "slides/week-14.2.html#why-revealjs",
    "title": "Presenting in Quarto",
    "section": "Why Revealjs?",
    "text": "Why Revealjs?\n\nYou can also make PowerPoint and Beamer (pdf) slides with Quarto\nRevealjs is more fun and has some key advantages\n\nIt’s web-based and interactive\nIt’s more accessible\nIt’s easy to share\n\nBut sometimes other formats can be useful\n\nCan you think of some scenarios where you might want to use PowerPoint or Beamer?"
  },
  {
    "objectID": "slides/week-14.2.html#setup",
    "href": "slides/week-14.2.html#setup",
    "title": "Presenting in Quarto",
    "section": "Setup",
    "text": "Setup\n\nSetting up a reveal.js presentation in Quarto is easy. You just specify the revealjs format. From there you can use the usual YAML arguments like title:, subtitle:, etc. that you would use in any Quarto document. Here’s a simple example:\n\n---\ntitle: \"Revealjs Presentation\"\nsubtitle: \"For Demonstration Purposes\"\nauthor: \"Your Name\"\ndate: today\nformat: revealjs\n---"
  },
  {
    "objectID": "slides/week-14.2.html#creating-slides",
    "href": "slides/week-14.2.html#creating-slides",
    "title": "Presenting in Quarto",
    "section": "Creating Slides",
    "text": "Creating Slides\n\nSlides are created using the standard markdown syntax. # gives you a new section, while ## gives you a new slide. Then you can write text and use - for bullet points.\n# Section Header\n\n## Slide 1\n\nThis is the first slide.\n\n- Bullet 1\n- Bullet 2\n- Bullet 3"
  },
  {
    "objectID": "slides/week-14.2.html#spacing",
    "href": "slides/week-14.2.html#spacing",
    "title": "Presenting in Quarto",
    "section": "Spacing",
    "text": "Spacing\n\nSometimes you want to have more spacing than the default spacing between lines on each slide.\nThe easiest way to handle this is to insert &lt;br&gt; tags where you want the extra space.\n\n## Slide 2\n\n&lt;br&gt;\n\nThis is the content for my second slide. It is going to have this line and then a line break and then some code. \n\n&lt;br&gt;\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n[1] 4\n\n\n:::\n:::"
  },
  {
    "objectID": "slides/week-14.2.html#section",
    "href": "slides/week-14.2.html#section",
    "title": "Presenting in Quarto",
    "section": "",
    "text": "If perchance you don’t want a title slide, just eliminate title, subtitle, etc. from the YAML header.\n\n---\nformat: revealjs\n---"
  },
  {
    "objectID": "slides/week-14.2.html#incremental-lists",
    "href": "slides/week-14.2.html#incremental-lists",
    "title": "Presenting in Quarto",
    "section": "Incremental Lists",
    "text": "Incremental Lists\n\nThere are two ways to get incremental lists. The first is to specify incremental: true in the YAML header.\n\n---\nformat: \n  revealjs:\n    incremental: true\n---"
  },
  {
    "objectID": "slides/week-14.2.html#incremental-lists-1",
    "href": "slides/week-14.2.html#incremental-lists-1",
    "title": "Presenting in Quarto",
    "section": "Incremental Lists",
    "text": "Incremental Lists\n\nThe other is to surround the relevant bullet points in a div with the class incremental.\n\n::: {.incremental}\n- Bullet 1\n- Bullet 2\n- Bullet 3\n:::"
  },
  {
    "objectID": "slides/week-14.2.html#incremental-lists-2",
    "href": "slides/week-14.2.html#incremental-lists-2",
    "title": "Presenting in Quarto",
    "section": "Incremental Lists",
    "text": "Incremental Lists\n\nOr, let’s say you have incremental: true in the YAML header, but you want to turn it off for a particular slide. In this case, you can use nonincremental.\n::: {.nonincremental}\n- Bullet 1\n- Bullet 2\n- Bullet 3\n:::"
  },
  {
    "objectID": "slides/week-14.2.html#your-turn",
    "href": "slides/week-14.2.html#your-turn",
    "title": "Presenting in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nStart a new Quarto project\nCreate a new Quarto document in your project folder\nAdd the YAML header and specify the revealjs format\nAdd slides and sections\nUse the incremental class to create incremental lists\nNow try setting incremental: true in the YAML header\nUse nonincremental to turn off incremental lists for a particular slide\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-14.2.html#content-overflow",
    "href": "slides/week-14.2.html#content-overflow",
    "title": "Presenting in Quarto",
    "section": "Content Overflow",
    "text": "Content Overflow\n\nSometimes you have too much material to fit on a slide. In this case, you can use the .smaller or .scrollable class. You can use curly braces to add these to a particular slide…\n\n## Slide Title {.smaller}\n\n## Slide Title {.scrollable}"
  },
  {
    "objectID": "slides/week-14.2.html#content-overflow-1",
    "href": "slides/week-14.2.html#content-overflow-1",
    "title": "Presenting in Quarto",
    "section": "Content Overflow",
    "text": "Content Overflow\n\nOr you can add them to the YAML header to apply them to the entire presentation…\n\n---\nformat:\n  revealjs:\n    smaller: true\n    scrollable: true\n---"
  },
  {
    "objectID": "slides/week-14.2.html#adding-images",
    "href": "slides/week-14.2.html#adding-images",
    "title": "Presenting in Quarto",
    "section": "Adding Images",
    "text": "Adding Images\n\nTo add an image, you can use the standard markdown syntax.\n\n![](images/your-image.png)"
  },
  {
    "objectID": "slides/week-14.2.html#adding-images-1",
    "href": "slides/week-14.2.html#adding-images-1",
    "title": "Presenting in Quarto",
    "section": "Adding Images",
    "text": "Adding Images\n\nTo control the width of the image, you can use the width attribute.\n\n![](images/your-image.png){width=50%}"
  },
  {
    "objectID": "slides/week-14.2.html#columns",
    "href": "slides/week-14.2.html#columns",
    "title": "Presenting in Quarto",
    "section": "Columns",
    "text": "Columns\n\nTo put content in columns, you can create a div with the columns class.\n:::: {.columns}\n\n::: {.column width=\"40%\"}\nLeft column\n:::\n\n::: {.column width=\"60%\"}\nRight column\n:::\n\n::::"
  },
  {
    "objectID": "slides/week-14.2.html#your-turn-1",
    "href": "slides/week-14.2.html#your-turn-1",
    "title": "Presenting in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nTry making a long slide with the .smaller class\nNow use the .scrollable class instead\nAdd an image to a slide\n\nGo to Wikimedia Commons and download an image\nAdd the image to your project folder\n\nMake a slide with two columns\n\nOn the left, write a few bullet points\nOn the right, add an image\nUse the width attribute to control the size of the image\n\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-14.2.html#code-blocks",
    "href": "slides/week-14.2.html#code-blocks",
    "title": "Presenting in Quarto",
    "section": "Code Blocks",
    "text": "Code Blocks\n\n\nTo add a code block, use the standard markdown syntax\nYou can specify the language for syntax highlighting\nJust like a normal HTML document, you can add chunk options"
  },
  {
    "objectID": "slides/week-14.2.html#code-blocks-1",
    "href": "slides/week-14.2.html#code-blocks-1",
    "title": "Presenting in Quarto",
    "section": "Code Blocks",
    "text": "Code Blocks\n\n\n```{r}\n#| label: leaflet_map1\n#| eval: false\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")\n```"
  },
  {
    "objectID": "slides/week-14.2.html#section-1",
    "href": "slides/week-14.2.html#section-1",
    "title": "Presenting in Quarto",
    "section": "",
    "text": "If your echo: is set to false, then you will just see the output.\n\n\n\n\n\n\n\n\nNote how the output of the leaflet is interactive!"
  },
  {
    "objectID": "slides/week-14.2.html#section-2",
    "href": "slides/week-14.2.html#section-2",
    "title": "Presenting in Quarto",
    "section": "",
    "text": "But if echo: is set to true, then you will see the code and the output…\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")"
  },
  {
    "objectID": "slides/week-14.2.html#section-3",
    "href": "slides/week-14.2.html#section-3",
    "title": "Presenting in Quarto",
    "section": "",
    "text": "And just like in a normal HTML document, you can also set these options in the YAML header.\n\nexecute:\n  echo: false\n  message: false\n  warning: false"
  },
  {
    "objectID": "slides/week-14.2.html#section-4",
    "href": "slides/week-14.2.html#section-4",
    "title": "Presenting in Quarto",
    "section": "",
    "text": "For presentation purposes, you may oly want to show specific lines of code.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")"
  },
  {
    "objectID": "slides/week-14.2.html#section-5",
    "href": "slides/week-14.2.html#section-5",
    "title": "Presenting in Quarto",
    "section": "",
    "text": "To do this, you would use the code-line-numbers option in the YAML header.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")"
  },
  {
    "objectID": "slides/week-14.2.html#your-turn-2",
    "href": "slides/week-14.2.html#your-turn-2",
    "title": "Presenting in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nStart a new slide\nAdd an R code chunk to it\nAdd a leaflet map to it\nRender the slide\nTry different chunk options\n\nDisplay only the output\nDisplay code and output\nDisplay just the code\nDisplay only particular lines\n\nTry adjusting evaluate options in the YAML header\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-14.2.html#themes",
    "href": "slides/week-14.2.html#themes",
    "title": "Presenting in Quarto",
    "section": "Themes",
    "text": "Themes\n\nYou can customize the look of your slides by using a different theme, e.g. \n\ntheme: dark\n\n\nSee the Reveal documentation for the full list of availalbe themes."
  },
  {
    "objectID": "slides/week-14.2.html#themes-1",
    "href": "slides/week-14.2.html#themes-1",
    "title": "Presenting in Quarto",
    "section": "Themes",
    "text": "Themes\n\nYou can also add a custom SCSS file to tweak an existing them or create your own:\n/*-- scss:defaults --*/\n\n$body-bg: #191919;\n$body-color: #fff;\n$link-color: #42affa;\n\n/*-- scss:rules --*/\n\n.reveal .slide blockquote {\n  border-left: 3px solid $text-muted;\n  padding-left: 0.5em;\n}"
  },
  {
    "objectID": "slides/week-14.2.html#syntax-highlighting",
    "href": "slides/week-14.2.html#syntax-highlighting",
    "title": "Presenting in Quarto",
    "section": "Syntax Highlighting",
    "text": "Syntax Highlighting\n\nQuarto offers 20 different syntax highlighting themes. Click here to see the available themes.\n\nYou can select your preferred theme by adding highlight-style to the YAML header, e.g. \n\nhighlight-style: github"
  },
  {
    "objectID": "slides/week-14.2.html#background-styling",
    "href": "slides/week-14.2.html#background-styling",
    "title": "Presenting in Quarto",
    "section": "Background Styling",
    "text": "Background Styling\n\nYou can change the color of your background by adding the background-color attribute to a slide.\n\n## Slide Title {background-color=\"aquamarine\"}"
  },
  {
    "objectID": "slides/week-14.2.html#background-styling-1",
    "href": "slides/week-14.2.html#background-styling-1",
    "title": "Presenting in Quarto",
    "section": "Background Styling",
    "text": "Background Styling\n\nYou can change the color of your background by adding the background-color attribute to a slide.\n\n## Slide Title {background-color=\"aquamarine\"}"
  },
  {
    "objectID": "slides/week-14.2.html#background-styling-2",
    "href": "slides/week-14.2.html#background-styling-2",
    "title": "Presenting in Quarto",
    "section": "Background Styling",
    "text": "Background Styling\n\nSimilarly, you can add a background image to your slide by adding the background-image attribute.\n\n## Slide Title {background-image=\"/images/drr6502-img.png\" data-background-size=\"contain\" data-background-opacity=\"0.5\"}"
  },
  {
    "objectID": "slides/week-14.2.html#background-styling-3",
    "href": "slides/week-14.2.html#background-styling-3",
    "title": "Presenting in Quarto",
    "section": "Background Styling",
    "text": "Background Styling\n\nSimilarly, you can add a background image to your slide by adding the background-image attribute.\n\n## Slide Title {background-image=\"/images/drr6502-img.png\" data-background-size=\"contain\" data-background-opacity=\"0.5\"}"
  },
  {
    "objectID": "slides/week-14.2.html#background-styling-4",
    "href": "slides/week-14.2.html#background-styling-4",
    "title": "Presenting in Quarto",
    "section": "Background Styling",
    "text": "Background Styling\n\nAnd you can add a background image to the title slide by adding the title-slide-attributes attribute to the YAML header.\n\n\n---\ntitle: My Slide Show\ntitle-slide-attributes:\n    data-background-image: /path/to/title_image.png\n    data-background-size: contain\n    data-background-opacity: \"0.5\"\n---\n\nSee here for an example."
  },
  {
    "objectID": "slides/week-14.2.html#a-few-more-tricks-and-tips",
    "href": "slides/week-14.2.html#a-few-more-tricks-and-tips",
    "title": "Presenting in Quarto",
    "section": "A Few More Tricks and Tips",
    "text": "A Few More Tricks and Tips\n\nFade your slide transitions with transition: fade\nAdd slide numbers with slide-number: true\nUse footer:to add a footer your slides\nUse logo: to add a logo to your slides\nYou can add a chalkboard to your slides by adding chalkboard: true to the YAML header\nYou can add speaker notes by creating a div and adding the notes attribute to it.\n\nThen you can view them by running the presentation in speaker mode\n\nYou can use multiplex: true to advance slides for your audience\nAnd much much more! Check out the guide for details\n\n\nThese are my speaker notes!"
  },
  {
    "objectID": "slides/week-14.2.html#my-yaml-for-this-presentation",
    "href": "slides/week-14.2.html#my-yaml-for-this-presentation",
    "title": "Presenting in Quarto",
    "section": "My YAML for This Presentation",
    "text": "My YAML for This Presentation\n ---\ntitle: \"Revealjs Slides\"\nsubtitle: \"Session 4--Visualizing Data\"\nfooter: \"[DRR Website](https://quarto.training)\"\nlogo: images/drr6502-logo.png\nformat:\n  revealjs:\n    theme: [simple, custom.scss]\n    transition: fade\n    slide-number: true\n    chalkboard: true\nexecute:\n  echo: false\n  message: false\n  warning: false\n  freeze: auto\n---"
  },
  {
    "objectID": "slides/week-14.2.html#your-turn-3",
    "href": "slides/week-14.2.html#your-turn-3",
    "title": "Presenting in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nAdd a theme to your presentation\nTry a different type of syntax highlighting\nChange the background color of a slide\nAdd a background image to a slide\nAdd a background image to the title slide\nTry using SCSS to modify the theme style\nTry a cool trick like chalkboard or multiplex\nUpload your presentation to Quarto Pub\n\nquarto publish quarto-pub mydocument.qmd"
  },
  {
    "objectID": "slides/week-12.1.html#peacesciencer-package",
    "href": "slides/week-12.1.html#peacesciencer-package",
    "title": "DIY Conflict Model",
    "section": "peacesciencer package",
    "text": "peacesciencer package\n\nlibrary(peacesciencer)\n\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()\n\nglimpse(conflict_df)\n\nRows: 7,036\nColumns: 20\n$ gwcode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ statename      &lt;chr&gt; \"United States of America\", \"United States of America\",…\n$ year           &lt;dbl&gt; 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1…\n$ ucdpongoing    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ucdponset      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ maxintensity   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ conflict_ids   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ v2x_polyarchy  &lt;dbl&gt; 0.605, 0.587, 0.599, 0.599, 0.587, 0.602, 0.601, 0.594,…\n$ polity2        &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ xm_qudsest     &lt;dbl&gt; 1.259180, 1.259180, 1.252190, 1.252190, 1.270106, 1.259…\n$ ethfrac        &lt;dbl&gt; 0.2226323, 0.2248701, 0.2271561, 0.2294918, 0.2318781, …\n$ ethpol         &lt;dbl&gt; 0.4152487, 0.4186156, 0.4220368, 0.4255134, 0.4290458, …\n$ relfrac        &lt;dbl&gt; 0.4980802, 0.5009111, 0.5037278, 0.5065309, 0.5093204, …\n$ relpol         &lt;dbl&gt; 0.7769888, 0.7770017, 0.7770303, 0.7770729, 0.7771274, …\n$ wbgdp2011est   &lt;dbl&gt; 28.539, 28.519, 28.545, 28.534, 28.572, 28.635, 28.669,…\n$ wbpopest       &lt;dbl&gt; 18.744, 18.756, 18.781, 18.804, 18.821, 18.832, 18.848,…\n$ sdpest         &lt;dbl&gt; 28.478, 28.456, 28.483, 28.469, 28.510, 28.576, 28.611,…\n$ wbgdppc2011est &lt;dbl&gt; 9.794, 9.762, 9.764, 9.730, 9.752, 9.803, 9.821, 9.857,…\n$ rugged         &lt;dbl&gt; 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073,…\n$ newlmtnest     &lt;dbl&gt; 3.214868, 3.214868, 3.214868, 3.214868, 3.214868, 3.214…"
  },
  {
    "objectID": "slides/week-12.1.html#your-turn",
    "href": "slides/week-12.1.html#your-turn",
    "title": "DIY Conflict Model",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nInstall/load the peacesciencer package\nVisit the package website\nCreate a conflict dataset based on the Gleditsch-Ward system of state-years\nFilter the dataset to include only years between 1946 and 1999\nAdd UCDP/PRIO Armed Conflict Dataset (ACD) data on intrastate wars\nAdd sets of variables and glimpse() the dataset after each set\n\nDemocracy variables\nFractionalization variables\nGDP variables\nRugged terrain variables\n\nOthers?\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-12.1.html#replicate-fearon-and-laitin-2003",
    "href": "slides/week-12.1.html#replicate-fearon-and-laitin-2003",
    "title": "DIY Conflict Model",
    "section": "Replicate Fearon and Laitin (2003)",
    "text": "Replicate Fearon and Laitin (2003)"
  },
  {
    "objectID": "slides/week-12.1.html#section",
    "href": "slides/week-12.1.html#section",
    "title": "DIY Conflict Model",
    "section": "",
    "text": "One way to do it:\n\nconflict_model &lt;- glm(ucdponset ~ ethfrac + relfrac + v2x_polyarchy + rugged +\n                        wbgdppc2011est + wbpopest,\n                  data = conflict_df,\n                  family = \"binomial\")\n\nsummary(conflict_model)\n\n\nCall:\nglm(formula = ucdponset ~ ethfrac + relfrac + v2x_polyarchy + \n    rugged + wbgdppc2011est + wbpopest, family = \"binomial\", \n    data = conflict_df)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -5.69260    1.40788  -4.043 5.27e-05 ***\nethfrac         0.80005    0.38072   2.101  0.03560 *  \nrelfrac        -0.39138    0.41673  -0.939  0.34764    \nv2x_polyarchy  -0.60161    0.50879  -1.182  0.23704    \nrugged          0.06413    0.07603   0.843  0.39897    \nwbgdppc2011est -0.37188    0.12059  -3.084  0.00204 ** \nwbpopest        0.29318    0.06735   4.353 1.34e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.5  on 6150  degrees of freedom\nResidual deviance: 1126.7  on 6144  degrees of freedom\n  (885 observations deleted due to missingness)\nAIC: 1140.7\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "slides/week-12.1.html#your-turn-1",
    "href": "slides/week-12.1.html#your-turn-1",
    "title": "DIY Conflict Model",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nApproximate the Fearon and Laitin (2003) model\nUse the glm() function to run the model\nAdd measures of ethnicity, democracy, terrain, population and wealth\nTry replacing the original measures with different ones of the same concept\n\nWhat happens?\n\nWhat else could you add?\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-12.1.html#section-1",
    "href": "slides/week-12.1.html#section-1",
    "title": "DIY Conflict Model",
    "section": "",
    "text": "So we can use ggplot to make a coefficient plot instead…"
  },
  {
    "objectID": "slides/week-12.1.html#section-2",
    "href": "slides/week-12.1.html#section-2",
    "title": "DIY Conflict Model",
    "section": "",
    "text": "Two steps… First, we need a coeficient map:\n\ncoef_map &lt;- c(\"ethfrac\" = \"Ethnic Frac\",  # map coefficients\n        \"relfrac\" = \"Religions Frac\",     #(change names and order)\n        \"v2x_polyarchy\" = \"Polyarchy\",\n        \"rugged\" = \"Terrain\",\n        \"wbgdppc2011est\" = \"Per capita GDP\",\n        \"wbpopest\" = \"Population\",\n        \"(Intercept)\" = \"Intercept\")"
  },
  {
    "objectID": "slides/week-12.1.html#section-3",
    "href": "slides/week-12.1.html#section-3",
    "title": "DIY Conflict Model",
    "section": "",
    "text": "Then, we can use modelplot to visualize the model:\n\nlibrary(modelsummary)\nlibrary(ggplot2)\n\nmodelplot(conflict_model, \n          coef_map = rev(coef_map), # rev() reverses list order\n          coef_omit = \"Intercept\", \n          color = \"blue\") + # use plus to add customizations like any ggplot object\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = .75) + # red 0 line\n  labs(\n    title = \"Figure 1: Predictors of Conflict Onset\",\n    caption = \"See appendix for data sources.\"\n  )"
  },
  {
    "objectID": "slides/week-12.1.html#your-turn-2",
    "href": "slides/week-12.1.html#your-turn-2",
    "title": "DIY Conflict Model",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nCreate a coefficient map for your model\nUse modelplot to visualize your model\nDon’t forget to add the title and caption\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-12.1.html#exponentiate-the-coefficients",
    "href": "slides/week-12.1.html#exponentiate-the-coefficients",
    "title": "DIY Conflict Model",
    "section": "Exponentiate the Coefficients",
    "text": "Exponentiate the Coefficients"
  },
  {
    "objectID": "slides/week-12.1.html#exponentiate-the-coefficients-1",
    "href": "slides/week-12.1.html#exponentiate-the-coefficients-1",
    "title": "DIY Conflict Model",
    "section": "Exponentiate the Coefficients",
    "text": "Exponentiate the Coefficients\n\nmodelplot(conflict_model, \n          exponentiate = TRUE, # exponentiate coefficients\n          coef_map = rev(coef_map), # rev() reverses list order\n          coef_omit = \"Intercept\", \n          color = \"blue\") + # use plus to add customizations like any ggplot object\n  geom_vline(xintercept = 1, color = \"red\", linetype = \"dashed\", linewidth = .75) + # red 0 line\n  labs(\n    title = \"Figure 1: Predictors of Conflict Onset\",\n    caption = \"See appendix for data sources.\"\n  )"
  },
  {
    "objectID": "slides/week-12.1.html#your-turn-3",
    "href": "slides/week-12.1.html#your-turn-3",
    "title": "DIY Conflict Model",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nExponentiate the coefficients in your model\nDo this by setting exponentiate = TRUE in modelplot\nInterpret the results\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-12.1.html#using-marginaleffects",
    "href": "slides/week-12.1.html#using-marginaleffects",
    "title": "DIY Conflict Model",
    "section": "Using marginaleffects",
    "text": "Using marginaleffects\n\n# load the marginaleffects library\nlibrary(marginaleffects)\n\n# select some countries for a given year\nselected_countries &lt;- conflict_df |&gt;\n  filter(\n    statename %in% c(\"United States of America\", \"Venezuela\", \"Rwanda\"),\n    year == 1999)\n\n# calculate margins for the subset\nmarg_effects &lt;- predictions(conflict_model, newdata = selected_countries)\n\n# tidy the results\ntidy(marg_effects) |&gt;\n  select(estimate, p.value, conf.low, conf.high, statename)"
  },
  {
    "objectID": "slides/week-12.1.html#using-marginaleffects-1",
    "href": "slides/week-12.1.html#using-marginaleffects-1",
    "title": "DIY Conflict Model",
    "section": "Using marginaleffects",
    "text": "Using marginaleffects\n\n\n\n# A tibble: 3 × 5\n  estimate  p.value conf.low conf.high statename               \n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                   \n1   0.0123 2.23e-28  0.00567    0.0264 United States of America\n2   0.0140 2.25e-70  0.00879    0.0222 Venezuela               \n3   0.0286 8.46e-39  0.0170     0.0477 Rwanda"
  },
  {
    "objectID": "slides/week-12.1.html#your-turn-4",
    "href": "slides/week-12.1.html#your-turn-4",
    "title": "DIY Conflict Model",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nUse marginaleffects to calculate the predicted probabilities for a few countries\nInterpret the results\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-12.1.html#run-multiple-models",
    "href": "slides/week-12.1.html#run-multiple-models",
    "title": "DIY Conflict Model",
    "section": "Run Multiple Models",
    "text": "Run Multiple Models\n\nethnicity &lt;- glm(ucdponset ~ ethfrac + relfrac + wbgdppc2011est + wbpopest, # store each model in an object\n                  data = conflict_df,\n                  family = \"binomial\")\n\ndemocracy &lt;- glm(ucdponset ~ v2x_polyarchy + wbgdppc2011est +  wbpopest,\n                  data = conflict_df,\n                  family = \"binomial\")\n\nterrain &lt;- glm(ucdponset ~ rugged + wbgdppc2011est + wbpopest ,\n                  data = conflict_df,\n                  family = \"binomial\")\n\nfull_model &lt;- glm(ucdponset ~ ethfrac + relfrac + v2x_polyarchy + rugged +\n                        wbgdppc2011est + wbpopest,\n                  data = conflict_df,\n                  family = \"binomial\")"
  },
  {
    "objectID": "slides/week-12.1.html#prep-data-for-display",
    "href": "slides/week-12.1.html#prep-data-for-display",
    "title": "DIY Conflict Model",
    "section": "Prep Data for Display",
    "text": "Prep Data for Display\n\n\nmodels &lt;- list(\"Ethnicity\" = ethnicity,  # store list of models in an object\n               \"Democracy\" = democracy, \n               \"Terrain\" = terrain, \n               \"Full Model\" = full_model)\n\ncoef_map &lt;- c(\"ethfrac\" = \"Ethnic Frac\",  # map coefficients\n        \"relfrac\" = \"Religions Frac\",     #(change names and order)\n        \"v2x_polyarchy\" = \"Polyarchy\",\n        \"rugged\" = \"Terrain\",\n        \"wbgdppc2011est\" = \"Per capita GDP\",\n        \"wbpopest\" = \"Population\",\n        \"(Intercept)\" = \"Intercept\")\n\ncaption = \"Table 1: Predictors of Conflict Onset\" # store caption\nreference = \"See appendix for data sources.\"      # store reference notes"
  },
  {
    "objectID": "slides/week-12.1.html#display-the-models",
    "href": "slides/week-12.1.html#display-the-models",
    "title": "DIY Conflict Model",
    "section": "Display the Models",
    "text": "Display the Models\n\nNote that you also need the gt package to display the table in this way…\n\nlibrary(modelsummary)\nlibrary(gt)\n\nmodelsummary(models,                      # display the table\n             stars = TRUE,                # include stars for significance\n             gof_map = c(\"nobs\"),         # goodness of fit stats to include   \n             coef_map = coef_map,         # coefficient mapping\n             title = caption,             # title\n             notes = reference, \n             output = \"gt\")           # source note"
  },
  {
    "objectID": "slides/week-12.1.html#display-the-models-1",
    "href": "slides/week-12.1.html#display-the-models-1",
    "title": "DIY Conflict Model",
    "section": "Display the Models",
    "text": "Display the Models\n\n\n\n\n\n\nTable 1: Predictors of Conflict Onset\n\n\n\nEthnicity\nDemocracy\nTerrain\nFull Model\n\n\n\n\nEthnic Frac\n0.744*\n\n\n0.800*\n\n\n\n(0.367)\n\n\n(0.381)\n\n\nReligions Frac\n-0.481\n\n\n-0.391\n\n\n\n(0.411)\n\n\n(0.417)\n\n\nPolyarchy\n\n-0.228\n\n-0.602\n\n\n\n\n(0.436)\n\n(0.509)\n\n\nTerrain\n\n\n0.031\n0.064\n\n\n\n\n\n(0.076)\n(0.076)\n\n\nPer capita GDP\n-0.474***\n-0.512***\n-0.543***\n-0.372**\n\n\n\n(0.104)\n(0.108)\n(0.092)\n(0.121)\n\n\nPopulation\n0.282***\n0.297***\n0.299***\n0.293***\n\n\n\n(0.067)\n(0.051)\n(0.050)\n(0.067)\n\n\nIntercept\n-4.703***\n-4.407***\n-4.296***\n-5.693***\n\n\n\n(1.327)\n(1.205)\n(1.143)\n(1.408)\n\n\nNum.Obs.\n6364\n6772\n6840\n6151\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nSee appendix for data sources."
  },
  {
    "objectID": "slides/week-12.1.html#your-turn-5",
    "href": "slides/week-12.1.html#your-turn-5",
    "title": "DIY Conflict Model",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nRun multiple models\nDisplay the results in a regression table\nTry rerunning with exponentiate = TRUE\nTry getting predicted probabilities with marginaleffects\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-10.2.html#load-packages",
    "href": "slides/week-10.2.html#load-packages",
    "title": "Multiple Regression",
    "section": "Load packages",
    "text": "Load packages\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(vdemdata)"
  },
  {
    "objectID": "slides/week-10.2.html#load-vdem-data",
    "href": "slides/week-10.2.html#load-vdem-data",
    "title": "Multiple Regression",
    "section": "Load VDEM Data",
    "text": "Load VDEM Data\n\n\nmodelData &lt;- vdem |&gt;\n  filter(year == 2006) |&gt; \n  select(country_name, \n         libdem = v2x_libdem, \n         wealth = e_gdppc, \n         oil_rents = e_total_oil_income_pc,\n         polarization = v2cacamps, \n         corruption = v2x_corr, \n         judicial_review = v2jureview_ord, \n         region = e_regionpol_6C, \n         regime = v2x_regime) |&gt; \n  mutate(log_wealth = log(wealth), \n         region = factor(\n           region,\n           labels=c(\"Eastern Europe\", \n             \"Latin America\", \n             \"MENA\", \n             \"SSAfrica\", \n             \"Western Europe and North America\", \n             \"Asia and Pacific\"))\n          )\n\nglimpse(modelData)\n\nRows: 177\nColumns: 10\n$ country_name    &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ libdem          &lt;dbl&gt; 0.482, 0.672, 0.888, 0.845, 0.640, 0.659, 0.768, 0.018…\n$ wealth          &lt;dbl&gt; 14.584, 10.008, 42.378, 48.410, 3.284, 10.580, 37.068,…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; -0.346, -1.748, -2.346, -1.711, -0.398, -0.029, -2.273…\n$ corruption      &lt;dbl&gt; 0.611, 0.226, 0.004, 0.023, 0.640, 0.400, 0.107, 0.892…\n$ judicial_review &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …\n$ region          &lt;fct&gt; Latin America, Latin America, Western Europe and North…\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n$ log_wealth      &lt;dbl&gt; 2.6799250, 2.3033848, 3.7466294, 3.8797064, 1.1890622,…"
  },
  {
    "objectID": "slides/week-10.2.html#linear-model-with-multiple-predictors",
    "href": "slides/week-10.2.html#linear-model-with-multiple-predictors",
    "title": "Multiple Regression",
    "section": "Linear Model with Multiple Predictors",
    "text": "Linear Model with Multiple Predictors\n\n\nPreviously, we were interested in GDP per capita as a predictor of democracy\nNow, let’s consider another predictor: polarization (also measured by V-Dem)"
  },
  {
    "objectID": "slides/week-10.2.html#polarization-measure-in-the-usa",
    "href": "slides/week-10.2.html#polarization-measure-in-the-usa",
    "title": "Multiple Regression",
    "section": "Polarization Measure in the USA",
    "text": "Polarization Measure in the USA"
  },
  {
    "objectID": "slides/week-10.2.html#polarization-and-democracy-in-2006",
    "href": "slides/week-10.2.html#polarization-and-democracy-in-2006",
    "title": "Multiple Regression",
    "section": "Polarization and Democracy in 2006",
    "text": "Polarization and Democracy in 2006"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-one-predictor",
    "href": "slides/week-10.2.html#model-with-one-predictor",
    "title": "Multiple Regression",
    "section": "Model with One Predictor",
    "text": "Model with One Predictor\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(libdem ~ polarization, data = modelData) |&gt; \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.383     0.0187     20.5  2.68e-48\n2 polarization  -0.0882    0.0140     -6.29 2.45e- 9\n\n\n\nHow do we interpret the intercept and the slope estimate?\nSignificance?"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-two-predictors",
    "href": "slides/week-10.2.html#model-with-two-predictors",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(libdem ~ polarization + log_wealth, data = modelData) |&gt; \n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)    0.200     0.0344      5.80 0.0000000318 \n2 polarization  -0.0550    0.0139     -3.96 0.000111     \n3 log_wealth     0.0952    0.0152      6.24 0.00000000334"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-two-predictors-1",
    "href": "slides/week-10.2.html#model-with-two-predictors-1",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc\\]\n\\[\\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc\\]"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-two-predictors-2",
    "href": "slides/week-10.2.html#model-with-two-predictors-2",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc\\]\n\\[\\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc\\]\n\\(a\\) is the predicted level of Y when BOTH GDP per capita and polarization are equal to 0"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-two-predictors-3",
    "href": "slides/week-10.2.html#model-with-two-predictors-3",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc\\]\n\\[\\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc\\]\n\n\\(b_1\\) is the impact of a 1-unit change in polarization on the predicted level of Y, holding GDP per capita fixed (all else equal)\nThe relationship between polarization and democracy, controlling for wealth"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-two-predictors-4",
    "href": "slides/week-10.2.html#model-with-two-predictors-4",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc\\]\n\\[\\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc\\]\n\n\\(b_2\\) is the impact of a 1-unit change in GDP per capita on the predicted level of Y, holding polarization fixed (all else equal)\nThe relationship between wealth and democracy, controlling for polarization"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-two-predictors-5",
    "href": "slides/week-10.2.html#model-with-two-predictors-5",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc\\]\n\nOLS is searching for combination of \\(a\\), \\(b_1\\), and \\(b_2\\) that minimize sum of squared residuals\nSame logic as model with one predictor, just more complicated"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-three-predictors",
    "href": "slides/week-10.2.html#model-with-three-predictors",
    "title": "Multiple Regression",
    "section": "Model with Three Predictors",
    "text": "Model with Three Predictors\n\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc + b_3*OilRents\\]"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-three-predictors-1",
    "href": "slides/week-10.2.html#model-with-three-predictors-1",
    "title": "Multiple Regression",
    "section": "Model with Three Predictors",
    "text": "Model with Three Predictors\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(libdem ~ polarization + log_wealth + oil_rents, data = modelData) |&gt; \n  tidy()\n\n# A tibble: 4 × 5\n  term           estimate  std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   0.157     0.0322          4.86 2.78e- 6\n2 polarization -0.0538    0.0130         -4.13 5.87e- 5\n3 log_wealth    0.132     0.0152          8.67 4.76e-15\n4 oil_rents    -0.0000412 0.00000615     -6.70 3.39e-10"
  },
  {
    "objectID": "slides/week-10.2.html#model-with-three-predictors-2",
    "href": "slides/week-10.2.html#model-with-three-predictors-2",
    "title": "Multiple Regression",
    "section": "Model with Three Predictors",
    "text": "Model with Three Predictors\n\n\\[\\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc + b_3*OilRents\\]\n\\[\\hat{Y_i} = a + -.05*Polarization + .13*GDPpc + .00004*OilRents\\] \n\\(b_3\\) is the impact of a 1-unit change in oil revenues per capita on the predicted level of Y, holding polarization and GDP fixed (all else equal)"
  },
  {
    "objectID": "slides/week-10.2.html#your-turn",
    "href": "slides/week-10.2.html#your-turn",
    "title": "Multiple Regression",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nIn the last session, you examined levels of democracy and corruption\nNow, let’s fit a multiple regression model predicting corruption with two predictors: democracy (libdem) and polarization (polarization)\nThen, interpret the coefficients\nEstimate a second multiple regression model that adds in GDP per capita (lg_gdppc)\nInterpret the coefficients\nWhat happens to the coefficients of the other variables when we add GDP per capita to the model?\n\nWhy do you think this happens?\n\nTry adding in additional predictors if there is time\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-10.2.html#model-1",
    "href": "slides/week-10.2.html#model-1",
    "title": "Multiple Regression",
    "section": "Model 1",
    "text": "Model 1\n\n\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.846     0.0268     31.5  7.42e-74\n2 libdem        -0.760     0.0588    -12.9  3.42e-27\n3 polarization   0.0495    0.0121      4.10 6.33e- 5"
  },
  {
    "objectID": "slides/week-10.2.html#model-2---adding-gdp-per-capita",
    "href": "slides/week-10.2.html#model-2---adding-gdp-per-capita",
    "title": "Multiple Regression",
    "section": "Model 2 - Adding GDP per capita",
    "text": "Model 2 - Adding GDP per capita\n\n\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.954     0.0283     33.8  5.21e-77\n2 libdem        -0.602     0.0576    -10.5  4.59e-20\n3 polarization   0.0332    0.0109      3.04 2.71e- 3\n4 log_wealth    -0.0846    0.0127     -6.67 3.49e-10"
  },
  {
    "objectID": "slides/week-10.2.html#judicial-review-and-democracy",
    "href": "slides/week-10.2.html#judicial-review-and-democracy",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy\n\nJudicial Review:\n\nDo high courts (Supreme Court, Constitutional Court, etc) have the power to rule on whether laws or policies are constitutional/legal? (Yes or No)\nDimension of Judicial Independence"
  },
  {
    "objectID": "slides/week-10.2.html#judicial-review-and-democracy-1",
    "href": "slides/week-10.2.html#judicial-review-and-democracy-1",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy"
  },
  {
    "objectID": "slides/week-10.2.html#judicial-review-and-democracy-2",
    "href": "slides/week-10.2.html#judicial-review-and-democracy-2",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(libdem ~ factor(judicial_review), data = modelData) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term                     estimate std.error statistic     p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)                 0.176    0.0480      3.67 0.000323   \n2 factor(judicial_review)1    0.275    0.0521      5.27 0.000000390"
  },
  {
    "objectID": "slides/week-10.2.html#judicial-review-and-democracy-3",
    "href": "slides/week-10.2.html#judicial-review-and-democracy-3",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy\n\n\\[\\widehat{Democracy_{i}} = 0.17 + 0.28*JudicialReview(yes)\\]\n\nSlope: countries with judicial review are expected, on average, to be 0.28 units more democratic on the liberal democracy index\n\nCompares baseline level (Judicial Review = 0) to the other level (Judicial Review = 1)\n\nIntercept: average democracy score of countries without judicial review\nAverage democracy score of countries with judicial review is 0.17 + 0.28 = 0.45"
  },
  {
    "objectID": "slides/week-10.2.html#dummy-variables",
    "href": "slides/week-10.2.html#dummy-variables",
    "title": "Multiple Regression",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nWe always leave one category out of the model, as the omitted reference category\nEach coefficient describes is the expected difference between level of the factor and the baseline level\nEverything is relative to the omitted reference category"
  },
  {
    "objectID": "slides/week-10.2.html#democracy-and-world-region",
    "href": "slides/week-10.2.html#democracy-and-world-region",
    "title": "Multiple Regression",
    "section": "Democracy and World Region",
    "text": "Democracy and World Region\n\n\nDoes region predict levels of democracy?\nSince Eastern Europe is the first category, default in R is to use that as the omitted category in models.\n\n\nlevels(modelData$region)\n\n[1] \"Eastern Europe\"                   \"Latin America\"                   \n[3] \"MENA\"                             \"SSAfrica\"                        \n[5] \"Western Europe and North America\" \"Asia and Pacific\""
  },
  {
    "objectID": "slides/week-10.2.html#democracy-and-world-region-1",
    "href": "slides/week-10.2.html#democracy-and-world-region-1",
    "title": "Multiple Regression",
    "section": "Democracy and World Region",
    "text": "Democracy and World Region\n\nHow should we interpret intercept? How about the coefficient on Latin America?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(libdem ~ region, data = modelData) |&gt;\n  tidy()\n\n# A tibble: 6 × 5\n  term                                   estimate std.error statistic  p.value\n  &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                              0.436     0.0362     12.0  1.54e-24\n2 regionLatin America                      0.0685    0.0537      1.28 2.04e- 1\n3 regionMENA                              -0.235     0.0573     -4.11 6.16e- 5\n4 regionSSAfrica                          -0.138     0.0458     -3.00 3.07e- 3\n5 regionWestern Europe and North America   0.373     0.0543      6.87 1.15e-10\n6 regionAsia and Pacific                  -0.134     0.0521     -2.57 1.10e- 2"
  },
  {
    "objectID": "slides/week-10.2.html#democracy-and-world-region-2",
    "href": "slides/week-10.2.html#democracy-and-world-region-2",
    "title": "Multiple Regression",
    "section": "Democracy and World Region",
    "text": "Democracy and World Region\n\nWhat if you want a different baseline category? How do we interpret now?\n\n\n# make SS Africa the reference category\nmodelData &lt;- modelData |&gt; \nmutate(newReg = relevel(region, ref=4)) \n\nlinear_reg() |&gt;\n      set_engine(\"lm\") |&gt;\n      fit(libdem ~ newReg, data = modelData) |&gt;\n      tidy()\n\n# A tibble: 6 × 5\n  term                                   estimate std.error statistic  p.value\n  &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                             0.298      0.0281   10.6    1.44e-20\n2 newRegEastern Europe                    0.138      0.0458    3.00   3.07e- 3\n3 newRegLatin America                     0.206      0.0486    4.24   3.63e- 5\n4 newRegMENA                             -0.0977     0.0525   -1.86   6.44e- 2\n5 newRegWestern Europe and North America  0.511      0.0493   10.4    7.57e-20\n6 newRegAsia and Pacific                  0.00351    0.0468    0.0750 9.40e- 1"
  },
  {
    "objectID": "slides/week-10.2.html#your-turn-1",
    "href": "slides/week-10.2.html#your-turn-1",
    "title": "Multiple Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhich types of regime have more corruption?\n\nV-Dem also includes a categorial regime variable: Closed autocracy (0), Electoral Autocracy (1), Electoral Democracy (2), Liberal Democracy (3)"
  },
  {
    "objectID": "slides/week-10.2.html#your-turn-2",
    "href": "slides/week-10.2.html#your-turn-2",
    "title": "Multiple Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhich types of regime have more corruption?\n\nFirst, let’s make this an easier factor variable to work with.\n\n# Make nicer regime factor variable\nmodelData &lt;- modelData |&gt; \n  mutate(regime = factor(regime,\n                         labels = c(\"Closed Autocracy\",\n                                   \"Electoral Autocracy\",\n                                  \"Electoral Democracy\",\n                                  \"Liberal Democracy\")))\nlevels(modelData$regime)\n\n[1] \"Closed Autocracy\"    \"Electoral Autocracy\" \"Electoral Democracy\"\n[4] \"Liberal Democracy\""
  },
  {
    "objectID": "slides/week-10.2.html#your-turn-3",
    "href": "slides/week-10.2.html#your-turn-3",
    "title": "Multiple Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhich types of regime have more corruption?\n\n\nFilter data to include only the year 2019 (or run the code to use modelData)\nMake a plot to visualize the relationship between regime type and level of corruption. - Which kind of plot is best in this situation?\nFit a linear model\nInterpret the intercept and coefficients\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-10.2.html#visualization",
    "href": "slides/week-10.2.html#visualization",
    "title": "Multiple Regression",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "slides/week-10.2.html#model",
    "href": "slides/week-10.2.html#model",
    "title": "Multiple Regression",
    "section": "Model",
    "text": "Model\n\n\n\n# A tibble: 4 × 5\n  term                      estimate std.error statistic  p.value\n  &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                 0.597     0.0395     15.1  1.93e-33\n2 regimeElectoral Autocracy   0.148     0.0470      3.14 1.99e- 3\n3 regimeElectoral Democracy  -0.0580    0.0486     -1.19 2.35e- 1\n4 regimeLiberal Democracy    -0.459     0.0497     -9.23 9.63e-17"
  },
  {
    "objectID": "slides/week-10.2.html#create-your-own-model",
    "href": "slides/week-10.2.html#create-your-own-model",
    "title": "Multiple Regression",
    "section": "Create Your Own Model",
    "text": "Create Your Own Model\n\n\nWhat is a theory that you would like to test with V-Dem data?\nWhat is the dependent variable?\nWhat are the independent variables?\nMap out steps to wrangle the data and fit a regression model\nWhat do you expect to find?\nNow go ahead and wrangle the data\nFit the model\nInterpret the coefficients and their significance\nDid the results match your expectations?"
  },
  {
    "objectID": "slides/week-10.1.html#linear-model-with-single-predictor",
    "href": "slides/week-10.1.html#linear-model-with-single-predictor",
    "title": "Linear Regression",
    "section": "Linear Model with Single Predictor",
    "text": "Linear Model with Single Predictor\n\nGoal: Estimate Democracy score (\\(\\hat{Y_{i}}\\)) of a country given level of GDP per capita (\\(X_{i}\\)).\n\nOr: Estimate relationship between GDP per capita and democracy."
  },
  {
    "objectID": "slides/week-10.1.html#linear-model-with-single-predictor-1",
    "href": "slides/week-10.1.html#linear-model-with-single-predictor-1",
    "title": "Linear Regression",
    "section": "Linear Model with Single Predictor",
    "text": "Linear Model with Single Predictor"
  },
  {
    "objectID": "slides/week-10.1.html#section",
    "href": "slides/week-10.1.html#section",
    "title": "Linear Regression",
    "section": "",
    "text": "Step 1: Specify model\n\n\nlinear_reg()"
  },
  {
    "objectID": "slides/week-10.1.html#section-1",
    "href": "slides/week-10.1.html#section-1",
    "title": "Linear Regression",
    "section": "",
    "text": "Step 2: Set model fitting engine\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model"
  },
  {
    "objectID": "slides/week-10.1.html#section-2",
    "href": "slides/week-10.1.html#section-2",
    "title": "Linear Regression",
    "section": "",
    "text": "Step 3: Fit model & estimate parameters\n\n… using formula syntax\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(lib_dem ~ log_wealth, data = modelData) \n\nparsnip model object\n\n\nCall:\nstats::lm(formula = lib_dem ~ log_wealth, data = data)\n\nCoefficients:\n(Intercept)   log_wealth  \n     0.1327       0.1197"
  },
  {
    "objectID": "slides/week-10.1.html#section-3",
    "href": "slides/week-10.1.html#section-3",
    "title": "Linear Regression",
    "section": "",
    "text": "Step 4: Tidy things up…\n\n\\[\\widehat{Democracy}_{i} = 0.13 + 0.12 * {loggdppc}_{i}\\]\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(lib_dem ~ log_wealth, data = modelData) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.133    0.0380      3.49 6.04e- 4\n2 log_wealth     0.120    0.0147      8.16 6.97e-14"
  },
  {
    "objectID": "slides/week-10.1.html#interpretation",
    "href": "slides/week-10.1.html#interpretation",
    "title": "Linear Regression",
    "section": "Interpretation?",
    "text": "Interpretation?\n\n\\[\\widehat{Democracy}_{i} = 0.13 + 0.12 * {loggdppc}_{i}\\]"
  },
  {
    "objectID": "slides/week-10.1.html#question",
    "href": "slides/week-10.1.html#question",
    "title": "Linear Regression",
    "section": "Question",
    "text": "Question\n\nHow do we get the “best” values for the slope and intercept?"
  },
  {
    "objectID": "slides/week-10.1.html#how-would-you-draw-the-best-line",
    "href": "slides/week-10.1.html#how-would-you-draw-the-best-line",
    "title": "Linear Regression",
    "section": "How would you draw the “best” line?",
    "text": "How would you draw the “best” line?"
  },
  {
    "objectID": "slides/week-10.1.html#how-would-you-draw-the-best-line-1",
    "href": "slides/week-10.1.html#how-would-you-draw-the-best-line-1",
    "title": "Linear Regression",
    "section": "How would you draw the “best” line?",
    "text": "How would you draw the “best” line?"
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression",
    "href": "slides/week-10.1.html#least-squares-regression",
    "title": "Linear Regression",
    "section": "Least squares regression",
    "text": "Least squares regression\n\n\nRemember the residual is the difference between the actual value and the predicted value\n\n\n\nThe regression line minimizes the sum of squared residuals."
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression-1",
    "href": "slides/week-10.1.html#least-squares-regression-1",
    "title": "Linear Regression",
    "section": "Least squares regression",
    "text": "Least squares regression\n\n\nResidual for each point is: \\(e_i = y_i - \\hat{y}_i\\)\nLeast squares regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\).\n\n\n\nWhy do we square the residual?\n\n\n\n\nWhy not take absolute value?\n\nPrinciple: larger penalty for residuals further away\nMath: makes the math easier and some nice properties (not our concern here…)"
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression-2",
    "href": "slides/week-10.1.html#least-squares-regression-2",
    "title": "Linear Regression",
    "section": "Least squares regression",
    "text": "Least squares regression"
  },
  {
    "objectID": "slides/week-10.1.html#very-simple-example",
    "href": "slides/week-10.1.html#very-simple-example",
    "title": "Linear Regression",
    "section": "Very Simple Example",
    "text": "Very Simple Example\nWhat should the slope and intercept be?"
  },
  {
    "objectID": "slides/week-10.1.html#example",
    "href": "slides/week-10.1.html#example",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\n\\(\\hat{Y} = 0 + 1*X\\)"
  },
  {
    "objectID": "slides/week-10.1.html#example-1",
    "href": "slides/week-10.1.html#example-1",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is the sum of squared residuals?"
  },
  {
    "objectID": "slides/week-10.1.html#example-2",
    "href": "slides/week-10.1.html#example-2",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 0*X\\)?"
  },
  {
    "objectID": "slides/week-10.1.html#example-3",
    "href": "slides/week-10.1.html#example-3",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 0*X\\)?\n\n\n(1-0)^2 + (2-0)^2 + (3-0)^2\n\n[1] 14"
  },
  {
    "objectID": "slides/week-10.1.html#example-4",
    "href": "slides/week-10.1.html#example-4",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 2*X\\)?"
  },
  {
    "objectID": "slides/week-10.1.html#example-5",
    "href": "slides/week-10.1.html#example-5",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 2*X\\)?\n\n\n(1-2)^2 + (2-4)^2 + (3-6)^2\n\n[1] 14"
  },
  {
    "objectID": "slides/week-10.1.html#one-more",
    "href": "slides/week-10.1.html#one-more",
    "title": "Linear Regression",
    "section": "One more…",
    "text": "One more…\nWhat is sum of squared residuals for \\(y = 0 + -1*X\\)?"
  },
  {
    "objectID": "slides/week-10.1.html#one-more-1",
    "href": "slides/week-10.1.html#one-more-1",
    "title": "Linear Regression",
    "section": "One more…",
    "text": "One more…\nWhat is sum of squared residuals for \\(y = 0 + -1*X\\)?\n\n\n(1+1)^2 + (2+2)^2 + (3+3)^2\n\n[1] 56"
  },
  {
    "objectID": "slides/week-10.1.html#cost-function",
    "href": "slides/week-10.1.html#cost-function",
    "title": "Linear Regression",
    "section": "Cost Function",
    "text": "Cost Function\nSum of Squared Residuals as function of possible values of \\(b\\)"
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression-3",
    "href": "slides/week-10.1.html#least-squares-regression-3",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression\n\n\nWhen we estimate a least squares regression, it is looking for the line that minimizes sum of squared residuals\nIn the simple example, I set \\(a=0\\) to make it easier. More complicated when searching for combination of \\(a\\) and \\(b\\) that minimize, but same basic idea"
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression-4",
    "href": "slides/week-10.1.html#least-squares-regression-4",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression\n\n\nThere is a way to solve for this analytically for linear regression (i.e., by doing math…)\n– They made us do this in grad school…\n\n\n\nIn machine learning, people also use gradient descent algorithm in which the computer searches over possible combinations of \\(a\\) and \\(b\\) until it settles on the lowest point."
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression-5",
    "href": "slides/week-10.1.html#least-squares-regression-5",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression"
  },
  {
    "objectID": "slides/week-10.1.html#least-squares-regression-6",
    "href": "slides/week-10.1.html#least-squares-regression-6",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression"
  },
  {
    "objectID": "slides/week-10.1.html#your-turn",
    "href": "slides/week-10.1.html#your-turn",
    "title": "Linear Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nAre democracies less corrupt?\n\n\nV-Dem includes a Political Corruption Index, which aggregates corruption in a number of spheres (see codebook for details).\nThe variable name is: v2x_corr : lower values mean less corruption\nSee started code HERE"
  },
  {
    "objectID": "slides/week-10.1.html#your-turn-1",
    "href": "slides/week-10.1.html#your-turn-1",
    "title": "Linear Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nAre democracies less corrupt?\n\n\n\nFilter the V-Dem data to only include the year 2019\nMake a scatterplot to visualize the relationship between democracy (X) and corruption (Y) (use the v2x_libdem variable for democracy)\nFit a linear model\nInterpret results for the slope and intercept\nFor a country with the average (mean) level of democracy, what is the predicted level of corruption?\n\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-10.1.html#create-your-own-model",
    "href": "slides/week-10.1.html#create-your-own-model",
    "title": "Linear Regression",
    "section": "Create Your Own Model",
    "text": "Create Your Own Model\n\n\nWhat is a theory that you would like to test with V-Dem data?\nWhat is the dependent variable?\nWhat is the independent variable?\nMap out steps to wrangle the data and fit a regression model\nWhat do you expect to find?\nNow go ahead and wrangle the data\nFit the model\nInterpret the coefficients and their significance\nDid the results match your expectations?"
  },
  {
    "objectID": "slides/week-5.1.html#todays-classwork",
    "href": "slides/week-5.1.html#todays-classwork",
    "title": "Categorical Data",
    "section": "Today’s Classwork",
    "text": "Today’s Classwork\n\nDownload classwork folder for today’s class from here"
  },
  {
    "objectID": "slides/week-5.1.html#section",
    "href": "slides/week-5.1.html#section",
    "title": "Categorical Data",
    "section": "",
    "text": "What are some ways we can classify data? 😎 💭\n\n\nanecdotal vs. representative\ncensus vs. sample\nobservational vs. experimental\ncategorical vs. numerical\ndiscrete vs. continuous\ncross-sectional vs. time series\nlongitudinal vs. panel\nunstructured vs. structured"
  },
  {
    "objectID": "slides/week-5.1.html#section-1",
    "href": "slides/week-5.1.html#section-1",
    "title": "Categorical Data",
    "section": "",
    "text": "What are some ways we can classify data? 😎 💭\n\nanecdotal vs. representative\ncensus vs. sample\nobservational vs. experimental\ncategorical vs. numerical\ndiscrete vs. continuous\ncross-sectional vs. time series\nlongitudinal vs. panel\nunstructured vs. structured"
  },
  {
    "objectID": "slides/week-5.1.html#variable-types",
    "href": "slides/week-5.1.html#variable-types",
    "title": "Categorical Data",
    "section": "Variable Types",
    "text": "Variable Types\n\nCategorical\n\nBinary - two categories\nNominal - multiple unordered categories\nOrdinal - multiple ordered categories\n\nNumerical\n\nContinuous - fractional values (measurement data)\nDiscrete - non-negative whole numbers (count data)"
  },
  {
    "objectID": "slides/week-5.1.html#section-2",
    "href": "slides/week-5.1.html#section-2",
    "title": "Categorical Data",
    "section": "",
    "text": "What types of variables are these? 🤔\n\nIs a country a democracy? (yes/no)\nPolity (-10 to 10 in 1 unit increments)\nV-Dem Polyarchy (0-1 in 0.01 increments)\nV-Dem Regimes of the World Measure\n\nclosed autocracy | electoral autocracy | electoral democracy | democracy\n\nNumber of protest events\nProtest types (sit in, march, strike, etc.)"
  },
  {
    "objectID": "slides/week-5.1.html#v-dem-regimes-of-the-world-measure",
    "href": "slides/week-5.1.html#v-dem-regimes-of-the-world-measure",
    "title": "Categorical Data",
    "section": "V-Dem Regimes of the World Measure",
    "text": "V-Dem Regimes of the World Measure\n\n\nClosed Autocracy\nElectoral Autocracy\nElectoral Democracy\nLiberal Democracy"
  },
  {
    "objectID": "slides/week-5.1.html#data-setup",
    "href": "slides/week-5.1.html#data-setup",
    "title": "Categorical Data",
    "section": "Data Setup",
    "text": "Data Setup\n\nlibrary(tidyverse)\nlibrary(vdemdata)\n\nvdem2022 &lt;- vdem |&gt;\n  filter(year == 2022) |&gt;\n  select(\n    country = country_name, \n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  ) |&gt;\n  mutate(regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~  \"Electoral Democracy\",\n                    3 ~  \"Liberal Democracy\")\n  )"
  },
  {
    "objectID": "slides/week-5.1.html#examine-the-data",
    "href": "slides/week-5.1.html#examine-the-data",
    "title": "Categorical Data",
    "section": "Examine the Data",
    "text": "Examine the Data\n\n\nglimpse(vdem2022)\n\nRows: 179\nColumns: 3\n$ country &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\", \"South…\n$ regime  &lt;chr&gt; \"Electoral Democracy\", \"Electoral Democracy\", \"Liberal Democra…\n$ region  &lt;chr&gt; \"Latin America\", \"Latin America\", \"The West\", \"The West\", \"Afr…"
  },
  {
    "objectID": "slides/week-5.1.html#section-3",
    "href": "slides/week-5.1.html#section-3",
    "title": "Categorical Data",
    "section": "",
    "text": "Let’s count the number of regimes by type…\n\n\nvdem2022 |&gt;\n  count(regime)\n\n               regime  n\n1    Closed Autocracy 33\n2 Electoral Autocracy 56\n3 Electoral Democracy 56\n4   Liberal Democracy 34"
  },
  {
    "objectID": "slides/week-5.1.html#section-4",
    "href": "slides/week-5.1.html#section-4",
    "title": "Categorical Data",
    "section": "",
    "text": "Now let’s visualize the distribution of regimes with a bar plot…"
  },
  {
    "objectID": "slides/week-5.1.html#section-5",
    "href": "slides/week-5.1.html#section-5",
    "title": "Categorical Data",
    "section": "",
    "text": "Now let’s visualize the distribution of regimes with a bar plot…\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = regime)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    x = \"Regime\",\n    y = \"Frequency\",\n    title = \"Regimes of the World in 2022\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.1.html#section-6",
    "href": "slides/week-5.1.html#section-6",
    "title": "Categorical Data",
    "section": "",
    "text": "Now let’s visualize the distribution of regimes with a bar plot…\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = regime)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    x = \"Regime\",\n    y = \"Frequency\",\n    title = \"Regimes of the World in 2022\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.1.html#more-about-geom_bar",
    "href": "slides/week-5.1.html#more-about-geom_bar",
    "title": "Categorical Data",
    "section": "More about geom_bar()",
    "text": "More about geom_bar()\n\n\ngeom_bar() is different from geom_col()\nUsed to create bar plots where the height of the bar represents counts or frequencies of categorical variable\nBy default, geom_bar() counts the number of occurrences of each category or group and plots it as the height of the bar\nLike geom_histogram(), geom_bar() only requires the x aesthetic (y is automatically calcualted for you)"
  },
  {
    "objectID": "slides/week-5.1.html#your-turn",
    "href": "slides/week-5.1.html#your-turn",
    "title": "Categorical Data",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nExpore the distribution of regimes for a different year\nPreprocess your data to include only the year you are interested in\nVisualize the distribution of regimes using geom_bar()\nUse the labs() function to change title\nWhat is different about the year that you chose relative to 2022?"
  },
  {
    "objectID": "slides/week-5.1.html#how-do-regimes-vary-by-region",
    "href": "slides/week-5.1.html#how-do-regimes-vary-by-region",
    "title": "Categorical Data",
    "section": "How do Regimes Vary by Region?",
    "text": "How do Regimes Vary by Region?"
  },
  {
    "objectID": "slides/week-5.1.html#how-do-regimes-vary-by-region-1",
    "href": "slides/week-5.1.html#how-do-regimes-vary-by-region-1",
    "title": "Categorical Data",
    "section": "How do Regimes Vary by Region?",
    "text": "How do Regimes Vary by Region?\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-5.1.html#how-do-regimes-vary-by-region-2",
    "href": "slides/week-5.1.html#how-do-regimes-vary-by-region-2",
    "title": "Categorical Data",
    "section": "How do Regimes Vary by Region?",
    "text": "How do Regimes Vary by Region?\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-5.1.html#what-did-we-just-do",
    "href": "slides/week-5.1.html#what-did-we-just-do",
    "title": "Categorical Data",
    "section": "What Did We Just Do?",
    "text": "What Did We Just Do?\n\n\nWe used geom_bar() to visualize the distribution of regimes by region\nEssentially, we used two categorical variables to create a bar plot\nRegime type is…\n\n…an ordered categorical variable\n\nRegion is…\n\n…a nominal categorical variable"
  },
  {
    "objectID": "slides/week-5.1.html#section-7",
    "href": "slides/week-5.1.html#section-7",
    "title": "Categorical Data",
    "section": "",
    "text": "Some regions have more countries than others. Why does this create an issue for telling a story with our data here?"
  },
  {
    "objectID": "slides/week-5.1.html#show-proportions-instead",
    "href": "slides/week-5.1.html#show-proportions-instead",
    "title": "Categorical Data",
    "section": "Show Proportions Instead",
    "text": "Show Proportions Instead"
  },
  {
    "objectID": "slides/week-5.1.html#section-8",
    "href": "slides/week-5.1.html#section-8",
    "title": "Categorical Data",
    "section": "",
    "text": "We use position = \"fill\" to normalize the data and make the plot more interpretable…\n\n\nvdem2022 %&gt;%\n  ggplot(., aes(x = region, fill = regime)) +\n      geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Proportion\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-5.1.html#your-turn-1",
    "href": "slides/week-5.1.html#your-turn-1",
    "title": "Categorical Data",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nExplore the distribution of regimes by region for a different year\nPreprocess your data to include only the year you are interested in\nVisualize the distribution of regimes using geom_bar() and position = \"fill\"\nUse the labs() function to change title\nWhat is different about the year that you chose relative to 2022?"
  },
  {
    "objectID": "slides/week-5.2.html#electoral-democracy-measure",
    "href": "slides/week-5.2.html#electoral-democracy-measure",
    "title": "Numerical Data",
    "section": "Electoral Democracy Measure",
    "text": "Electoral Democracy Measure\n\n\nTo what extent is the ideal of electoral democracy in its fullest sense achieved?\nMeasure runs from 0 (lowest) to 1 (highest)\n0.5 is a cutoff for distinguishing electoral democracy from electoral autocracy\n\n\nThe electoral principle of democracy seeks to embody the core value of making rulers responsive to citizens, achieved through electoral competition for the electorate’s approval under circumstances when suffrage is extensive; political and civil society organizations can operate freely; elections are clean and not marred by fraud or systematic irregularities; and elections affect the composition of the chief executive of the country. In between elections, there is freedom of expression and an independent media capable of presenting alternative views on matters of political relevance. – V-Dem Codebook"
  },
  {
    "objectID": "slides/week-5.2.html#other-high-level-v-dem-measures",
    "href": "slides/week-5.2.html#other-high-level-v-dem-measures",
    "title": "Numerical Data",
    "section": "Other High-Level V-Dem Measures",
    "text": "Other High-Level V-Dem Measures\n\n\nLiberal Democracy\nEgalitarian Democracy\nParticipatory Democracy\nDeliberative Democracy\n\nAll continuous measures, ranging from 0 to 1. Let’s take a look at how to summarize data like this!"
  },
  {
    "objectID": "slides/week-5.2.html#data-setup",
    "href": "slides/week-5.2.html#data-setup",
    "title": "Numerical Data",
    "section": "Data Setup",
    "text": "Data Setup\n\n\n# Load packages \nlibrary(vdemdata)\nlibrary(tidyverse)\n\n# Create dataset for year 2022, with country name, year, and electoral dem\nvdem2022 &lt;- vdem |&gt;\n  filter(year == 2022)  |&gt;\n  select(\n    country = country_name, \n    year, \n    polyarchy = v2x_polyarchy, \n    region = e_regionpol_6C \n    ) |&gt;\n  mutate(region = case_match(region, \n                        1 ~ \"Eastern Europe\", \n                        2 ~ \"Latin America\",  \n                        3 ~ \"Middle East\",   \n                        4 ~ \"Africa\", \n                        5 ~ \"The West\", \n                        6 ~ \"Asia\"))"
  },
  {
    "objectID": "slides/week-5.2.html#examine-the-data",
    "href": "slides/week-5.2.html#examine-the-data",
    "title": "Numerical Data",
    "section": "Examine the Data",
    "text": "Examine the Data\n\n\nglimpse(vdem2022)\n\nRows: 179\nColumns: 4\n$ country   &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\", \"Sou…\n$ year      &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, …\n$ polyarchy &lt;dbl&gt; 0.586, 0.780, 0.896, 0.896, 0.662, 0.713, 0.823, 0.092, 0.20…\n$ region    &lt;chr&gt; \"Latin America\", \"Latin America\", \"The West\", \"The West\", \"A…"
  },
  {
    "objectID": "slides/week-5.2.html#section",
    "href": "slides/week-5.2.html#section",
    "title": "Numerical Data",
    "section": "",
    "text": "How can we summarize measures of democracy? 🤔\n\nWe could calculate the mean.\n\nvdem2022 |&gt;\n  summarize(mean_democracy = mean(polyarchy))\n\n  mean_democracy\n1       0.496514\n\n\nThe mean is the average of the values. Common measure of central tendency but sensitive to outliers."
  },
  {
    "objectID": "slides/week-5.2.html#section-1",
    "href": "slides/week-5.2.html#section-1",
    "title": "Numerical Data",
    "section": "",
    "text": "How can we summarize measures of democracy? 🤔\n\nWe could calculate the median.\n\nvdem2022 |&gt;\n  summarize(median_democracy = median(polyarchy))\n\n  median_democracy\n1            0.503\n\n\nThe median is the value that separates the higher half from the lower half of the data."
  },
  {
    "objectID": "slides/week-5.2.html#section-2",
    "href": "slides/week-5.2.html#section-2",
    "title": "Numerical Data",
    "section": "",
    "text": "We can also describe the shape of the distribution…\n\nsymmetric (e.g. normal)\nright-skewed\nleft-skewed\nunimodal (one peak)\nbimodal (multiple peaks)"
  },
  {
    "objectID": "slides/week-5.2.html#histograms",
    "href": "slides/week-5.2.html#histograms",
    "title": "Numerical Data",
    "section": "Histograms",
    "text": "Histograms\n\nUsed to represent the distribution of a continuous variable\nThe x-axis represents the range of values\nThe y-axis represents the frequency of each value\nThe bars represent the number of observations in each range or “bin”\nThe shape of the histogram can tell us a lot about the distribution of the data"
  },
  {
    "objectID": "slides/week-5.2.html#symmetric-distributions",
    "href": "slides/week-5.2.html#symmetric-distributions",
    "title": "Numerical Data",
    "section": "Symmetric Distributions",
    "text": "Symmetric Distributions"
  },
  {
    "objectID": "slides/week-5.2.html#symmetric-distributions-1",
    "href": "slides/week-5.2.html#symmetric-distributions-1",
    "title": "Numerical Data",
    "section": "Symmetric Distributions",
    "text": "Symmetric Distributions"
  },
  {
    "objectID": "slides/week-5.2.html#skewed-distributions",
    "href": "slides/week-5.2.html#skewed-distributions",
    "title": "Numerical Data",
    "section": "Skewed Distributions",
    "text": "Skewed Distributions"
  },
  {
    "objectID": "slides/week-5.2.html#skewed-distributions-1",
    "href": "slides/week-5.2.html#skewed-distributions-1",
    "title": "Numerical Data",
    "section": "Skewed Distributions",
    "text": "Skewed Distributions"
  },
  {
    "objectID": "slides/week-5.2.html#bimodal-distribution",
    "href": "slides/week-5.2.html#bimodal-distribution",
    "title": "Numerical Data",
    "section": "Bimodal Distribution",
    "text": "Bimodal Distribution"
  },
  {
    "objectID": "slides/week-5.2.html#when-is-the-mean-useful",
    "href": "slides/week-5.2.html#when-is-the-mean-useful",
    "title": "Numerical Data",
    "section": "When is the Mean Useful?",
    "text": "When is the Mean Useful?"
  },
  {
    "objectID": "slides/week-5.2.html#when-is-the-mean-useful-1",
    "href": "slides/week-5.2.html#when-is-the-mean-useful-1",
    "title": "Numerical Data",
    "section": "When is the Mean Useful?",
    "text": "When is the Mean Useful?"
  },
  {
    "objectID": "slides/week-5.2.html#when-is-the-mean-useful-2",
    "href": "slides/week-5.2.html#when-is-the-mean-useful-2",
    "title": "Numerical Data",
    "section": "When is the Mean Useful?",
    "text": "When is the Mean Useful?"
  },
  {
    "objectID": "slides/week-5.2.html#when-is-the-mean-useful-3",
    "href": "slides/week-5.2.html#when-is-the-mean-useful-3",
    "title": "Numerical Data",
    "section": "When is the mean useful?",
    "text": "When is the mean useful?\n\n\nThe Mean works well as a summary statistic when the distribution is relatively symmetric\nNot as well when distributions are skewed or bimodal (or multi-modal)\nWith skewed distributions, the mean is sensitive to extreme values\nThe median is more robust"
  },
  {
    "objectID": "slides/week-5.2.html#lesson",
    "href": "slides/week-5.2.html#lesson",
    "title": "Numerical Data",
    "section": "Lesson",
    "text": "Lesson\n\nAlways look at your data!!\nWhen reading or in a presentation, ask yourself:\n\nDoes the mean make sense given the distribution of the measure?\nCould extreme values in a skewed distribution make the mean not as useful?\nHave the analysts shown you the distribution? If not, ask about it!"
  },
  {
    "objectID": "slides/week-5.2.html#visualize-our-measure",
    "href": "slides/week-5.2.html#visualize-our-measure",
    "title": "Numerical Data",
    "section": "Visualize Our Measure",
    "text": "Visualize Our Measure"
  },
  {
    "objectID": "slides/week-5.2.html#visualize-our-measure-1",
    "href": "slides/week-5.2.html#visualize-our-measure-1",
    "title": "Numerical Data",
    "section": "Visualize Our Measure",
    "text": "Visualize Our Measure\n\n\nmn &lt;- mean(vdem2022$polyarchy)\nmed &lt;- median(vdem2022$polyarchy)\n\nggplot(vdem2022, aes(x = polyarchy )) +\n  geom_histogram(binwidth = .05, fill = \"steelblue\") +\n   labs(\n    x = \"Electoral Democracy\", \n    y = \"Frequency\", \n    title = \"Distribution of Electoral Democracy in 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) + \n  geom_vline(xintercept = mn, linewidth = 1, color = \"darkorange\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.2.html#visualize-our-measure-2",
    "href": "slides/week-5.2.html#visualize-our-measure-2",
    "title": "Numerical Data",
    "section": "Visualize Our Measure",
    "text": "Visualize Our Measure\n\n\nmn &lt;- mean(vdem2022$polyarchy)\nmed &lt;- median(vdem2022$polyarchy)\n\nggplot(vdem2022, aes(x = polyarchy )) +\n  geom_histogram(binwidth = .05, fill = \"steelblue\") +\n   labs(\n    x = \"Electoral Democracy\", \n    y = \"Frequency\", \n    title = \"Distribution of Electoral Democracy in 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) + \n  geom_vline(xintercept = mn, linewidth = 1, color = \"darkorange\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.2.html#your-turn",
    "href": "slides/week-5.2.html#your-turn",
    "title": "Numerical Data",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nLook at the V-Dem codebook\nSelect a different high-level measure of democracy\nPreprocess your data to include tha measure in your data frame\nCalculate the mean and median and store as a variable\nVisualize the distribution of the measure\nInclude a vertical line for the mean\nNow try the median"
  },
  {
    "objectID": "slides/week-5.2.html#recap",
    "href": "slides/week-5.2.html#recap",
    "title": "Numerical Data",
    "section": "Recap",
    "text": "Recap\n\n\nWe can use statistics like mean or median to describe the center of a variable\nWe can visualize the entire distribution to charachterize the distribution of the variable\nWe should also say something about the spread of the distribution"
  },
  {
    "objectID": "slides/week-5.2.html#why-measure-and-visualize-spread",
    "href": "slides/week-5.2.html#why-measure-and-visualize-spread",
    "title": "Numerical Data",
    "section": "Why Measure and Visualize Spread?",
    "text": "Why Measure and Visualize Spread?"
  },
  {
    "objectID": "slides/week-5.2.html#measures-of-spread-range",
    "href": "slides/week-5.2.html#measures-of-spread-range",
    "title": "Numerical Data",
    "section": "Measures of Spread: Range",
    "text": "Measures of Spread: Range\n\n\nRange (min and max values)\nNot ideal b/c does not tell us much about where most of the values are located\n\n\nvdem2022 |&gt;\n  summarize(min = min(polyarchy),\n            max = max(polyarchy))\n\n    min   max\n1 0.015 0.918"
  },
  {
    "objectID": "slides/week-5.2.html#measure-of-spread-interquartile-range",
    "href": "slides/week-5.2.html#measure-of-spread-interquartile-range",
    "title": "Numerical Data",
    "section": "Measure of Spread: Interquartile Range",
    "text": "Measure of Spread: Interquartile Range\nIQR: 25th percentile - 75th percentile"
  },
  {
    "objectID": "slides/week-5.2.html#interquartile-range",
    "href": "slides/week-5.2.html#interquartile-range",
    "title": "Numerical Data",
    "section": "Interquartile Range",
    "text": "Interquartile Range\n\nThe middle 50 percent of the countries in the data lie between 0.262 and 0.747\nThe IQR (0.485) is the difference between the Q3 and Q1 values\n\n\nvdem2022 %&gt;% \n  summarize(IQRlow =  quantile(polyarchy, .25),\n            IQRhigh = quantile(polyarchy, .75),\n            IQRlength = IQR(polyarchy)\n          )\n\n  IQRlow IQRhigh IQRlength\n1  0.259  0.7485    0.4895"
  },
  {
    "objectID": "slides/week-5.2.html#box-plot",
    "href": "slides/week-5.2.html#box-plot",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\n\nA box plot is a graphical representation of the distribution based on the median and quartiles\nIt is a standardized way of displaying the distribution of data based on a five number summary: minimum, first quartile, median, third quartile, and maximum"
  },
  {
    "objectID": "slides/week-5.2.html#box-plot-1",
    "href": "slides/week-5.2.html#box-plot-1",
    "title": "Numerical Data",
    "section": "Box Plot",
    "text": "Box Plot\n\n\nCode\nggplot(vdem2022, aes(x = \"\", y = polyarchy)) +\n  geom_boxplot(fill = \"steelblue\") + \n   labs(\n    x = \"\", \n    y = \"Electoral Democracy\", \n    title = \"Distribution of Electoral Democracy in 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.2.html#measure-of-spead-standard-deviation",
    "href": "slides/week-5.2.html#measure-of-spead-standard-deviation",
    "title": "Numerical Data",
    "section": "Measure of Spead: Standard Deviation",
    "text": "Measure of Spead: Standard Deviation\n\n\nCan think of it as something like the “average distance” of each data point from the mean\n\n\nvdem2022 |&gt;\n  summarize(mean = mean(polyarchy),\n            stdDev = sd(polyarchy))\n\n      mean    stdDev\n1 0.496514 0.2629688"
  },
  {
    "objectID": "slides/week-5.2.html#standard-deviation",
    "href": "slides/week-5.2.html#standard-deviation",
    "title": "Numerical Data",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nA low standard deviation indicates that the values tend to be close to the mean\nA high standard deviation indicates that the values are spread out over a wider range"
  },
  {
    "objectID": "slides/week-5.2.html#starting-with-variance",
    "href": "slides/week-5.2.html#starting-with-variance",
    "title": "Numerical Data",
    "section": "Starting with Variance",
    "text": "Starting with Variance\n\n\nVariance is a step towards calculating the standard deviation.\nIt quantifies the average squared deviation of each number from the mean of the data set."
  },
  {
    "objectID": "slides/week-5.2.html#calculating-deviation-from-the-mean",
    "href": "slides/week-5.2.html#calculating-deviation-from-the-mean",
    "title": "Numerical Data",
    "section": "Calculating Deviation from the Mean",
    "text": "Calculating Deviation from the Mean\n\nFirst, calculate the mean (\\(\\bar{X}\\)) of the dataset.\nFor each data point (\\(X_i\\)), calculate its deviation from the mean: \\[e_i = X_i - \\bar{X}\\]\n\nExample with a mean of 5:\n\nFor a data point where \\((X_i = 0): (0 - 5 = -5)\\)\nFor a data point where \\((X_i = 10): (10 - 5 = 5)\\)"
  },
  {
    "objectID": "slides/week-5.2.html#squaring-the-deviations",
    "href": "slides/week-5.2.html#squaring-the-deviations",
    "title": "Numerical Data",
    "section": "Squaring the Deviations",
    "text": "Squaring the Deviations\n\nSquaring each deviation (\\(e_i\\)) to eliminate negative values: \\[e_i^2 = (X_i - \\bar{X})^2\\]\nSumming up all squared deviations: \\[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\]\nThis sum represents the total squared deviation from the mean."
  },
  {
    "objectID": "slides/week-5.2.html#calculating-the-variance",
    "href": "slides/week-5.2.html#calculating-the-variance",
    "title": "Numerical Data",
    "section": "Calculating the Variance",
    "text": "Calculating the Variance\n\nDivide the total squared deviation by \\((n-1)\\) (to account for the sample variance): \\[\\text{Variance} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\]\nUsing \\((n-1)\\) ensures an unbiased estimate of the population variance when calculating from a sample."
  },
  {
    "objectID": "slides/week-5.2.html#deriving-the-standard-deviation",
    "href": "slides/week-5.2.html#deriving-the-standard-deviation",
    "title": "Numerical Data",
    "section": "Deriving the Standard Deviation",
    "text": "Deriving the Standard Deviation\n\n\nThe standard deviation is the square root of the variance: \\[s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\]\nTaking the square root converts the variance back to the units of the original data."
  },
  {
    "objectID": "slides/week-5.2.html#standard-deviation-simple-example",
    "href": "slides/week-5.2.html#standard-deviation-simple-example",
    "title": "Numerical Data",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nx = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ne &lt;- x - mean(x)\ne\n\n [1] -5 -4 -3 -2 -1  0  1  2  3  4  5"
  },
  {
    "objectID": "slides/week-5.2.html#standard-deviation-simple-example-1",
    "href": "slides/week-5.2.html#standard-deviation-simple-example-1",
    "title": "Numerical Data",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nx = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ne_squared &lt;- e^2\ne_squared\n\n [1] 25 16  9  4  1  0  1  4  9 16 25"
  },
  {
    "objectID": "slides/week-5.2.html#standard-deviation-simple-example-2",
    "href": "slides/week-5.2.html#standard-deviation-simple-example-2",
    "title": "Numerical Data",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nx = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum_e_squared &lt;- sum(e_squared)\nsum_e_squared\n\n[1] 110"
  },
  {
    "objectID": "slides/week-5.2.html#standard-deviation-simple-example-3",
    "href": "slides/week-5.2.html#standard-deviation-simple-example-3",
    "title": "Numerical Data",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nvariance &lt;- sum_e_squared/(length(x)-1)\nvariance\n\n[1] 11"
  },
  {
    "objectID": "slides/week-5.2.html#standard-deviation-simple-example-4",
    "href": "slides/week-5.2.html#standard-deviation-simple-example-4",
    "title": "Numerical Data",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nstandard_dev &lt;- sqrt(variance)\nstandard_dev\n\n[1] 3.316625\n\nsd(x)\n\n[1] 3.316625"
  },
  {
    "objectID": "slides/week-5.2.html#your-turn-1",
    "href": "slides/week-5.2.html#your-turn-1",
    "title": "Numerical Data",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nCalculate measures of spread for the polyarchy variable in the V-Dem data (mean, median, IQR, standard deviation)\nHow would you interpret these measures?\nTry a box plot for the polyarchy variable\nTry another variable in the V-Dem data\nHow does it compare to polyarchy?"
  },
  {
    "objectID": "slides/week-5.2.html#calculating-statistics-by-groups",
    "href": "slides/week-5.2.html#calculating-statistics-by-groups",
    "title": "Numerical Data",
    "section": "Calculating Statistics by groups",
    "text": "Calculating Statistics by groups\n\n\nWhat if we want to describe electoral democracy and see how it differs by some different variable? For example, by world region, or by year?\nIn this case we want to combine numerical summaries with categorical variables\nThis brings us back to bar chart"
  },
  {
    "objectID": "slides/week-5.2.html#calculating-statistics-by-groups-1",
    "href": "slides/week-5.2.html#calculating-statistics-by-groups-1",
    "title": "Numerical Data",
    "section": "Calculating Statistics by Groups",
    "text": "Calculating Statistics by Groups\n\nLet’s calculate the mean and median of electoral democracy in each world region\nFor this, we add the group_by() to our previous code\n\n\nvdem2022 |&gt;\n  group_by(region) |&gt;\n  summarize(mean_dem = mean(polyarchy),\n            median_dem = median(polyarchy))\n\n# A tibble: 6 × 3\n  region         mean_dem median_dem\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 Africa            0.402      0.381\n2 Asia              0.424      0.414\n3 Eastern Europe    0.535      0.568\n4 Latin America     0.605      0.683\n5 Middle East       0.230      0.207\n6 The West          0.855      0.853"
  },
  {
    "objectID": "slides/week-5.2.html#calculating-statistics-by-groups-2",
    "href": "slides/week-5.2.html#calculating-statistics-by-groups-2",
    "title": "Numerical Data",
    "section": "Calculating Statistics by Groups",
    "text": "Calculating Statistics by Groups\n\nLet’s store our statistics as a new data object, democracy_region\n\n\ndemocracy_region &lt;- vdem2022 |&gt; \n  group_by(region) |&gt;\n  summarize(mean_dem = mean(polyarchy),\n            median_dem = median(polyarchy))\n\ndemocracy_region\n\n# A tibble: 6 × 3\n  region         mean_dem median_dem\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 Africa            0.402      0.381\n2 Asia              0.424      0.414\n3 Eastern Europe    0.535      0.568\n4 Latin America     0.605      0.683\n5 Middle East       0.230      0.207\n6 The West          0.855      0.853"
  },
  {
    "objectID": "slides/week-5.2.html#visualize-using-our-bar-chart-skills",
    "href": "slides/week-5.2.html#visualize-using-our-bar-chart-skills",
    "title": "Numerical Data",
    "section": "Visualize using our Bar Chart Skills",
    "text": "Visualize using our Bar Chart Skills\n\n\nCode\nggplot(democracy_region, aes(x = reorder(region, -mean_dem), y = mean_dem)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Mean Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    ) + \n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.2.html#numerical-variable-by-group",
    "href": "slides/week-5.2.html#numerical-variable-by-group",
    "title": "Numerical Data",
    "section": "Numerical Variable by Group",
    "text": "Numerical Variable by Group\nHow should we interpret this plot?\n\n\nCode\nlibrary(ggridges)\n#library(forcats)\n  ggplot(vdem2022, aes(x = polyarchy, y = region, fill = region)) +\n    geom_density_ridges() +\n  labs(\n    x = \"Electoral Democracy\",\n    y = \"Region\",\n    title = \"A Ridge Plot\",\n    caption = \"Source: V-Dem Institute\",\n  ) +\n  scale_fill_viridis_d() +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-5.2.html#your-turn-2",
    "href": "slides/week-5.2.html#your-turn-2",
    "title": "Numerical Data",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nMake a bar chart summarizing polyarchy or some other V-Dem variable\nNow try your hand at a ridge plot"
  },
  {
    "objectID": "slides/week-3.1.html#where-does-data-come-from",
    "href": "slides/week-3.1.html#where-does-data-come-from",
    "title": "Working with Data",
    "section": "Where Does Data Come From?",
    "text": "Where Does Data Come From?\n\nThoughts? 😎 💭\n\n\nYour boss or a client sends you a file\nSurvey data collected by you or someone else\nYou can download it from a website\nYou can scrape it from a website\nA package (e.g. unvotes)\nYou can access it through an API"
  },
  {
    "objectID": "slides/week-3.1.html#the-concept-of-tidy-data",
    "href": "slides/week-3.1.html#the-concept-of-tidy-data",
    "title": "Working with Data",
    "section": "The Concept of “Tidy Data”",
    "text": "The Concept of “Tidy Data”\n\n\nEach column represents a single variable\nEach row represents a single observation\nEach cell represents a single value"
  },
  {
    "objectID": "slides/week-3.1.html#tidy-data-example",
    "href": "slides/week-3.1.html#tidy-data-example",
    "title": "Working with Data",
    "section": "Tidy Data Example",
    "text": "Tidy Data Example"
  },
  {
    "objectID": "slides/week-3.1.html#the-concept-of-clean-data",
    "href": "slides/week-3.1.html#the-concept-of-clean-data",
    "title": "Working with Data",
    "section": "The Concept of “Clean Data”",
    "text": "The Concept of “Clean Data”\n\n\nColumn names are easy to work with and are not duplicated\nMissing values have been dealt with\nThere are no repeated observations or columns\nThere are no blank observations or columns\nThe data are in the proper format\n\nFor example dates should be formatted as dates"
  },
  {
    "objectID": "slides/week-3.1.html#messy-data-example",
    "href": "slides/week-3.1.html#messy-data-example",
    "title": "Working with Data",
    "section": "Messy Data Example",
    "text": "Messy Data Example"
  },
  {
    "objectID": "slides/week-3.1.html#which-of-these-is-likely-tidyclean",
    "href": "slides/week-3.1.html#which-of-these-is-likely-tidyclean",
    "title": "Working with Data",
    "section": "Which of These is Likely Tidy/Clean?",
    "text": "Which of These is Likely Tidy/Clean?\n\n\nYour boss or a client sends you a file\nSurvey data collected by you or someone else\nYou can download it from a website\nYou can scrape it from a website\nA curated collection (e.g. unvotes)\nYou can access it through an API"
  },
  {
    "objectID": "slides/week-3.1.html#how-do-we-get-tidyclean-data",
    "href": "slides/week-3.1.html#how-do-we-get-tidyclean-data",
    "title": "Working with Data",
    "section": "How Do We Get Tidy/Clean Data?",
    "text": "How Do We Get Tidy/Clean Data?\n\n\nWrangle it ourselves\nUse a package where it has been wrangled for us\nDownload via an API"
  },
  {
    "objectID": "slides/week-3.1.html#apis",
    "href": "slides/week-3.1.html#apis",
    "title": "Working with Data",
    "section": "APIs",
    "text": "APIs\n\n\nAPI stands for “Application Programming Interface”\nWay for two computers to talk to each other\n\n\n\n\n\n\n%% Note: fig-width option not working as of Quarto 1.4, try again in 1.5 \n\nflowchart LR\n    Client--&gt;|Request|id1[(API)]\n    id1[(API)]--&gt;|Response|Client\n    id1[(API)]--&gt;Server\n    Server--&gt;id1[(API)]\n\n\n\n\n\n\n\nIn this class, we access APIs through packages"
  },
  {
    "objectID": "slides/week-3.1.html#packages-in-r",
    "href": "slides/week-3.1.html#packages-in-r",
    "title": "Working with Data",
    "section": "Packages in R",
    "text": "Packages in R\n\n\nMuch easier than reading in data from messy flat file!\nExamples in this course\n\nVarieties of Democracy (V-Dem) through vdemdata\nWorld Bank data through wbstats or WDI\nUN voting data through unvotes\n\nBut there are many similar packages out there (please explore!)"
  },
  {
    "objectID": "slides/week-3.1.html#this-lesson",
    "href": "slides/week-3.1.html#this-lesson",
    "title": "Working with Data",
    "section": "This Lesson",
    "text": "This Lesson\n\n\nAccess V-Dem API with vdemdata Package\nThis is the only package for V-Dem\nJust downloads all the data\nSo we have to use dplyr functions like filter() and select()"
  },
  {
    "objectID": "slides/week-3.1.html#section",
    "href": "slides/week-3.1.html#section",
    "title": "Working with Data",
    "section": "",
    "text": "Run this code and see what happens.\nWhat is vdem and what does it do?\n\n\n# Load packages\nlibrary(vdemdata) # to download V-Dem data\nlibrary(dplyr)\n\n# Download the data\ndemocracy &lt;- vdem \n\n# View the data\nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-3.1.html#filter-select-mutate",
    "href": "slides/week-3.1.html#filter-select-mutate",
    "title": "Working with Data",
    "section": "filter(), select(), mutate()",
    "text": "filter(), select(), mutate()\n\n\nfilter() is used to select observations based on their values\nselect() is used to select variables\nmutate() is used to create new variables or modifying existing ones"
  },
  {
    "objectID": "slides/week-3.1.html#filter",
    "href": "slides/week-3.1.html#filter",
    "title": "Working with Data",
    "section": "filter()",
    "text": "filter()\n\nRun this code. What do you see?\nTry changing the year\nFor one year, use == instead of &gt;=\nOr try &lt;= and see what happens\n\n\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year &gt;= 1990) # filter out years less than 1990\n  \nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-3.1.html#versus",
    "href": "slides/week-3.1.html#versus",
    "title": "Working with Data",
    "section": "= versus ==",
    "text": "= versus ==\n\n\n= is used to assign values to variables, just like &lt;-\n== is used to test if two values are equal to each other\nSo filter(year == 1990) will give you just the observations for 1990"
  },
  {
    "objectID": "slides/week-3.1.html#and",
    "href": "slides/week-3.1.html#and",
    "title": "Working with Data",
    "section": ">= and <=",
    "text": "&gt;= and &lt;=\n\n&gt;= is used to test if a value is greater than or equal to another value\n&lt;= is used to test if a value is less than or equal to another value\nSo filter(year &gt;= 1990) will give you the observations for 1990 and later\nAnd filter(year &lt;= 1990) will give you the observations for 1990 and earlier"
  },
  {
    "objectID": "slides/week-3.1.html#select",
    "href": "slides/week-3.1.html#select",
    "title": "Working with Data",
    "section": "select()",
    "text": "select()\n\nRun this code. What do you see?\nNow try v2x_libdem instead of v2x_polyarchy\nChoose more from the codebook\n\n\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  select(                  # select (and rename) these variables\n    country = country_name,     # before the = sign is new name  \n    vdem_ctry_id = country_id,  # after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy\n  )\n  \nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-3.1.html#mutate",
    "href": "slides/week-3.1.html#mutate",
    "title": "Working with Data",
    "section": "mutate()",
    "text": "mutate()\n\nModify the code to create new variable that is three times the value of polyarchy\nHow about polyarchy squared?\n\n\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year == 2015) |&gt; # keep only observations from 2015\n  select(                  # select (and rename) these variables\n    country = country_name,     # name before the = sign is new name  \n    vdem_ctry_id = country_id,  # name after the = sign is old name\n    year, \n    polyarchy = v2x_polyarchy \n    ) |&gt;\n  mutate(\n    polyarchy_dbl = polyarchy * 2 # create variable 2X polyarchy\n  )\n  \nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-3.1.html#some-common-arithmetic-operators",
    "href": "slides/week-3.1.html#some-common-arithmetic-operators",
    "title": "Working with Data",
    "section": "Some Common Arithmetic Operators",
    "text": "Some Common Arithmetic Operators\n\n\n+ addition\n- subtraction\n* multiplication\n/ division\n^ exponentiation (also **)"
  },
  {
    "objectID": "slides/week-3.1.html#vdemdata-example",
    "href": "slides/week-3.1.html#vdemdata-example",
    "title": "Working with Data",
    "section": "vdemdata Example",
    "text": "vdemdata Example\n\n\n# Load packages\nlibrary(vdemdata) # to download V-Dem data\nlibrary(dplyr)\n\n# Download the data\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year == 2015)  |&gt; # filter year, keep 2015\n  select(                  # select (and rename) these variables\n    country = country_name,     # the name before the = sign is the new name  \n    vdem_ctry_id = country_id,  # the name after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )\n\n# View the data\nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-3.1.html#section-1",
    "href": "slides/week-3.1.html#section-1",
    "title": "Working with Data",
    "section": "",
    "text": "Use filter() to select years…\n\n# Download the data\ndemocracy &lt;- vdem |&gt; \n  filter(year == 2015)  |&gt; # keep 2015\n  select(                 \n    country = country_name,       \n    vdem_ctry_id = country_id,  \n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region,\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )"
  },
  {
    "objectID": "slides/week-3.1.html#section-2",
    "href": "slides/week-3.1.html#section-2",
    "title": "Working with Data",
    "section": "",
    "text": "Use select() to choose variables…\n\n# Download the data\ndemocracy &lt;- vdem |&gt; \n  filter(year == 2015)  |&gt; \n  select(                  # select (and rename) these variables\n    country = country_name,     # the name before the = sign is the new name  \n    vdem_ctry_id = country_id,  # the name after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )"
  },
  {
    "objectID": "slides/week-3.1.html#section-3",
    "href": "slides/week-3.1.html#section-3",
    "title": "Working with Data",
    "section": "",
    "text": "Use mutate with case_match() to Recode Region….\n\n# Download the data\ndemocracy &lt;- vdem |&gt;\n  filter(year == 2015)  |&gt; \n  select(                  \n    country = country_name,     \n    vdem_ctry_id = country_id,  \n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n                    # number on the left of the ~ is the V-Dem region code\n                    # we are changing the number to the country name on the right\n                    # of the equals sign\n  )"
  },
  {
    "objectID": "slides/week-3.1.html#save-it",
    "href": "slides/week-3.1.html#save-it",
    "title": "Working with Data",
    "section": "Save It!",
    "text": "Save It!\n\n\nwrite_csv(democracy, \"democracy.csv\")"
  },
  {
    "objectID": "slides/week-3.1.html#visualize-it",
    "href": "slides/week-3.1.html#visualize-it",
    "title": "Working with Data",
    "section": "Visualize It!",
    "text": "Visualize It!\n\n\nlibrary(ggplot2)\n\nggplot(democracy, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy in 2015\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-3.1.html#visualize-it-1",
    "href": "slides/week-3.1.html#visualize-it-1",
    "title": "Working with Data",
    "section": "Visualize It!",
    "text": "Visualize It!"
  },
  {
    "objectID": "slides/week-3.1.html#try-it-yourself",
    "href": "slides/week-3.1.html#try-it-yourself",
    "title": "Working with Data",
    "section": "Try it Yourself",
    "text": "Try it Yourself\n\nGo to the V-Dem Codebook\nSelect a democracy indicator from Part 2.1 (high level indicators) to visualize\nNote the indicator code (e.g. “v2x_polyarchy” for the polyarchy score)\nChange the code and download the data so you can visualize it\nNow make a scatter plot of your indicator versus GDP\nBonus: How would you make a line chart?"
  },
  {
    "objectID": "slides/week-1.1.html#welcome",
    "href": "slides/week-1.1.html#welcome",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Welcome!",
    "text": "Welcome!\n\nProfessor Teitelbaum ejt@gwu.edu\nOffice Hours: Tuesdays, 3:00-5:00 p.m.\nMonroe Hall Rm 411 or online\nGo to my calendly page to sign up for a slot"
  },
  {
    "objectID": "slides/week-1.1.html#why-take-this-course",
    "href": "slides/week-1.1.html#why-take-this-course",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Why Take this Course?",
    "text": "Why Take this Course?\n\n\nInternational Affairs is changing!\nData is everywhere and it is changing the way government works\nYou will be a better consumer of data and research\nYou can be a “bridge builder” between traditional analysts and data scientists on your team"
  },
  {
    "objectID": "slides/week-1.1.html#data-informed-dipomacy",
    "href": "slides/week-1.1.html#data-informed-dipomacy",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Data Informed Dipomacy",
    "text": "Data Informed Dipomacy\n\n\nData is a critical instrument of diplomacy. When our workforce has data at their fingertips they are better prepared to engage diplomatically, manage effectively, and lead globally.\n\nSecretary of State Anthony Blinken, 20221\nhttps://www.state.gov/data/"
  },
  {
    "objectID": "slides/week-1.1.html#open-goverment-data-act-2018",
    "href": "slides/week-1.1.html#open-goverment-data-act-2018",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Open Goverment Data Act (2018)",
    "text": "Open Goverment Data Act (2018)\n\nRequires government data assets to be published as machine-readable data in open formats\nRequires Chief Data Officers (CDOs) to be appointed at federal agencies\nRequires CDOs to develop and maintain comprehensive data inventories\nHas led to a proliferation of data science roles in the federal government\n\n\n\nhttps://crsreports.congress.gov/product/pdf/IF/IF12299"
  },
  {
    "objectID": "slides/week-1.1.html#open-source-intelligence",
    "href": "slides/week-1.1.html#open-source-intelligence",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Open Source Intelligence",
    "text": "Open Source Intelligence\n\n\nMore than 90% of the analysis in the intelligence community is based on open source information\nGovernment agencies use a lot of the same datasets that we will be using in this class\nYet the OSINT community has only begun to scratch the surface of what is possible with data science"
  },
  {
    "objectID": "slides/week-1.1.html#monitoring-evaluation-and-learning",
    "href": "slides/week-1.1.html#monitoring-evaluation-and-learning",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Monitoring, Evaluation and Learning",
    "text": "Monitoring, Evaluation and Learning\n\nAnother important use of data in international affairs is monitoring, evaluation and learning (MEL)\nMEL is a process that helps organizations track and assess the performance of their programs\nMEL is a key component of the evaluation strategy of the World Bank and other agencies\nA major component of MEL is the use of randomized control trials (RCTs) and other designs, which you will learn about in this class"
  },
  {
    "objectID": "slides/week-1.1.html#skillsknowledge-you-will-gain",
    "href": "slides/week-1.1.html#skillsknowledge-you-will-gain",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Skills/Knowledge You Will Gain",
    "text": "Skills/Knowledge You Will Gain\n\n\nR coding skills (and RStudio), with focus on “tidy” approach and reproducible research\nQuarto (html documents, PDFs, presentations, websites, books, blogs, …)\nHow to access and “clean” data so that you can analyze it\nWhen you hear terms like “machine learning”, you’ll have some sense of what people are talking about"
  },
  {
    "objectID": "slides/week-1.1.html#structure-of-the-course",
    "href": "slides/week-1.1.html#structure-of-the-course",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Structure of the Course",
    "text": "Structure of the Course\n\nData Visualization\n\nSummarizing and communicating effectively with data\n\nStatistical Inference\n\nMaking rigorous conclusions from data\n\nModeling\n\nFor prediction and forecasting\nFor drawing causal conclusions"
  },
  {
    "objectID": "slides/week-1.1.html#how-do-i-get-an-a-requirements",
    "href": "slides/week-1.1.html#how-do-i-get-an-a-requirements",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "How do I get an “A”? (requirements)",
    "text": "How do I get an “A”? (requirements)\n\n\nLabs (20 percent)\n3 Data Analysis Assignments (45 percent; 15 percent each)\nFinal Project (20 percent)"
  },
  {
    "objectID": "slides/week-1.1.html#course-website",
    "href": "slides/week-1.1.html#course-website",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Course Website",
    "text": "Course Website\n\n\nhttps://quant4ia.rocks\nLet’s take a quick tour!"
  },
  {
    "objectID": "slides/week-1.1.html#install-r-and-rstudio",
    "href": "slides/week-1.1.html#install-r-and-rstudio",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\n\n\nIf you haven’t already…\nGo to the RStudio download page\nDownload R and then RStudio"
  },
  {
    "objectID": "slides/week-1.1.html#set-up-rstudio",
    "href": "slides/week-1.1.html#set-up-rstudio",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Set up RStudio",
    "text": "Set up RStudio\n\n\nGo to Tools&gt;Global Options\nUnder Code, enable native pipe operator (|&gt;)\nUnder Appearance, choose a theme\nConfigure panes\n\nGo to Pane Layout\nMove Source, Console, etc. to preferred positions"
  },
  {
    "objectID": "slides/week-1.1.html#illustration",
    "href": "slides/week-1.1.html#illustration",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "slides/week-1.1.html#install-key-packages",
    "href": "slides/week-1.1.html#install-key-packages",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Install key packages",
    "text": "Install key packages\n\nInstall the Tidyverse group of packages from the console\n\ninstall.packages(\"tidyverse\")\n\nInstall devtools\n\ninstall.packages(\"devtools\"))\n\nInstall tinytex (for PDF rendering)\n\nGo to your terminal and type quarto install tinytex"
  },
  {
    "objectID": "slides/week-1.1.html#illustration-1",
    "href": "slides/week-1.1.html#illustration-1",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "slides/week-1.1.html#lets-get-going-.-.-.",
    "href": "slides/week-1.1.html#lets-get-going-.-.-.",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Let’s get going . . .",
    "text": "Let’s get going . . .\n\nYour first data visualizations…\n(and making sure we have R and RStudio installed and ready to roll)"
  },
  {
    "objectID": "slides/week-1.1.html#example-make-a-map",
    "href": "slides/week-1.1.html#example-make-a-map",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Example: Make a map!",
    "text": "Example: Make a map!"
  },
  {
    "objectID": "slides/week-1.1.html#example-plotting-democracy-over-time",
    "href": "slides/week-1.1.html#example-plotting-democracy-over-time",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Example: Plotting Democracy Over Time",
    "text": "Example: Plotting Democracy Over Time"
  },
  {
    "objectID": "slides/week-1.1.html#example-un-voting-trends",
    "href": "slides/week-1.1.html#example-un-voting-trends",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Example: UN Voting Trends",
    "text": "Example: UN Voting Trends"
  },
  {
    "objectID": "slides/week-1.1.html#your-task",
    "href": "slides/week-1.1.html#your-task",
    "title": "Quantitative Analysis for IA Practitioners",
    "section": "Your Task",
    "text": "Your Task\n\nMake sure R and RStudio installed (we can help if needed)\nCreate a folder for this class somewhere on your machine\n\nCreate a sub-folder called “classwork”\nDownload and save week1-classwork.qmd in that folder\n\nOpen the week1-classwork.qmd file in RStudio, which has code for 3 data viz activities\n\nMap making\nDemocracy Over Time\nUN Voting patterns\n\nFollow the instructions to update the code\nClick the little green arrow to run the code chunk\nClick Render to update your HTML output\nComplete as much as you can (no problem if you do not finish)"
  },
  {
    "objectID": "slides/week-11.2.html#multiple-logistic-regression-1",
    "href": "slides/week-11.2.html#multiple-logistic-regression-1",
    "title": "Regression Tables",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\n\nJust as with linear models, we can also run multiple logistic regressions\nWe can include multiple predictors in the model\nUsually we want to include our main predictor of interest and control variables\nThe interpetation of the coefficients is the same as in the bivariate models"
  },
  {
    "objectID": "slides/week-11.2.html#section",
    "href": "slides/week-11.2.html#section",
    "title": "Regression Tables",
    "section": "",
    "text": "Download the data using the peacesciencer package if you haven’t already…\n\n\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()"
  },
  {
    "objectID": "slides/week-11.2.html#section-1",
    "href": "slides/week-11.2.html#section-1",
    "title": "Regression Tables",
    "section": "",
    "text": "Example, including multiple predictors associated with conflict:\n\nconflict_model &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(factor(ucdponset) ~ ethfrac + relfrac + v2x_polyarchy + \n                          rugged + wbgdppc2011est + wbpopest,\n                  data= conflict_df,\n                  family = \"binomial\")\n\ntidy(conflict_model)\n\n# A tibble: 7 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     -5.69      1.41      -4.04  0.0000527\n2 ethfrac          0.800     0.381      2.10  0.0356   \n3 relfrac         -0.391     0.417     -0.939 0.348    \n4 v2x_polyarchy   -0.602     0.509     -1.18  0.237    \n5 rugged           0.0641    0.0760     0.843 0.399    \n6 wbgdppc2011est  -0.372     0.121     -3.08  0.00204  \n7 wbpopest         0.293     0.0673     4.35  0.0000134"
  },
  {
    "objectID": "slides/week-11.2.html#predicted-probabilities",
    "href": "slides/week-11.2.html#predicted-probabilities",
    "title": "Regression Tables",
    "section": "Predicted Probabilities",
    "text": "Predicted Probabilities\n\n\n\nShow the code\n# load the \nlibrary(marginaleffects)\n\n# seledct some countries for a given year\nselected_countries &lt;- conflict_df |&gt;\n  filter(\n    statename %in% c(\"United States of America\", \"Venezuela\", \"Rwanda\"),\n    year == 1999)\n\n# extract the model\nconflict_fit &lt;- conflict_model$fit\n\n# calculate margins for the subset\nmarg_effects &lt;- predictions(conflict_fit, newdata = selected_countries)\n\n# tidy the results\ntidy(marg_effects) |&gt;\n  select(estimate, p.value, conf.low, conf.high, statename)\n\n\n# A tibble: 3 × 5\n  estimate  p.value conf.low conf.high statename               \n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                   \n1   0.0123 2.23e-28  0.00567    0.0264 United States of America\n2   0.0140 2.25e-70  0.00879    0.0222 Venezuela               \n3   0.0286 8.46e-39  0.0170     0.0477 Rwanda"
  },
  {
    "objectID": "slides/week-11.2.html#your-turn",
    "href": "slides/week-11.2.html#your-turn",
    "title": "Regression Tables",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nRun a multivariate logistic model using conflict onset as the outcome variable\nSelect alternative variables and/or alternative measures of the same variables\nInterpret some of the coefficients\nCalculate the precicted probability in a handful of country-years based on your analysis\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-11.2.html#whats-in-a-regression-table",
    "href": "slides/week-11.2.html#whats-in-a-regression-table",
    "title": "Regression Tables",
    "section": "What’s in a Regression Table?",
    "text": "What’s in a Regression Table?"
  },
  {
    "objectID": "slides/week-11.2.html#regression-tables-with-modelsummary",
    "href": "slides/week-11.2.html#regression-tables-with-modelsummary",
    "title": "Regression Tables",
    "section": "Regression Tables with modelsummary",
    "text": "Regression Tables with modelsummary\n\n\nOftentimes we want to show multiple models at once (like F&L)\nWe want to compare across them and see which is the best model\nHow can we do that?\nThere are many ways to do this in R\nWe will use the modelsummary package"
  },
  {
    "objectID": "slides/week-11.2.html#run-multiple-models",
    "href": "slides/week-11.2.html#run-multiple-models",
    "title": "Regression Tables",
    "section": "Run Multiple Models",
    "text": "Run Multiple Models\n\n\nethnicity &lt;- glm(ucdponset ~ ethfrac + relfrac + wbgdppc2011est + wbpopest, # store each model in an object\n                  data = conflict_df,\n                  family = \"binomial\")\n\ndemocracy &lt;- glm(ucdponset ~ v2x_polyarchy + wbgdppc2011est +  wbpopest,\n                  data = conflict_df,\n                  family = \"binomial\")\n\nterrain &lt;- glm(ucdponset ~ rugged + wbgdppc2011est + wbpopest ,\n                  data = conflict_df,\n                  family = \"binomial\")\n\nfull_model &lt;- glm(ucdponset ~ ethfrac + relfrac + v2x_polyarchy + rugged +\n                        wbgdppc2011est + wbpopest,\n                  data = conflict_df,\n                  family = \"binomial\")"
  },
  {
    "objectID": "slides/week-11.2.html#prep-data-for-display",
    "href": "slides/week-11.2.html#prep-data-for-display",
    "title": "Regression Tables",
    "section": "Prep Data for Display",
    "text": "Prep Data for Display\n\n\nmodels &lt;- list(\"Ethnicity\" = ethnicity,  # store list of models in an object\n               \"Democracy\" = democracy, \n               \"Terrain\" = terrain, \n               \"Full Model\" = full_model)\n\ncoef_map &lt;- c(\"ethfrac\" = \"Ethnic Frac\",  # map coefficients\n        \"relfrac\" = \"Religions Frac\",     #(change names and order)\n        \"v2x_polyarchy\" = \"Polyarchy\",\n        \"rugged\" = \"Terrain\",\n        \"wbgdppc2011est\" = \"Per capita GDP\",\n        \"wbpopest\" = \"Population\",\n        \"(Intercept)\" = \"Intercept\")\n\ncaption = \"Table 1: Predictors of Conflict Onset\" # store caption\nreference = \"See appendix for data sources.\"      # store reference notes"
  },
  {
    "objectID": "slides/week-11.2.html#display-the-models",
    "href": "slides/week-11.2.html#display-the-models",
    "title": "Regression Tables",
    "section": "Display the Models",
    "text": "Display the Models\n\n\n \n\n  \n    \n    \n    tinytable_hxnn1kj38rfd9dtkp6ma\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Table 1: Predictors of Conflict Onset\n              \n                 \n                Ethnicity\n                Democracy\n                Terrain\n                Full Model\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nSee appendix for data sources.\n        \n                \n                  Ethnic Frac   \n                  0.744*   \n                           \n                           \n                  0.800*   \n                \n                \n                                \n                  (0.367)  \n                           \n                           \n                  (0.381)  \n                \n                \n                  Religions Frac\n                  -0.481   \n                           \n                           \n                  -0.391   \n                \n                \n                                \n                  (0.411)  \n                           \n                           \n                  (0.417)  \n                \n                \n                  Polyarchy     \n                           \n                  -0.228   \n                           \n                  -0.602   \n                \n                \n                                \n                           \n                  (0.436)  \n                           \n                  (0.509)  \n                \n                \n                  Terrain       \n                           \n                           \n                  0.031    \n                  0.064    \n                \n                \n                                \n                           \n                           \n                  (0.076)  \n                  (0.076)  \n                \n                \n                  Per capita GDP\n                  -0.474***\n                  -0.512***\n                  -0.543***\n                  -0.372** \n                \n                \n                                \n                  (0.104)  \n                  (0.108)  \n                  (0.092)  \n                  (0.121)  \n                \n                \n                  Population    \n                  0.282*** \n                  0.297*** \n                  0.299*** \n                  0.293*** \n                \n                \n                                \n                  (0.067)  \n                  (0.051)  \n                  (0.050)  \n                  (0.067)  \n                \n                \n                  Intercept     \n                  -4.703***\n                  -4.407***\n                  -4.296***\n                  -5.693***\n                \n                \n                                \n                  (1.327)  \n                  (1.205)  \n                  (1.143)  \n                  (1.408)  \n                \n                \n                  Num.Obs.      \n                  6364     \n                  6772     \n                  6840     \n                  6151"
  },
  {
    "objectID": "slides/week-11.2.html#your-turn-1",
    "href": "slides/week-11.2.html#your-turn-1",
    "title": "Regression Tables",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nGot to the peacesciencer documentation\nHow close are our data to F&L’s?\nCould we change something to better approximate their results?\nRun multiple models using different predictors\nDisplay the models using modelsummary\nTry to get as close to F&L as you can!\n\n\n\n\n−+\n12:00"
  },
  {
    "objectID": "slides/week-11.2.html#section-2",
    "href": "slides/week-11.2.html#section-2",
    "title": "Regression Tables",
    "section": "",
    "text": "This don’t look too good…\n\n\nShow the code\nmodelsummary(conflict_model, \n             stars = TRUE,  \n             gof_map = c(\"nobs\"),\n             coef_map = coef_map,\n             title = caption, \n             notes = reference)\n\n\n \n\n  \n    \n    \n    tinytable_xhz2vx33g1l22ylfx2dl\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        Table 1: Predictors of Conflict Onset\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nSee appendix for data sources.\n        \n                \n                  Ethnic Frac   \n                  0.800*   \n                \n                \n                                \n                  (0.381)  \n                \n                \n                  Religions Frac\n                  -0.391   \n                \n                \n                                \n                  (0.417)  \n                \n                \n                  Polyarchy     \n                  -0.602   \n                \n                \n                                \n                  (0.509)  \n                \n                \n                  Terrain       \n                  0.064    \n                \n                \n                                \n                  (0.076)  \n                \n                \n                  Per capita GDP\n                  -0.372** \n                \n                \n                                \n                  (0.121)  \n                \n                \n                  Population    \n                  0.293*** \n                \n                \n                                \n                  (0.067)  \n                \n                \n                  Intercept     \n                  -5.693***\n                \n                \n                                \n                  (1.408)  \n                \n                \n                  Num.Obs.      \n                  6151"
  },
  {
    "objectID": "slides/week-11.2.html#section-3",
    "href": "slides/week-11.2.html#section-3",
    "title": "Regression Tables",
    "section": "",
    "text": "So we can use ggplot to make a coefficient plot instead…\n\n\nShow the code\nlibrary(ggplot2)\n\nmodelplot(conflict_model, \n          coef_map = rev(coef_map), # rev() reverses list order\n          coef_omit = \"Intercept\", \n          color = \"blue\") + # use plus to add customizations like any ggplot object\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = .75) + # red 0 line\n  labs(\n    title = \"Figure 1: Predictors of Conflict Onset\",\n    caption = \"See appendix for data sources.\"\n  )"
  },
  {
    "objectID": "slides/week-11.2.html#your-turn-2",
    "href": "slides/week-11.2.html#your-turn-2",
    "title": "Regression Tables",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nTake one of your models\nUse modelplot to create a coefficient plot of it\nCustomize the plot to your liking\nInterpret the results\nDiscuss advantages of coefficient plots with a neighbor\n\n\n\n\n−+\n12:00"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This page displays an outline of the topics, content, and assignments for the term. Each module starts on a Monday. There are no assignments due on Sundays.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nReadings\nVideos\nModule\nSlides\nAssignments\n\n\n\n\n1\nJan 13\nCourse Intro\n\n📺\n📒\n🖥️\n\n\n\n\n\nGetting Started with Quarto\n📖\n📺\n📒\n🖥️\n\n\n\n2\nJan 20\nBar charts, histograms\n📖 📖\n📺 📺\n📒\n🖥️\n\n\n\n\n\nLine charts, scatter plots\n📖 📖\n📺 📺\n📒\n🖥️\n📘\n\n\n3\nJan 26\nDownloading and manipulating data\n📖\n📺\n📒\n🖥️\n\n\n\n\n\nSummarizing by groups\n📖\n📺\n📒\n🖥️\n📘\n\n\n4\nFeb 3\nHomework 1: Visualize modernization theory\n📖\n\n\n\n🧮\n\n\n5\nFeb 10\nCategorical data\n📖\n📺\n📒\n🖥️\n\n\n\n\n\nContinuous data\n📖\n📺\n📒\n🖥️\n📘\n\n\n6\nFeb 17\nDescribing distributions\n📖\n\n📒\n🖥️\n\n\n\n\n\nSampling and uncertainty\n📖\n📺 📺\n📒\n🖥️\n📘\n\n\n7\nFeb 24\nSingle proportion tests\n📖\n\n📒\n🖥️\n\n\n\n\n\nDifferences between two groups\n📖\n📺\n📒\n🖥️\n📘\n\n\n8\nMar 3\nHomework 2: Evaluate a claim\n📖\n\n\n\n🧮\n\n\n\nMar 10\nNo Class\nSpring Break\n☀️🌴🏄\n\n\n\n\n\n9\nMar 17\nFitting a line to data\n📖\n📺 📺\n📒\n🖥️\n📘\n\n\n\n\nLeast squares regression\n📖\n📺 📺\n📒\n🖥️\n\n\n\n10\nMar 24\nMultiple linear regression\n📖\n📺\n📒\n\n📘\n\n\n\n\nDealing with outliers\n📖 📖 📖\n📺 📺\n📒 📗\n\n📘\n\n\n11\nMar 31\nLogistic regression\n📖\n📺 📺\n📒\n🖥️\n\n\n\n\n\nInterpreting results\n\n📺\n📒\n🖥️\n📘\n\n\n12\nApr 7\nMultiple predictors and interactions\n📖\n📺\n📒\n🖥️\n\n\n\n\n\nVisualizing and communicating results\n\n📺 📺\n📒\n🖥️\n📘\n\n\n13\nApr 14\nHomework 3: Analyzing conflict\n\n\n\n\n🧮\n\n\n14\nApr 21\nFinal project workshop\n\n\n\n🖥️ 🖥️\n✍️",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "weeks/week-z.html",
    "href": "weeks/week-z.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Review the course syllabus and support resources\n📖 r4ds\n\n8.1 & 8.2 on importing data\n6.1 - 6.3 on tidy data\n20.3 on mutating joins\n4.5 on grouping data\n\n\n\n\n📖 Norris, The Impact of Electoral Reform on Women’s Representation"
  },
  {
    "objectID": "weeks/week-z.html#readings",
    "href": "weeks/week-z.html#readings",
    "title": "Week 1",
    "section": "",
    "text": "📖 Review the course syllabus and support resources\n📖 r4ds\n\n8.1 & 8.2 on importing data\n6.1 - 6.3 on tidy data\n20.3 on mutating joins\n4.5 on grouping data\n\n\n\n\n📖 Norris, The Impact of Electoral Reform on Women’s Representation"
  },
  {
    "objectID": "weeks/week-z.html#modules",
    "href": "weeks/week-z.html#modules",
    "title": "Week 1",
    "section": "Modules",
    "text": "Modules\nModule 1.1–Working with Flat Files\nModule 1.2–Working with APIs"
  },
  {
    "objectID": "weeks/week-z.html#assignments",
    "href": "weeks/week-z.html#assignments",
    "title": "Week 1",
    "section": "Assignments",
    "text": "Assignments\n📘 Quiz 1\n📘 Quiz 2\n🧮 Coding Assignment 1"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\nHomework 2 due by 11:59pm on Sunday, March 8.",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#readings",
    "href": "weeks/week-8.html#readings",
    "title": "Week 8",
    "section": "Readings",
    "text": "Readings\n📖 Casey et al.",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#assignments",
    "href": "weeks/week-8.html#assignments",
    "title": "Week 8",
    "section": "Assignments",
    "text": "Assignments\n🧮 Homework 2",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, April 5.",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#readings",
    "href": "weeks/week-12.html#readings",
    "title": "Week 12",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 9.3 and 9.4",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#videos",
    "href": "weeks/week-12.html#videos",
    "title": "Week 12",
    "section": "Videos",
    "text": "Videos\n📺 Effect Multipliers in Logistic Regression\n📺 Display Multiple Models with Model Summary\n📺 Coefficient Plots for Single Models",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#modules",
    "href": "weeks/week-12.html#modules",
    "title": "Week 12",
    "section": "Modules",
    "text": "Modules\n📒 Module 12.1\n📒 Module 12.2",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#assignments",
    "href": "weeks/week-12.html#assignments",
    "title": "Week 12",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, March 29.",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#readings",
    "href": "weeks/week-11.html#readings",
    "title": "Week 11",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 9.1 and 9.2",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#videos",
    "href": "weeks/week-11.html#videos",
    "title": "Week 11",
    "section": "Videos",
    "text": "Videos\n📺 Classification: Motivation\n📺 Logistic Regression\n📺 Odds Ratios in Logistic Regression",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#modules",
    "href": "weeks/week-11.html#modules",
    "title": "Week 11",
    "section": "Modules",
    "text": "Modules\n📒 Module 11.1\n📒 Module 11.2",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#assignments",
    "href": "weeks/week-11.html#assignments",
    "title": "Week 11",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, January 25",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#readings",
    "href": "weeks/week-2.html#readings",
    "title": "Week 2",
    "section": "Readings",
    "text": "Readings\n📖 r4ds, ch 7\n📖 r4ds, ch 2\n📖 r4ds, 10.1 - 10.4\n📖 r4ds, 12.2",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#videos",
    "href": "weeks/week-2.html#videos",
    "title": "Week 2",
    "section": "Videos",
    "text": "Videos\n🖥 Make a Bar Chart in R with ggplot2\n🖥 Make a Histogram in R with ggplot2\n🖥 Make a Line Chart in R with ggplot2\n🖥 Make a Scatterplot in R with ggplot2",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#assignments",
    "href": "weeks/week-2.html#assignments",
    "title": "Week 2",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab 1, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Review the course syllabus and support resources\n📖 Read this blog about a project-oriented workflow in R.",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#readings",
    "href": "weeks/week-1.html#readings",
    "title": "Week 1",
    "section": "",
    "text": "📖 Review the course syllabus and support resources\n📖 Read this blog about a project-oriented workflow in R.",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#videos",
    "href": "weeks/week-1.html#videos",
    "title": "Week 1",
    "section": "Videos",
    "text": "Videos\n🖥️ R and RStudio\n🖥 Getting Started with Quarto",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, February 15.",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#readings",
    "href": "weeks/week-5.html#readings",
    "title": "Week 5",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 4\n📖 IMS, chap. 5.1 - 5.3",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#videos",
    "href": "weeks/week-5.html#videos",
    "title": "Week 5",
    "section": "Videos",
    "text": "Videos\n📺 Exploring Categorical Data\n📺 Exploring Numerical Data",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#modules",
    "href": "weeks/week-5.html#modules",
    "title": "Week 5",
    "section": "Modules",
    "text": "Modules\n📒 Module 5.1\n📒 Module 5.2",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#assignments",
    "href": "weeks/week-5.html#assignments",
    "title": "Week 5",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, February 22.",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#readings",
    "href": "weeks/week-6.html#readings",
    "title": "Week 6",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 5.4 - 5.5\n📖 IMS, chap. 12",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#videos",
    "href": "weeks/week-6.html#videos",
    "title": "Week 6",
    "section": "Videos",
    "text": "Videos\n📺 The Right and Wrong Way to Interpret a CI\n📺 Bootstrapping Main Ideas",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#modules",
    "href": "weeks/week-6.html#modules",
    "title": "Week 6",
    "section": "Modules",
    "text": "Modules\n📒 Module 6.1\n📒 Module 6.2",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#assignments",
    "href": "weeks/week-6.html#assignments",
    "title": "Week 6",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "assignments/homework-2.html",
    "href": "assignments/homework-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Community Driven Development (CDD) programs have become increasingly popular in international development over the past several decades. CDD programs directly involve impacted communities in the selection, design, and implementation of development projects. Usually, this also involves building upon or creating community level organizations (or committees) that are involved with the projects, with the goal of creating democratic and inclusive decision-making processes. See this 3ie working paper for discussion of some common features of CDD.\nGiven these design features, proponents of CDD argue that these programs should (a) improve the quality and impact of development projects (e.g., public goods) AND (b) improve social cohesion, inclusion, and local governance. For this reason, CDD has been implemented in a number of post-conflict settings, where the hope is that CDD can both improve development outcomes and contribute to rebuilding social cohesion and governance (which are often harmed by conflict).\nDoes CDD improve development outcomes? Does it improve social cohesion after conflict? Your goal in this assignment is to answer these questions, using data from an evaluation of a CDD program in post-conflict Sierra Leone.\nThe relevant files for this assignment are available in the classwork folder in the homework-2 file.",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#overview",
    "href": "assignments/homework-2.html#overview",
    "title": "Assignment 2",
    "section": "",
    "text": "Community Driven Development (CDD) programs have become increasingly popular in international development over the past several decades. CDD programs directly involve impacted communities in the selection, design, and implementation of development projects. Usually, this also involves building upon or creating community level organizations (or committees) that are involved with the projects, with the goal of creating democratic and inclusive decision-making processes. See this 3ie working paper for discussion of some common features of CDD.\nGiven these design features, proponents of CDD argue that these programs should (a) improve the quality and impact of development projects (e.g., public goods) AND (b) improve social cohesion, inclusion, and local governance. For this reason, CDD has been implemented in a number of post-conflict settings, where the hope is that CDD can both improve development outcomes and contribute to rebuilding social cohesion and governance (which are often harmed by conflict).\nDoes CDD improve development outcomes? Does it improve social cohesion after conflict? Your goal in this assignment is to answer these questions, using data from an evaluation of a CDD program in post-conflict Sierra Leone.\nThe relevant files for this assignment are available in the classwork folder in the homework-2 file.",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#the-gobifo-program-in-sierra-leone",
    "href": "assignments/homework-2.html#the-gobifo-program-in-sierra-leone",
    "title": "Assignment 2",
    "section": "The GoBifo Program in Sierra Leone",
    "text": "The GoBifo Program in Sierra Leone\nSierra Leone was devastated by a civil war lasting from 1991 to 2002. In the post-conflict context, the Ministry of Local Government and Community Development implemented the GoBifo (“forge ahead”) CDD project in Sierra Leone from 2005-2009 (with support from the World Bank and other international donors). See this report for a detailed summary of the program.\nThe program had two key components:\n\nBlock grants of about 4,667 dollars (about 100 per household in each community) that communities could allocate to local development projects, skills trainings, and small-business investment.\n“Technical assistance that promoted democratic decision-making, the participation of socially marginalized women and youth in local politics, and transparent budgeting practices” (Casey, Glennerster, and Miguel 2012)\n\nThe GiBifo program also included an impact evaluation, conducted by economists Katherine Casey, Rachel Glennerster, and Edward Miguel. The academic paper generated from the project is in the References section (Casey, Glennerster, and Miguel 2012).\nWait until after you complete your assignment to read the full paper.\nImportantly, the impact evaluation included randomization: communities in the study were randomly assigned to receive the GoBifo program, or not. In this sense, the impact evaluation was similar to a clinical drug trial, where some subjects are randomly assigned to receive the medicine while others do not receive it. This aspect of the design makes it easier to assess the causal impact of this project: we can compare outcomes in the treatment (GoBifo program) and control groups, and test for whether any differences we observe are likely to be due to chance.",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#load-data-and-packages",
    "href": "assignments/homework-2.html#load-data-and-packages",
    "title": "Assignment 2",
    "section": "Load Data and Packages",
    "text": "Load Data and Packages\nYou can use the code below to load the packages and data. Make sure the .csv file is saved in your RProj folder for this project.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nmyData &lt;- read_csv(\"gobifo_data.csv\")",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#data",
    "href": "assignments/homework-2.html#data",
    "title": "Assignment 2",
    "section": "Data",
    "text": "Data\n“gobifo_data.csv” provides a condensed version of the data collected by Casey, Glennerster, and Miguel (2012) for their evaluation of the project. The study team gathered literally hundreds of outcome variables, and the paper includes an interesting discussion of how they analyze the data and interpret the results given the large number of outcomes they measured.\nThe unit of analysis (or row in the dataset) is the village.\nFor our purposes, we will focus on a smaller set of variables, summarized below:\n\nTreatment variable (named t in the data): this equals treatment if the community was randomly assigned to be in the program, and control if not. As a character variable, tidymodels will automatically recognize it as a factor. But be sure to use quotation marks when working with the two categories, e.g. filter(t == \"treatment\").\nCommunity Decision Making Infrastructure: Is there a village development committee in the community? (vdc)\nPublic Goods: Is there a functioning primary school in the community? (f_psch)\nInclusion, Gender equality in participatory decision making (role_wmn).\n\n“Enumerator account of how actively women participated in the deliberation compared to men, ranging from 5 = no difference between women and men to 1 = women not active at all compared to men”\n\nSocial Capital: Trust in other in the community (trust_own)\nConflict: Percentage of respondents in the village that reported they had NO conflicts/disputes with others that required outside intervention (no_conflict)",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#part-1-setup-and-load-data-10-points",
    "href": "assignments/homework-2.html#part-1-setup-and-load-data-10-points",
    "title": "Assignment 2",
    "section": "Part 1: Setup and Load Data (10 points)",
    "text": "Part 1: Setup and Load Data (10 points)\n\nCreate a folder and an RProj file for this assignment. Save “gobifo_data.csv” in that folder. Create a new Quarto document where you will complete the work for this assignment. Make sure to include a title, your name, and the date. (1 point)\nCreate a code chunk that loads the packages you will need: tidyverse and tidymodels (1 point)\nUse read_csv() to read the “gobifo_data.csv” data in to RStudio. Save the dataset as an object called “myData” (3 points)\nExamine the data. How many observations (villages) are there in the study? You can use glimpse() to determine the number of rows. How many were in the treatment (t) group (in the program) and how many were in control? You can use summarize() for this calculation. (5 points)",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#part-2-generate-estimates-and-95-percent-confidence-intervals-30-points",
    "href": "assignments/homework-2.html#part-2-generate-estimates-and-95-percent-confidence-intervals-30-points",
    "title": "Assignment 2",
    "section": "Part 2: Generate Estimates and 95 Percent Confidence Intervals (30 points)",
    "text": "Part 2: Generate Estimates and 95 Percent Confidence Intervals (30 points)\nThere are five response variables in the data. Each of these corresponds to a different outcome of interest: village decision making infrastructure (vdc), public goods (f_psch), inclusion/gender equality (role_wmn), social capital and trust (trust_own), and social conflict (no_conflict).\n\nFor each of the outcomes: use group_by() and summarize() to calculate the mean of each response variable in both treatment and control villages. (5 points)\nSelect TWO outcome variables of interest to you. For each, use tidymodels to generate 95 percent confidence intervals around these estimates. You should do this for villages in treatment and control separately. (10 points)\nStore your estimates and the upper and lower bounds of your confidence intervals for future use. Label this object estimates. (5 points)\nWrite a short paragraph that correctly interprets the confidence intervals in treatment and control groups for both outcomes you selected. Based on your confidence intervals, what is your initial conclusion about whether the program had an impact on the outcome variables you selected. (10 points)",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#part-3-graph-your-estimates-and-confidence-intervals-20-points",
    "href": "assignments/homework-2.html#part-3-graph-your-estimates-and-confidence-intervals-20-points",
    "title": "Assignment 2",
    "section": "Part 3: Graph your estimates and confidence intervals (20 points)",
    "text": "Part 3: Graph your estimates and confidence intervals (20 points)\n\nFor both outcomes you chose, graph your estimate and 95 percent confidence interval in treatment and control groups. You can produce a bar plot or use geom_point, whichever you think best displays the information. This should generate 2 separate graphs, one for each outcome. For full credit, produce graphs that you could use professionally: that is, these graphs should use appropriate labels, colors, and a nice theme (15 points)\n\nBonus 2 points if you can figure out how to include all of this information on one graph!\n\nWrite a short paragraph interpreting the graphs: what do they teach us about the impact of the program on the two outcomes in treatment vs control. (5 points)",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#part-4-conduct-hypothesis-tests-30-points",
    "href": "assignments/homework-2.html#part-4-conduct-hypothesis-tests-30-points",
    "title": "Assignment 2",
    "section": "Part 4: Conduct Hypothesis Tests (30 points)",
    "text": "Part 4: Conduct Hypothesis Tests (30 points)\nIn this section, you wil conduct hypothesis tests that assess whether the GoBifo program had an impact.\n\nFor the two outcomes you selected: Clearly state the null hypothesis and the alternative hypothesis for each test. Write these out in a list. (5 points)\nFor each outcome, calculate the treatment effect of the program: this is the mean in treatment minus the mean in control. What are the treatment effects? (5 points)\nFor each outcome, use tidymodels to conduct a hypothesis test to assess whether the treatment effects you estimate are likely to be due to chance. For each, produce the p-value. (10 points)\nSelect one outcome: make a histogram of the null distribution from your hypothesis test. Adjust the bar width as needed. Include a dotted vertical line with your estimate on the plot. Generally, what does this graph tell you about how likely your treatment effect is to be due to chance alone? (5 points)\nInterpret both p-values in language that people who know nothing about statistics might understand. For each outcome (response) variable, do you reject the null hypothesis? (5 points)",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#part-5-interpretation-and-conclusion-10-points",
    "href": "assignments/homework-2.html#part-5-interpretation-and-conclusion-10-points",
    "title": "Assignment 2",
    "section": "Part 5: Interpretation and Conclusion (10 points)",
    "text": "Part 5: Interpretation and Conclusion (10 points)\nWrite 2-3 paragraphs that summarize the results of your analysis and accurately interpret them. What is the big picture takeaway of your analysis? What does your analysis teach us about the impact of this CDD program? Are there policy or program implications of the findings for CDD in post-conflict settings?",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/homework-2.html#extra-credit",
    "href": "assignments/homework-2.html#extra-credit",
    "title": "Assignment 2",
    "section": "Extra Credit",
    "text": "Extra Credit\n\nThere are three outcome variables you did not select: create visualizions that display the means and 95% confidence intervals in treatment and control groups for these three outcomes. (2 points)\nConduct hypothesis tests for each the three additional outcomes. What do you conclude from each test? (2 points)\nWrite a paragraph updating your conclusion about the impact of the program and your interpretation of all of the evidence (2 points)\nCreate a single visualization that could “tell the whole story” of the results from this study. (3 points)\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nHead over to Blackboard and go to the Homework 2 assignment. There you will find a link that you can use to upload your project folder.\nIf you worked with a partner, only one of you should submit the project folder but both of you should still make a submission to Blackboard so that we have a submission to grade. If you have worked with someoone but still have a substantially different project, you can submit separately.\nThe person submitting the project folder should take a screen shot of the confirmation message (“Finished Uploading”) and share it with the other group members. This screen shot will be what you submit in Blackboard.\nNext, click “Create Submission” and write a statement stating whether you worked on the project independently or whether you are submitting jointly with someone else. Also note whether you used a large language model (LLM) like ChatGPT to help you with the work and which parts of the assignment you used the LLM for. It is fine to use an LLM to help with your code but you must acknowledge the use of it in your submission.\nThen, upload the screenshot you saved in the first step. Now submit the assignment.\nPlease remember to upload your entire project folder and not just the Quarto document.",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio\n🔗 go here to install R and R Studio\n\n\nQuarto\n🔗 see the Guide and Reference sections\n\n\nPosit Cheat Sheets\n🔗 cheat sheets for important packages\n\n\nQuizzes, etc.\n🔗 on Blackboard\n\n\nProf T’s Zoom Office\n🔗 on Zoom\n\n\nOffice Hours Appointments\n🔗 on Calendly",
    "crumbs": [
      "Course Information",
      "Useful links"
    ]
  },
  {
    "objectID": "assignments/homework-1.html",
    "href": "assignments/homework-1.html",
    "title": "Homework 1",
    "section": "",
    "text": "For this assignment, you are going to evaluate modernization theory as laid out in Seymour Martin Lipset’s classic article entitled “Some Social Requisites of Democracy: Economic Development and Political Legitimacy.” How classic is this article? According to Google Scholar, this piece has been cited more than 11.5 thousand times!\nWe are going to use data from V-Dem and modern data viz tools to explore Lipset’s hypothesis that economic modernization is highly correlated with democracy. We have already done this to some extent by looking at the relationship between wealth and the polyarchy score. But we are going to broaden things out by looking at other measures of modernization and democracy contained in the V-Dem dataset.\nBefore starting on this assignment, you will want to have a look at the V-Dem codebook. Look through the sections titled “V-Dem Democracy Indices” (section 2 of the codebook) and “Background Factors (E).” There are five democracy indicators, one of which is the polyarchy index. There are a number of background factors, many of which pertain to economic modernization. We are going to be looking at the relationship between these two sets of variables. In the code book, you will also find a list of country names and codes, which will be useful for downloading and filtering the data.\nNow have a look at “Some Social Requisites of Democracy” and in particular pay attention to the indicators in Table II and the discussion surrounding them. Think of each indicator (e.g. urbanization, education, etc.) as a sub-hypothesis of his theory. Which of these sub-hypotheses about modernization do you think is most compelling? Which would you like to test?\nYou have the option of doing this assignment in Posit Cloud or downloading the project folder and working locally. Either way you must submit your zipped project folder with the rendered HTML file included (see submission instructions at the end of the document) to Blackboard.",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#overview",
    "href": "assignments/homework-1.html#overview",
    "title": "Homework 1",
    "section": "",
    "text": "For this assignment, you are going to evaluate modernization theory as laid out in Seymour Martin Lipset’s classic article entitled “Some Social Requisites of Democracy: Economic Development and Political Legitimacy.” How classic is this article? According to Google Scholar, this piece has been cited more than 11.5 thousand times!\nWe are going to use data from V-Dem and modern data viz tools to explore Lipset’s hypothesis that economic modernization is highly correlated with democracy. We have already done this to some extent by looking at the relationship between wealth and the polyarchy score. But we are going to broaden things out by looking at other measures of modernization and democracy contained in the V-Dem dataset.\nBefore starting on this assignment, you will want to have a look at the V-Dem codebook. Look through the sections titled “V-Dem Democracy Indices” (section 2 of the codebook) and “Background Factors (E).” There are five democracy indicators, one of which is the polyarchy index. There are a number of background factors, many of which pertain to economic modernization. We are going to be looking at the relationship between these two sets of variables. In the code book, you will also find a list of country names and codes, which will be useful for downloading and filtering the data.\nNow have a look at “Some Social Requisites of Democracy” and in particular pay attention to the indicators in Table II and the discussion surrounding them. Think of each indicator (e.g. urbanization, education, etc.) as a sub-hypothesis of his theory. Which of these sub-hypotheses about modernization do you think is most compelling? Which would you like to test?\nYou have the option of doing this assignment in Posit Cloud or downloading the project folder and working locally. Either way you must submit your zipped project folder with the rendered HTML file included (see submission instructions at the end of the document) to Blackboard.",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#step-1-gather-your-data-20-pts",
    "href": "assignments/homework-1.html#step-1-gather-your-data-20-pts",
    "title": "Homework 1",
    "section": "Step 1: Gather your data (20 pts)",
    "text": "Step 1: Gather your data (20 pts)\nInsert a code chunk below this paragraph and label it. Use the vdemdata package to download data for your analysis. Since we already looked at the polyarchy score and wealth in class, you need to use a different measure of democracy and a different background factor for your analysis. Use a select() verb to include country, year, region (e_regionpol_6C), at least one of the other four measures of democracy, and one background factor that is not per capita GDP. Store your data in an object called dem_data. Pipe in a mutate() verb and use case_match() to label the regions. Review module 3.1 if you are confused on how to do this.",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#step-2-make-a-line-chart-showing-country-trends-20-pts",
    "href": "assignments/homework-1.html#step-2-make-a-line-chart-showing-country-trends-20-pts",
    "title": "Homework 1",
    "section": "Step 2: Make a line chart showing country trends (20 pts)",
    "text": "Step 2: Make a line chart showing country trends (20 pts)\na) Insert a code chunk below this paragraph and label it. Filter your dem_data to include three or four countries at various levels of economic development and create a line chart of your democracy indicator. See the World Bank country classifications by income level to make your selections. Save the data as a new data frame called dem_data_line.\nNote: From here on out I will expect you to know to add a code chunk and label it. So I won’t keep repeating that portion of the instructions.\nb) Make a line chart using ggplot2. Be sure to specify x =, y = and color = in your aes() call and use geom_line() to create the chart. Add a colorblind-friendly color map using viridis. Now add appropriate axis labels, a title and a caption.\nc) In a few sentences, interpret your chart. Have the more developed countries achieved a higher level of democracy? Put your answer right below this line in markdown text.",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#step-3-make-a-bar-chart-comparing-regional-levels-20-pts",
    "href": "assignments/homework-1.html#step-3-make-a-bar-chart-comparing-regional-levels-20-pts",
    "title": "Homework 1",
    "section": "Step 3: Make a bar chart comparing regional levels (20 pts)",
    "text": "Step 3: Make a bar chart comparing regional levels (20 pts)\na) Going back to your original dem_data data frame, filter the data for a single year and then group by region and summarize your democracy indicator by mean. Save the new data in an object called bar_chart_data.\nb) Use ggplot() and geom_col() to create a bar chart showing levels of democracy across the regions with bar_chart_data. Make sure to add appropriate axis labels, a title and a caption. Change the fill color and add a theme to spruce it up a bit.\nc) Interpret your bar chart. Do you see evidence that more developed regions have higher levels of democracy?",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#step-4-make-a-scatter-plot-to-show-20-pts",
    "href": "assignments/homework-1.html#step-4-make-a-scatter-plot-to-show-20-pts",
    "title": "Homework 1",
    "section": "Step 4: Make a scatter plot to show (20 pts)",
    "text": "Step 4: Make a scatter plot to show (20 pts)\na) Start with the dem_data data frame again, Now take an average of multiple years using group_by() and summarize() to analyze. If you choose a recent period, make sure that the data are available. Some of the background variables in V-Dem are not entirely up to date. You can check the availability of the data by looking at the V-Dem codebook or using glimpse() or View() to look at your data. Save your your data in a new object called dem_data_scatter.\nb) Now build a scatter plot with ggplot2. Put your modernization-related variable (background variable) on the x-axis and your measure of democracy on the y-axis and color the points by region. Add a trend line with geom_smooth(). Add appropriate labels and a viridis color map. Change the theme to theme_minimal.",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#step-5-conclusion-20-pts",
    "href": "assignments/homework-1.html#step-5-conclusion-20-pts",
    "title": "Homework 1",
    "section": "Step 5: Conclusion (20 pts)",
    "text": "Step 5: Conclusion (20 pts)\nRender your document and write a brief conclusion to your analysis. What did you find? Did you find support for Lipset’s theory? What are the limitations of your analysis?",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-1.html#step-6-bonus-questions-one-point-each",
    "href": "assignments/homework-1.html#step-6-bonus-questions-one-point-each",
    "title": "Homework 1",
    "section": "Step 6: Bonus questions (one point each)",
    "text": "Step 6: Bonus questions (one point each)\na) Facet wrap your scatter plot by region.\nb) Remove the facet_wrap() call and display the relationship for one region and use geom_text() to label your points.\nc) Remove the text labels and make your scatter plot interactive using ggplotly(). Make sure that your tooltip includes the information that you want to display to the user.\nd) Add annotation to your plot using hline() or vline() to highlight a significant point in your data, like the mean or some other significant value.\ne) Upload your rendered HTML file to Quarto Pub and share the link with the class on Discord.\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nHead over to Blackboard and go to the Homework 1 assignment. Click “Create Sumbission” and write a brief statement saying that you have submitted the assignment and that all of the work is your own.\nFrom there you have upload a compressed (zipped) version of your project folder including the rendered HTML file. To compress your project folder, right-click the project folder and choose Compress (Mac) or Send to → Compressed (zipped) folder (Windows). Then, upload the resulting .zip file.\nIf you like, you can also upload your rendered HTML as a webpage to Quarto Pub and share the link below your submission statement.",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/homework-3.html",
    "href": "assignments/homework-3.html",
    "title": "Homework 3",
    "section": "",
    "text": "In this homework assignment, we are going to be exploring the hugely influential article by Fearon and Laitin (2003) on civil war onset. Click here to view the original article and see Table 1 for the model that we will be replicating. Click here to access the assignment Dropbox folder that includes the data we will be using for this exercise.",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#load-the-data",
    "href": "assignments/homework-3.html#load-the-data",
    "title": "Homework 3",
    "section": "Load the Data",
    "text": "Load the Data\nLoad the data which is included in the “fl_data_model1.csv” file in the Homework 3 assignment folder.\nThe unit of analysis is the country-year.\n\nOutcome Variable\n\nonset - Whether a country-year experienced civil war onset (1 = yes, 0 = no)\n\n\n\nMain Predictors\n\nwarl- Whether had a war ongoing in the previous year\ngdpenl - GDP per capita (not logged)\nlpopl1 - The log of population lagged by one year\nlmtnest - The log of the percentage of country that has mountainous terrain\nncontig - An indicator for noncontiguous state\nOil - An indicator for whether the country is an oil exporter\nnwstate - An indicator for whether a country was created in the last two years\ninstab - An indicator for whether the Polity score changes by 3 or more in a given year\npolity2l - The Polity2 democracy score\nethfrac - Ethno-linguistic fractionalization index\nrelfrac - Religious fractionalization index",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#replicate-model-1-from-fearon-and-laitin-20",
    "href": "assignments/homework-3.html#replicate-model-1-from-fearon-and-laitin-20",
    "title": "Homework 3",
    "section": "Replicate Model 1 from Fearon and Laitin (20%)",
    "text": "Replicate Model 1 from Fearon and Laitin (20%)\nRun the first model (Model 1) from Table 1 of the Fearon and Laitin article. Use summary() to view the results. Interpret the coefficient on the Oil exporter variable. How much does being an oil exporter increase the risk of conflict onset?",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#display-the-results-as-a-coefficient-plot-20",
    "href": "assignments/homework-3.html#display-the-results-as-a-coefficient-plot-20",
    "title": "Homework 3",
    "section": "Display the Results as a Coefficient Plot (20%)",
    "text": "Display the Results as a Coefficient Plot (20%)\nNow use the modelplot() function from the modelsummary package to display the results as a coefficient plot.\nWhen making your coefficient plot, start with the coefficient map. You might want to write it in the order that you’d like to see the coefficients displayed and then reverse it so that you can use the same map for your regression table below.\nBe sure to load the modelsummary and ggplot2 packages before you get started with the rest of your code.\nInterpret the results. Which predictor has largest positive effect on the probability of conflict onset? Which one has the strongest negative effect?",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#create-a-regression-table-20",
    "href": "assignments/homework-3.html#create-a-regression-table-20",
    "title": "Homework 3",
    "section": "Create a Regression Table (20%)",
    "text": "Create a Regression Table (20%)\nDo stepwise regressions with different categories of regressors based on the same model to see how things change as you put different sets of variables into the model. Show the results of your models in separate columns in a regression table.\nStep one: Run your glm() models and store them as objects. Here you might want to summarize them with summary() or tidy() from the broom package as you go along.\nStep two: Save your models as a list and set up your caption and reference note.\nStep three: Call modelsummary to display the table as a gt object.\nStep four: Briefly interpret your results. What changes as you add different categories of variables to the model? Do any results become significant that were not significant before?",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#show-odds-ratios-and-interpret-20",
    "href": "assignments/homework-3.html#show-odds-ratios-and-interpret-20",
    "title": "Homework 3",
    "section": "Show Odds Ratios and Interpret (20%)",
    "text": "Show Odds Ratios and Interpret (20%)\nNow make the table display odds ratios instead of coefficients and interpret one or two of the coefficients that you find interesting. (Note: you have already talked about Oil, so choose a different one for this part.)",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#display-predicted-probabilities-20",
    "href": "assignments/homework-3.html#display-predicted-probabilities-20",
    "title": "Homework 3",
    "section": "Display Predicted Probabilities (20%)",
    "text": "Display Predicted Probabilities (20%)\nNow display predicted probabilities for a handful of country-years. Filter the data for a particular year and the country cases that you are interested in. Find the predicted probabilities using the precitions() function from the marginaleffects package. Interpret the results. Why do some of the countries you chose have a higher probability of conflict onset than the others?",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/homework-3.html#extra-credit-4-pts",
    "href": "assignments/homework-3.html#extra-credit-4-pts",
    "title": "Homework 3",
    "section": "Extra Credit (4 pts)",
    "text": "Extra Credit (4 pts)\nIf you would like to do some extra credit, you can try to recreate and extend the F&L analysis with data from the peacesciencer package. Display your results in a regression table or coefficient plot.\nHere are some questions to answer about your analysis:\n\nWhich variables were you able to include from peacesciencer? What variables that were in F&L were you not able to find? What are you missing? (1 pt.)\nDo you get results similar to F&L with the peacesciencer data if you take a similar timeframe? (1 pt.)\nIf you rerun the F&L model with the same variables that you have available for peacesciencer, do you get similar results? If there are differences, why would this be? (1 pt.)\nWhat happens if you extend the analysis from 1945-1999 to 1945-present using the peacesciencer package? What changes? (1 pt.)\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nHead over to Blackboard and go to the Homework 2 assignment. There you will find a link that you can use to upload your project folder.\nIf you worked with a partner, only one of you should submit the project folder but both of you should still make a submission to Blackboard so that we have a submission to grade. If you have worked with someoone but still have a substantially different project, you can submit separately.\nThe person submitting the project folder should take a screen shot of the confirmation message (“Finished Uploading”) and share it with the other group members. This screen shot will be what you submit in Blackboard.\nNext, click “Create Submission” and write a statement stating whether you worked on the project independently or whether you are submitting jointly with someone else. Also note whether you used a large language model (LLM) like ChatGPT to help you with the work and which parts of the assignment you used the LLM for. It is fine to use an LLM to help with your code but you must acknowledge the use of it in your submission.\nThen, upload the screenshot you saved in the first step. Now submit the assignment.\nPlease remember to upload your entire project folder and not just the Quarto document.",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, March 1.",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#readings",
    "href": "weeks/week-7.html#readings",
    "title": "Week 7",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 16\n📖 IMS, chap. 11",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#videos",
    "href": "weeks/week-7.html#videos",
    "title": "Week 7",
    "section": "Videos",
    "text": "Videos\n📺 Using Randomization to Analyze Gender Discrimination",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#modules",
    "href": "weeks/week-7.html#modules",
    "title": "Week 7",
    "section": "Modules",
    "text": "Modules\n📒 Module 7.1\n📒 Module 7.2",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#assignments",
    "href": "weeks/week-7.html#assignments",
    "title": "Week 7",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nHomework Assignment 1 due by 11:59pm on Sunday, February 8.",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#readings",
    "href": "weeks/week-4.html#readings",
    "title": "Week 4",
    "section": "Readings",
    "text": "Readings\n📖 Lipset, Some Social Requisites of Democracy",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#assignments",
    "href": "weeks/week-4.html#assignments",
    "title": "Week 4",
    "section": "Assignments",
    "text": "Assignments\n📘 Homework 1",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, February 1.",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#readings",
    "href": "weeks/week-3.html#readings",
    "title": "Week 3",
    "section": "Readings",
    "text": "Readings\n📖 r4ds, ch 3",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#videos",
    "href": "weeks/week-3.html#videos",
    "title": "Week 3",
    "section": "Videos",
    "text": "Videos\n🖥 Transforming Data with dplyr Verbs\n🖥 Summarizing Data by Groups with dplyr Verbs",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#assignments",
    "href": "weeks/week-3.html#assignments",
    "title": "Week 3",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab 2, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, March 22.",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#readings",
    "href": "weeks/week-10.html#readings",
    "title": "Week 10",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 8.1 and 8.2\n📖 IMS, chap. 7.3\n📖 IMS, chap. 5.7\n📖 IMS, chap. 24.6",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#videos",
    "href": "weeks/week-10.html#videos",
    "title": "Week 10",
    "section": "Videos",
    "text": "Videos\n📺 Linear Regression with Multiple Variables\n📺 What Should You Do With Outliers?\n📺 Checking Model Assumptions with Residual Plots",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#modules",
    "href": "weeks/week-10.html#modules",
    "title": "Week 10",
    "section": "Modules",
    "text": "Modules\n📒 Module 10.1\n📒 Module 10.2\n📗 Module 10.3 (Optional)",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#assignments",
    "href": "weeks/week-10.html#assignments",
    "title": "Week 10",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important\n\n\n\nHomework 3 due by 11:59pm on Sunday, April 12.",
    "crumbs": [
      "Weekly Materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#assignments",
    "href": "weeks/week-13.html#assignments",
    "title": "Week 13",
    "section": "Assignments",
    "text": "Assignments\n🧮 Homework 3",
    "crumbs": [
      "Weekly Materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\nLab due by 11:59pm on Sunday, March 15.",
    "crumbs": [
      "Weekly Materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#readings",
    "href": "weeks/week-9.html#readings",
    "title": "Week 9",
    "section": "Readings",
    "text": "Readings\n📖 IMS, chap. 7.1\n📖 IMS, chap. 7.2",
    "crumbs": [
      "Weekly Materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#videos",
    "href": "weeks/week-9.html#videos",
    "title": "Week 9",
    "section": "Videos",
    "text": "Videos\n📺 Linear Regression with One Variable, Part 1\n📺 Linear Regression with One Variable, Part 2\n📺 Cost Function\n📺 Cost Function Intuition",
    "crumbs": [
      "Weekly Materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#modules",
    "href": "weeks/week-9.html#modules",
    "title": "Week 9",
    "section": "Modules",
    "text": "Modules\n📒 Module 9.1\n📒 Module 9.2",
    "crumbs": [
      "Weekly Materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#assignments",
    "href": "weeks/week-9.html#assignments",
    "title": "Week 9",
    "section": "Assignments",
    "text": "Assignments\n📘 Lab, available on Posit Cloud.",
    "crumbs": [
      "Weekly Materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important\n\n\n\nProject assignment due by 11:59pm on Sunday, April 19.",
    "crumbs": [
      "Weekly Materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week-14.html#slides",
    "href": "weeks/week-14.html#slides",
    "title": "Week 14",
    "section": "Slides",
    "text": "Slides\n🖥️ Workshop Slides 1\n🖥️ Workshop Slides 2",
    "crumbs": [
      "Weekly Materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week-14.html#assignments",
    "href": "weeks/week-14.html#assignments",
    "title": "Week 14",
    "section": "Assignments",
    "text": "Assignments\n✍️ Project Assignment 3",
    "crumbs": [
      "Weekly Materials",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week-zz.html",
    "href": "weeks/week-zz.html",
    "title": "Week 2",
    "section": "",
    "text": "📖 r4ds\n\nChapter 2 on data visualization\n12.2 on labels\n10.1 - 10.4 on layers, geoms and facets\n12.4 on scales\n12.3 & 12.5 on themes and annotations\n\n\n\n\n📖 Lipset, Some Social Requisites of Democracry"
  },
  {
    "objectID": "weeks/week-zz.html#readings",
    "href": "weeks/week-zz.html#readings",
    "title": "Week 2",
    "section": "",
    "text": "📖 r4ds\n\nChapter 2 on data visualization\n12.2 on labels\n10.1 - 10.4 on layers, geoms and facets\n12.4 on scales\n12.3 & 12.5 on themes and annotations\n\n\n\n\n📖 Lipset, Some Social Requisites of Democracry"
  },
  {
    "objectID": "weeks/week-zz.html#modules",
    "href": "weeks/week-zz.html#modules",
    "title": "Week 2",
    "section": "Modules",
    "text": "Modules\nModule 2.1–Working with Flat Files\nModule 2.2–Working with APIs"
  },
  {
    "objectID": "weeks/week-zz.html#assignments",
    "href": "weeks/week-zz.html#assignments",
    "title": "Week 2",
    "section": "Assignments",
    "text": "Assignments\n📘 Quiz 1\n📘 Quiz 2\n🧮 Coding Assignment 2"
  },
  {
    "objectID": "instructor.html",
    "href": "instructor.html",
    "title": "Instructors",
    "section": "",
    "text": "Emmanuel Teitelbaum is an associate professor of political science and international affairs at the The George Washington University His current research focuses on labor standards and understanding how labor unions, nonprofit organizations, consumers and corporations can help to promote them. He also has a strong interest in development studies and serves as a co-managing editor of the Journal of Development Studies.\nAt GW, Professor Teitelbaum teaches courses on comparative politics, comparative political economy and Data Science. He is on faculty in the Department of Political Science and the Elliott School of International Affairs where he is affiliated with the Sigur Center for Asian Studies as well as the Institute for International Economic Policy. Professor Teitelbaum is the founding co-director of the Elliott School’s Data Literacy Initiative and the Data Analytics for Policy Professionals (DAPP) program.",
    "crumbs": [
      "Course Information",
      "Instructor"
    ]
  },
  {
    "objectID": "instructor.html#office-hours",
    "href": "instructor.html#office-hours",
    "title": "Instructors",
    "section": "Office hours",
    "text": "Office hours\nProfessor Teitelbaum’s office hours this semester are on Thursdays from 1:00 - 3:00 p.m. Please reference his Calendly page to sign up for a slot (or two, if needed). He is available for consultation virtually on Zoom or in his office at 411 Monroe Hall (2115 G. St. NW).\nIf you have a special request for a meeting outside of normal office hours, please send Professor Teitelbaum and email at ejt@gwu.",
    "crumbs": [
      "Course Information",
      "Instructor"
    ]
  },
  {
    "objectID": "slides/week-3.2.html#group-summarize-and-arrange",
    "href": "slides/week-3.2.html#group-summarize-and-arrange",
    "title": "Summarizing Data",
    "section": "Group, Summarize and Arrange",
    "text": "Group, Summarize and Arrange\n\n\ngroup_by(), summarize(), arrange()\nA very common sequence of dplyr verbs:\n\nTake an average or some other statistic for a group\nRank from high to low values of summary value"
  },
  {
    "objectID": "slides/week-3.2.html#setup",
    "href": "slides/week-3.2.html#setup",
    "title": "Summarizing Data",
    "section": "Setup",
    "text": "Setup\n\n# Load packages\nlibrary(vdemdata) # to download V-Dem data\nlibrary(dplyr)\n\n# Download the data\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year == 2015)  |&gt; # filter year, keep 2015\n  select(                  # select (and rename) these variables\n    country = country_name,     # the name before the = sign is the new name  \n    vdem_ctry_id = country_id,  # the name after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy,\n    libdem = v2x_libdem,\n    corruption = v2x_corr,\n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )\n\n# View the data\nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-3.2.html#summarize-by-region",
    "href": "slides/week-3.2.html#summarize-by-region",
    "title": "Summarizing Data",
    "section": "Summarize by Region",
    "text": "Summarize by Region\n\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- democracy |&gt; # save result as new object\n  group_by(region)  |&gt; # group data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    libdem = median(libdem, na.rm = TRUE),\n    corruption = sd(corruption, na.rm = TRUE),\n    gdp_pc = max(gdp_pc, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Print the data\ndem_summary"
  },
  {
    "objectID": "slides/week-3.2.html#summarize-by-region-1",
    "href": "slides/week-3.2.html#summarize-by-region-1",
    "title": "Summarizing Data",
    "section": "Summarize by Region",
    "text": "Summarize by Region\n\n\n\n# A tibble: 6 × 5\n  region         polyarchy libdem corruption gdp_pc\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 The West           0.877  0.824     0.0617   81.7\n2 Latin America      0.648  0.468     0.281    30.8\n3 Eastern Europe     0.550  0.444     0.301    31.7\n4 Asia               0.444  0.317     0.265    64.8\n5 Africa             0.431  0.251     0.231    30.6\n6 Middle East        0.271  0.171     0.251    91.2"
  },
  {
    "objectID": "slides/week-3.2.html#section",
    "href": "slides/week-3.2.html#section",
    "title": "Summarizing Data",
    "section": "",
    "text": "Use group_by() to group countries by region…\n\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- democracy |&gt; # save result as new object\n  group_by(region)  |&gt; # group data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    libdem = median(libdem, na.rm = TRUE),\n    corruption = sd(corruption, na.rm = TRUE),\n    gdp_pc = max(gdp_pc, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Print the data\ndem_summary"
  },
  {
    "objectID": "slides/week-3.2.html#section-1",
    "href": "slides/week-3.2.html#section-1",
    "title": "Summarizing Data",
    "section": "",
    "text": "Use summarize() to get the regional means polyarchy and gpd_pc….\n\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- democracy |&gt; # save result as new object\n  group_by(region)  |&gt; # group data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    libdem = median(libdem, na.rm = TRUE),\n    corruption = sd(corruption, na.rm = TRUE),\n    gdp_pc = max(gdp_pc, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Print the data\ndem_summary"
  },
  {
    "objectID": "slides/week-3.2.html#section-2",
    "href": "slides/week-3.2.html#section-2",
    "title": "Summarizing Data",
    "section": "",
    "text": "Then use arrange() with desc() to sort in descending order by polyarchy score…\n\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- democracy |&gt; # save result as new object\n  group_by(region)  |&gt; # group data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    libdem = median(libdem, na.rm = TRUE),\n    corruption = sd(corruption, na.rm = TRUE),\n    gdp_pc = max(gdp_pc, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Print the data\ndem_summary"
  },
  {
    "objectID": "slides/week-3.2.html#section-3",
    "href": "slides/week-3.2.html#section-3",
    "title": "Summarizing Data",
    "section": "",
    "text": "We are printing the data frame instead of using glimpse() here…\n\n\n# group_by(), summarize() and arrange()\ndem_summary &lt;- democracy |&gt; # save result as new object\n  group_by(region)  |&gt; # group data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy = mean(polyarchy, na.rm = TRUE), # calculate mean, remove NAs\n    libdem = median(libdem, na.rm = TRUE),\n    corruption = sd(corruption, na.rm = TRUE),\n    gdp_pc = max(gdp_pc, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(polyarchy)) # arrange in descending order by polyarchy score\n\n# Print the data\ndem_summary"
  },
  {
    "objectID": "slides/week-3.2.html#some-common-arithmetic-functions",
    "href": "slides/week-3.2.html#some-common-arithmetic-functions",
    "title": "Summarizing Data",
    "section": "Some Common Arithmetic Functions",
    "text": "Some Common Arithmetic Functions\n\n\nsqrt() square root\nlog() natural logarithm\nmean() mean\nmedian() median\nsd() standard deviation"
  },
  {
    "objectID": "slides/week-3.2.html#try-it-yourself",
    "href": "slides/week-3.2.html#try-it-yourself",
    "title": "Summarizing Data",
    "section": "Try it Yourself",
    "text": "Try it Yourself\n\nTry running a group_by(), summarize() and arrange() in your Quarto document\nTry changing the parameters to answer these questions:\n\n\nTry summarizing the data with a different function for one or more of the variables.\n\n\nWhat is the median value of polyarchy for The West?\nWhat is the max value of libdem for Eastern Europe?\nWhat is the standard deviation of corruption for Africa?\nWhat is the mean of gdp_pc for the Middle East?\n\n\nNow try grouping by country instead of region.\n\n\nWhat is the median value of polyarchy for Sweden?\nWhat is the max value of libdem New Zealand?\nWhat is the standard deviation of corruption for Spain?\nWhat is the interquartile range of gdp_pc for Germany?\n\n\nSort countries in descending order based on the mean value of gdp_pc (instead of the median value of polyarchy). Which country ranks first based on this sorting?\nNow try sorting countries in ascending order based on the median value of libdem (hint: delete “desc” from the arrange() call). Which country ranks at the “top” of the list?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-3.2.html#visualize-it",
    "href": "slides/week-3.2.html#visualize-it",
    "title": "Summarizing Data",
    "section": "Visualize It!",
    "text": "Visualize It!\n\n\nlibrary(ggplot2)\n\nggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 2015\", \n    caption = \"Source: V-Dem Institute\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-3.2.html#visualize-it-1",
    "href": "slides/week-3.2.html#visualize-it-1",
    "title": "Summarizing Data",
    "section": "Visualize It!",
    "text": "Visualize It!"
  },
  {
    "objectID": "slides/week-3.2.html#try-it-yourself-1",
    "href": "slides/week-3.2.html#try-it-yourself-1",
    "title": "Summarizing Data",
    "section": "Try it Yourself",
    "text": "Try it Yourself\n\nRun the code and a bar chart with the dem_summary data you wrangled, again grouping by region (instead of country)\nTry visualizing different variables, e.g. libdem, corruption, gdp_pc\nTry different summary statistics, e.g. mean, median, standard deviation, etc.\nTry grouping by country instead of region and visualizing that\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-11.1.html#link-to-code",
    "href": "slides/week-11.1.html#link-to-code",
    "title": "Logistic Regression",
    "section": "Link to Code",
    "text": "Link to Code\n\nPlease find the code here"
  },
  {
    "objectID": "slides/week-11.1.html#binary-outcomes-1",
    "href": "slides/week-11.1.html#binary-outcomes-1",
    "title": "Logistic Regression",
    "section": "Binary Outcomes",
    "text": "Binary Outcomes\n\n\nSo far we have looked at continuous or numerical outcomes (response variables)\nWe are often also interested in outcome variables that are binary (Yes/No, or 1/0)\n\nDid violence happen, or not?\nClassification: is this email spam?"
  },
  {
    "objectID": "slides/week-11.1.html#example-conflict-onset",
    "href": "slides/week-11.1.html#example-conflict-onset",
    "title": "Logistic Regression",
    "section": "Example: Conflict Onset",
    "text": "Example: Conflict Onset\n\n\nDid a civil war begin in a given country in a given year? (yes/no)\nPredictors: wealth, democracy, terrain, ethnic diversity, etc.\nSeminal work by Fearon and Laitin (2003)\nWe can use logistic regression to model this binary outcome"
  },
  {
    "objectID": "slides/week-11.1.html#modeling",
    "href": "slides/week-11.1.html#modeling",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\n\nWe can treat each outcome (conflict onset) as successes and failures arising from separate Bernoulli trials\nBernoulli trial: a random experiment with exactly two possible outcomes, “success” and “failure”, in which the probability of success is the same every time the experiment is conducted\nSuccess is usually coded as 1, failure as 0\nSo ironically, conflict onset is a “success” in this context"
  },
  {
    "objectID": "slides/week-11.1.html#modeling-1",
    "href": "slides/week-11.1.html#modeling-1",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\nEach Bernoulli trial can have a separate probability of success\n\n\\[ y_i ∼ Bern(p) \\]"
  },
  {
    "objectID": "slides/week-11.1.html#modeling-2",
    "href": "slides/week-11.1.html#modeling-2",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\n\nWe can then use the predictor variables to model that probability of success, \\(p_i\\)\nWe can’t really use a linear model for \\(p_i\\) (since \\(p_i\\) must be between 0 and 1) but we can transform the linear model to have the appropriate range"
  },
  {
    "objectID": "slides/week-11.1.html#generalized-linear-models",
    "href": "slides/week-11.1.html#generalized-linear-models",
    "title": "Logistic Regression",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\nThis is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs)\nLogistic regression is a very common example"
  },
  {
    "objectID": "slides/week-11.1.html#glms",
    "href": "slides/week-11.1.html#glms",
    "title": "Logistic Regression",
    "section": "GLMs",
    "text": "GLMs\n\nAll GLMs have the following three characteristics:\n\nA probability distribution describing a generative model for the outcome variable\nA linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\]\nA link function that relates the linear model to the parameter of the outcome distribution"
  },
  {
    "objectID": "slides/week-11.1.html#logistic-regression",
    "href": "slides/week-11.1.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nLogistic regression is a GLM used to model a binary categorical outcome (0 or 1)\nIn logistic regression, the link function that connects \\(\\eta_i\\) to \\(p_i\\) is the logit function\nLogit function: For \\(0\\le p \\le 1\\)\n\n\\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]"
  },
  {
    "objectID": "slides/week-11.1.html#logit-function",
    "href": "slides/week-11.1.html#logit-function",
    "title": "Logistic Regression",
    "section": "Logit Function",
    "text": "Logit Function"
  },
  {
    "objectID": "slides/week-11.1.html#logistic-regression-model",
    "href": "slides/week-11.1.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\n\n\\(y_i \\sim \\text{Bern}(p_i)\\)\n\\(\\eta_i = \\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)"
  },
  {
    "objectID": "slides/week-11.1.html#logistic-regression-model-1",
    "href": "slides/week-11.1.html#logistic-regression-model-1",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\n\n\\(\\text{logit}(p_i) = \\eta_i = \\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\nNow take inverse logit to get \\(p\\)\n\n\\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\]"
  },
  {
    "objectID": "slides/week-11.1.html#the-peacesciencer-package",
    "href": "slides/week-11.1.html#the-peacesciencer-package",
    "title": "Logistic Regression",
    "section": "The peacesciencer Package",
    "text": "The peacesciencer Package\n\nThe peacesciencer package provides a number of datasets and functions for analyzing conflict and peace\nProvides data from a number of important datasets in the field of conflict studies, e.g.\n\nCorrelates of War (CoW) project\nUppsala Conflict Data Program (UCDP)\nMilitarized Interstate Dispute (MID) dataset\n\nProvides functions for analyzing conflict and adding control variables to the dataset"
  },
  {
    "objectID": "slides/week-11.1.html#using-the-peacesciencer-package",
    "href": "slides/week-11.1.html#using-the-peacesciencer-package",
    "title": "Logistic Regression",
    "section": "Using the peacesciencer Package",
    "text": "Using the peacesciencer Package\n\n\nlibrary(peacesciencer)\nlibrary(tidymodels)\n\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()\n\nglimpse(conflict_df)\n\nRows: 7,036\nColumns: 20\n$ gwcode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ statename      &lt;chr&gt; \"United States of America\", \"United States of America\",…\n$ year           &lt;dbl&gt; 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1…\n$ ucdpongoing    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ucdponset      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ maxintensity   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ conflict_ids   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ v2x_polyarchy  &lt;dbl&gt; 0.605, 0.587, 0.599, 0.599, 0.587, 0.602, 0.601, 0.594,…\n$ polity2        &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ xm_qudsest     &lt;dbl&gt; 1.259180, 1.259180, 1.252190, 1.252190, 1.270106, 1.259…\n$ ethfrac        &lt;dbl&gt; 0.2226323, 0.2248701, 0.2271561, 0.2294918, 0.2318781, …\n$ ethpol         &lt;dbl&gt; 0.4152487, 0.4186156, 0.4220368, 0.4255134, 0.4290458, …\n$ relfrac        &lt;dbl&gt; 0.4980802, 0.5009111, 0.5037278, 0.5065309, 0.5093204, …\n$ relpol         &lt;dbl&gt; 0.7769888, 0.7770017, 0.7770303, 0.7770729, 0.7771274, …\n$ wbgdp2011est   &lt;dbl&gt; 28.539, 28.519, 28.545, 28.534, 28.572, 28.635, 28.669,…\n$ wbpopest       &lt;dbl&gt; 18.744, 18.756, 18.781, 18.804, 18.821, 18.832, 18.848,…\n$ sdpest         &lt;dbl&gt; 28.478, 28.456, 28.483, 28.469, 28.510, 28.576, 28.611,…\n$ wbgdppc2011est &lt;dbl&gt; 9.794, 9.762, 9.764, 9.730, 9.752, 9.803, 9.821, 9.857,…\n$ rugged         &lt;dbl&gt; 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073,…\n$ newlmtnest     &lt;dbl&gt; 3.214868, 3.214868, 3.214868, 3.214868, 3.214868, 3.214…"
  },
  {
    "objectID": "slides/week-11.1.html#running-a-logistic-regression",
    "href": "slides/week-11.1.html#running-a-logistic-regression",
    "title": "Logistic Regression",
    "section": "Running a Logistic Regression",
    "text": "Running a Logistic Regression\n\n\nImplementation is not very different from a linear model\nWe just need to update our code to run a GLM\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\n\ndefine family = \"binomial\" for the link function to be used in the model"
  },
  {
    "objectID": "slides/week-11.1.html#bivariate-logistic-regression",
    "href": "slides/week-11.1.html#bivariate-logistic-regression",
    "title": "Logistic Regression",
    "section": "Bivariate Logistic Regression",
    "text": "Bivariate Logistic Regression\n\n\nconflict_model &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(factor(ucdponset) ~ wbgdppc2011est,\n                  data= conflict_df,\n                  family = \"binomial\")\n\ntidy(conflict_model)\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      -1.16     0.426      -2.73 6.37e- 3\n2 wbgdppc2011est   -0.331    0.0526     -6.29 3.20e-10"
  },
  {
    "objectID": "slides/week-11.1.html#interpreting-the-results",
    "href": "slides/week-11.1.html#interpreting-the-results",
    "title": "Logistic Regression",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\n\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times \\text{logGDPpc}\\]"
  },
  {
    "objectID": "slides/week-11.1.html#interpreting-the-results-1",
    "href": "slides/week-11.1.html#interpreting-the-results-1",
    "title": "Logistic Regression",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\n\n\nFor a quick interpretation of the coefficients, we can exponentiate them\nThe exponentiated coefficient is the odds ratio\nFor each one-unit increase in the independent variable, the odds of the outcome occurring increase (or decrease) by a factor of the exponentiated coefficient"
  },
  {
    "objectID": "slides/week-11.1.html#interpreting-the-results-2",
    "href": "slides/week-11.1.html#interpreting-the-results-2",
    "title": "Logistic Regression",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\n\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times \\text{logGDPpc}\\]\n\nFor each one unit increase in log GDP per capita, the odds of the outcome occurring are multiplied by approximately 0.718, assuming other variables in the model are held constant.\n\nThis means that an increase in GDP per capita is associated with a decrease in the odds of the outcome occurring. The odds of the outcome decrease by about 28.2% for each unit increase in GDP per capita (on average)."
  },
  {
    "objectID": "slides/week-11.1.html#your-turn",
    "href": "slides/week-11.1.html#your-turn",
    "title": "Logistic Regression",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nRun a bivariate logistic regression using ucdp onset as the outcome variable\nFirst replicate the results using GDP per capita as the predictor\nNow try a different predictor\nInterpret the results\n\nWhat is the average effect of the predictor on conflict onset?\nHow do you interpret that effect in terms of the odds of conflict onset?\n\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-11.1.html#calculating-predicted-probabilities",
    "href": "slides/week-11.1.html#calculating-predicted-probabilities",
    "title": "Logistic Regression",
    "section": "Calculating Predicted Probabilities",
    "text": "Calculating Predicted Probabilities\n\nProbability of conflict onset for a country with a log per capita GDP of 9 (about $8,000):\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times 9\\] \\[\\log\\left(\\frac{p}{1-p}\\right) = -4.13\\]\n\\[\\frac{p}{1-p} = \\exp(-4.13)\\]\n\\[\\frac{p}{1-p} = 0.016\\]\n\\[p = 0.016 \\times (1 - p)\\] \\[p = 0.016 - 0.016p\\]\n\\[1.016p = 0.016\\] \\[p = 0.016 / 1.016\\] \\[p = 0.0158\\]"
  },
  {
    "objectID": "slides/week-11.1.html#using-marginaleffects",
    "href": "slides/week-11.1.html#using-marginaleffects",
    "title": "Logistic Regression",
    "section": "Using marginaleffects",
    "text": "Using marginaleffects\n\n# load the marginaleffects library\nlibrary(marginaleffects)\n\n# select some countries for a given year\nselected_countries &lt;- conflict_df |&gt;\n  filter(\n    statename %in% c(\"United States of America\", \"Venezuela\", \"Rwanda\"),\n    year == 1999)\n\n# extract the model\nconflict_fit &lt;- conflict_model$fit\n\n# calculate margins for the subset\nmarg_effects &lt;- predictions(conflict_fit, newdata = selected_countries)\n\n# tidy the results\ntidy(marg_effects) |&gt;\n  select(estimate, p.value, conf.low, conf.high, statename)"
  },
  {
    "objectID": "slides/week-11.1.html#using-marginaleffects-1",
    "href": "slides/week-11.1.html#using-marginaleffects-1",
    "title": "Logistic Regression",
    "section": "Using marginaleffects",
    "text": "Using marginaleffects\n\n\n\n# A tibble: 3 × 5\n  estimate   p.value conf.low conf.high statename               \n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                   \n1  0.00853 4.20e-161  0.00606    0.0120 United States of America\n2  0.0141  0          0.0113     0.0175 Venezuela               \n3  0.0311  1.36e-250  0.0256     0.0377 Rwanda"
  },
  {
    "objectID": "slides/week-11.1.html#your-turn-1",
    "href": "slides/week-11.1.html#your-turn-1",
    "title": "Logistic Regression",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nSelect your favorite three countries and a recent year\nCalculate the predicted proability of conflict onset for that year using the marginal effects package\nIf you have time, try to do the calcualation by hand as well\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-1.2.html#quarto-1",
    "href": "slides/week-1.2.html#quarto-1",
    "title": "Getting Started",
    "section": "Quarto",
    "text": "Quarto\n\n\nQuarto is an open-source scientific publishing platform\nAllows you to integrate text with code\nKind of like a word processor for data science\nCan use it to create reports, books, websites, etc.\nCan make HTML, PDF, Word, and other formats\nCan use R, Python, Julia, and other languages"
  },
  {
    "objectID": "slides/week-1.2.html#project-oriented-workflow",
    "href": "slides/week-1.2.html#project-oriented-workflow",
    "title": "Getting Started",
    "section": "Project Oriented Workflow",
    "text": "Project Oriented Workflow\n\n\nAlways start a document in a project folder\n\nThat way you don’t have to do setwd\nAlso can share easily with other people\n\nGo to File&gt;New Project\nCreate a Quarto project folder"
  },
  {
    "objectID": "slides/week-1.2.html#visual-editor",
    "href": "slides/week-1.2.html#visual-editor",
    "title": "Getting Started",
    "section": "Visual Editor",
    "text": "Visual Editor\n\nThere are two ways to edit Quarto docs\n\nSource (markdown)\nVisual editor\n\nVisual editor\n\nWYSIWYM\nApproximates appearance\n\nTry both and see what you like"
  },
  {
    "objectID": "slides/week-1.2.html#rendering-documents",
    "href": "slides/week-1.2.html#rendering-documents",
    "title": "Getting Started",
    "section": "Rendering Documents",
    "text": "Rendering Documents\n\nRendering = converting to another format\n\nDefault format is HTML\nCan also render to PDF, Word, etc.\n\nTo render a Quarto document\n\nClick on the Render button\n\nOr keyboard shortcut (Cmd/Ctrl + Shift + K)\n\n\nBy default, Quarto will preview the document in your browser\nBut you can also preview in Viewer pane\n\nClick on the gear icon next to the Render button\nSelect “Preview in Viewer Pane”"
  },
  {
    "objectID": "slides/week-1.2.html#illustration",
    "href": "slides/week-1.2.html#illustration",
    "title": "Getting Started",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "slides/week-1.2.html#lets-try-quarto",
    "href": "slides/week-1.2.html#lets-try-quarto",
    "title": "Getting Started",
    "section": "Let’s Try Quarto!",
    "text": "Let’s Try Quarto!\n\nCreate a new Quarto document\n\nFile&gt;New File&gt;Quarto Document\n\nSave the document in your project folder\nRender it\n\nClick on the Render button\nOr keyboard shortcut (Cmd/Ctr + Shift + K)\n\nTry out the visual editor"
  },
  {
    "objectID": "slides/week-1.2.html#document-elements",
    "href": "slides/week-1.2.html#document-elements",
    "title": "Getting Started",
    "section": "Document Elements",
    "text": "Document Elements\n\n\nYAML Header\nMarkdown content\nCode chunks"
  },
  {
    "objectID": "slides/week-1.2.html#yaml-header",
    "href": "slides/week-1.2.html#yaml-header",
    "title": "Getting Started",
    "section": "YAML Header",
    "text": "YAML Header\n\n\nMetadata about the document\n\nTitle, author, date, etc.\n\nOutput format\nExecution options"
  },
  {
    "objectID": "slides/week-1.2.html#yaml-header-1",
    "href": "slides/week-1.2.html#yaml-header-1",
    "title": "Getting Started",
    "section": "YAML Header",
    "text": "YAML Header\n---\ntitle: \"My Documnet\"\nauthor: \"Your Name\"\ndate: today\ndate-format: long\nformat: html\nexecute:\n  echo: false\n  message: false\n---\n\nTry changing some of these options in your document\nThen render it again\nLook in the Quarto guide for other options to try"
  },
  {
    "objectID": "slides/week-1.2.html#markdown",
    "href": "slides/week-1.2.html#markdown",
    "title": "Getting Started",
    "section": "Markdown",
    "text": "Markdown\n\n\nMarkdown is a simple markup language\nYou can use it to format text\nYou can also use it to embed images, tables, etc.\nAnd to embed code chunks…"
  },
  {
    "objectID": "slides/week-1.2.html#markdown-syntax---basic-authoring",
    "href": "slides/week-1.2.html#markdown-syntax---basic-authoring",
    "title": "Getting Started",
    "section": "Markdown Syntax - Basic Authoring",
    "text": "Markdown Syntax - Basic Authoring\n\nFor basic text you can just start typing\nFor line breaks use two spaces and return (enter)\nHeadings (use #, ##, ###, etc.)\n\n# is the largest heading (level 1)\n## is the next largest (level 2)\n### is the next largest (level 3)\nEtc."
  },
  {
    "objectID": "slides/week-1.2.html#markdown-syntax---styling",
    "href": "slides/week-1.2.html#markdown-syntax---styling",
    "title": "Getting Started",
    "section": "Markdown Syntax - Styling",
    "text": "Markdown Syntax - Styling\n\nEmphasis = Italics (use *)\n\nBold (use **)\n\nLists\n\nBullet points (use -)\nNumbered lists (use 1.)"
  },
  {
    "objectID": "slides/week-1.2.html#markdown-syntax---content",
    "href": "slides/week-1.2.html#markdown-syntax---content",
    "title": "Getting Started",
    "section": "Markdown Syntax - Content",
    "text": "Markdown Syntax - Content\n\nLinks (use [text](url))\nImages (use ![](file path or url))\nCode chunks\n\nR code chunks (```{r}…```)\nPython code chunks (```{python}…```)\nEtc."
  },
  {
    "objectID": "slides/week-1.2.html#try-some-markdown",
    "href": "slides/week-1.2.html#try-some-markdown",
    "title": "Getting Started",
    "section": "Try Some Markdown",
    "text": "Try Some Markdown\n\n\nCheck out the Markdown Cheatsheet\nTry editing the markdown in your document\nTry some of the other things you find in the guide\nThen render it again"
  },
  {
    "objectID": "slides/week-1.2.html#code-chunks",
    "href": "slides/week-1.2.html#code-chunks",
    "title": "Getting Started",
    "section": "Code Chunks",
    "text": "Code Chunks\n\nIncorporate R code (could also be Python, Julia, etc.)\nAdd a code chunk with the ‘+’ button\nRun the code chunk by clicking the play button\n\nOr use keyboard shortcut (Cmd/Ctrl + Shift + Enter)\n\nRun all chunks up that point by clicking the down arrow\n\nOr use keyboard shortcut (Cmd/Ctrl + Shift + K)\n\nRun a single line with shortcut (Cmd/Ctrl + Enter)"
  },
  {
    "objectID": "slides/week-1.2.html#code-chunk-options",
    "href": "slides/week-1.2.html#code-chunk-options",
    "title": "Getting Started",
    "section": "Code Chunk Options",
    "text": "Code Chunk Options\n\nUse #| (hash-pipe) to add options\nlabel is a unique identifier for the chunk\nOptions to control what happens when you render\n\necho controls whether the code is shown\neval controls whether the code is run\nmessage controls whether messages are shown\nwarning controls whether warnings are shown"
  },
  {
    "objectID": "slides/week-1.2.html#code-chunk-options-1",
    "href": "slides/week-1.2.html#code-chunk-options-1",
    "title": "Getting Started",
    "section": "Code Chunk Options",
    "text": "Code Chunk Options\n\n\nCode-chunk options override global options set in YAML header\nSee documentation for more options\nYou can also use write chunk options inline with chunk name,\n\ne.g., {r, echo = FALSE} ..."
  },
  {
    "objectID": "slides/week-1.2.html#illustration-1",
    "href": "slides/week-1.2.html#illustration-1",
    "title": "Getting Started",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "slides/week-1.2.html#try-it-yourself",
    "href": "slides/week-1.2.html#try-it-yourself",
    "title": "Getting Started",
    "section": "Try it Yourself!",
    "text": "Try it Yourself!\n\nCreate a code chunk\nCopy this code chunk into your document\n\n\nlibrary(ggplot2)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\nTry adding some chunk options in your document\nThen render it again"
  },
  {
    "objectID": "slides/week-1.2.html#r-packages-and-functions",
    "href": "slides/week-1.2.html#r-packages-and-functions",
    "title": "Getting Started",
    "section": "R Packages and Functions",
    "text": "R Packages and Functions\n\n\nA function is a set of instructions\n\nread_csv() is a function\nggplot() is a function\n\nA package is a collection of functions\n\nreadr is a package that contains the read_csv() function\nggplot2 is a package that contains the ggplot() function\n\nUse install.packages() to install packages\nUse library() to load packages\nYou can install packages from CRAN"
  },
  {
    "objectID": "slides/week-1.2.html#the-tidyverse",
    "href": "slides/week-1.2.html#the-tidyverse",
    "title": "Getting Started",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\nThe Tidyverse is a collection of data science packages\nIt is also considered a dialect of R\nIn this class, we will be using many Tidyverse packages\n\nggplot2 for data visualization\nreadr for reading data\ndplyr for data manipulation\ntidyr for data tidying\nEtc.\n\nAt first we will load the packages independently, e.g. library(ggplot2)\nLater we will load them all at once with library(tidyverse)"
  },
  {
    "objectID": "slides/week-7.1.html#jobs-training-programs",
    "href": "slides/week-7.1.html#jobs-training-programs",
    "title": "Hypothesis Testing 1",
    "section": "Jobs Training Programs",
    "text": "Jobs Training Programs\n\nInternational development organizations are sometimes interested in providing training to people in order to help them find a job\nImagine the unemployment rate in a low-income country is 30%\nOne organization claims that its jobs training program is a success because only 15 of the 60 people that they trained did not have a job (25% unemployment rate)\nWhat should we think about this claim? Is this a successful program?"
  },
  {
    "objectID": "slides/week-7.1.html#todays-code",
    "href": "slides/week-7.1.html#todays-code",
    "title": "Hypothesis Testing 1",
    "section": "Today’s Code",
    "text": "Today’s Code\nhttps://www.dropbox.com/scl/fo/g4tdpdwcij78nyydvr4bp/h?rlkey=hctskhle6222csnwre5ir9gz3&dl=0"
  },
  {
    "objectID": "slides/week-7.1.html#section",
    "href": "slides/week-7.1.html#section",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Now let’s create some data to match our hypothetical example.\n\n\nlibrary(tidyverse)\n\njobs_program &lt;- tibble(\n  outcome = c(rep(\"unemployed\", 15), rep(\"employed\", 45))\n)"
  },
  {
    "objectID": "slides/week-7.1.html#section-1",
    "href": "slides/week-7.1.html#section-1",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Use the base R head() function to see the first five rows.\n\nhead(jobs_program)\n\n# A tibble: 6 × 1\n  outcome   \n  &lt;chr&gt;     \n1 unemployed\n2 unemployed\n3 unemployed\n4 unemployed\n5 unemployed\n6 unemployed"
  },
  {
    "objectID": "slides/week-7.1.html#section-2",
    "href": "slides/week-7.1.html#section-2",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Use the tail() function to see the last five rows.\n\ntail(jobs_program)\n\n# A tibble: 6 × 1\n  outcome \n  &lt;chr&gt;   \n1 employed\n2 employed\n3 employed\n4 employed\n5 employed\n6 employed"
  },
  {
    "objectID": "slides/week-7.1.html#section-3",
    "href": "slides/week-7.1.html#section-3",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Now let’s visualize it with a bar chart."
  },
  {
    "objectID": "slides/week-7.1.html#question",
    "href": "slides/week-7.1.html#question",
    "title": "Hypothesis Testing 1",
    "section": "Question",
    "text": "Question\n\nIs it possible to assess this hypothetical organization’s claim using the data and information presented thus far?\n\n“Our jobs program is a success because only 15 of the 60 people that we trained did not have a job. Thus our 25% unemployment rate beats the country’s unemployment rate of 30%.”"
  },
  {
    "objectID": "slides/week-7.1.html#correlation-vs.-causation",
    "href": "slides/week-7.1.html#correlation-vs.-causation",
    "title": "Hypothesis Testing 1",
    "section": "Correlation vs. causation",
    "text": "Correlation vs. causation\n\n\nNo.\nWe need to know more about how people were selected for the program in order to assess causality (e.g., were they randomly assigned?)\nBut, we can still ask whether the unemployment rate of \\(\\hat{p}\\) = 0.25 could be due to chance."
  },
  {
    "objectID": "slides/week-7.1.html#hypothesis-testing-intuition",
    "href": "slides/week-7.1.html#hypothesis-testing-intuition",
    "title": "Hypothesis Testing 1",
    "section": "Hypothesis Testing Intuition",
    "text": "Hypothesis Testing Intuition\n\n\nWe are going to assume “nothing is going on”\n\nIn this case, the jobs program had no impact\n\n\n\n\nWe are going to figure out what the distribution of outcomes we we might observe could be if nothing is going on\n\nIn this case: if we take a sample of 60 from a population where the parameter is 0.3\n\n\n\n\n\nWe will assess how likely we would be to observe our data if nothing is going on\n\nIf very unlikely, we conclude that something is probably going on"
  },
  {
    "objectID": "slides/week-7.1.html#stating-our-hypotheses",
    "href": "slides/week-7.1.html#stating-our-hypotheses",
    "title": "Hypothesis Testing 1",
    "section": "Stating our Hypotheses",
    "text": "Stating our Hypotheses\n\nNull hypothesis (\\(H_0\\)): “There is nothing going on.”\n\nUnemployment rate among those in the jobs program is no different than the country average of 30%.\n\n\nAlternative hypothesis (\\(H_A\\)): “There is something going on.”\n\nUnemployment rate is lower than the country average of 30%."
  },
  {
    "objectID": "slides/week-7.1.html#hypothesis-test",
    "href": "slides/week-7.1.html#hypothesis-test",
    "title": "Hypothesis Testing 1",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\n\nHypothesis test: If the null hypothesis were true, is the data we have in our sample likely to have been generated by chance (due to random variability)?\nIf yes, we do NOT reject the null hypothesis\nIf not very likely, we reject the null hypothesis"
  },
  {
    "objectID": "slides/week-7.1.html#hypothesis-testing-framework",
    "href": "slides/week-7.1.html#hypothesis-testing-framework",
    "title": "Hypothesis Testing 1",
    "section": "Hypothesis Testing Framework",
    "text": "Hypothesis Testing Framework\n\nStart with null hypothesis, \\(H_0\\), represents the status quo\nSet an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what we’re testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/week-7.1.html#p-values-and-critical-values",
    "href": "slides/week-7.1.html#p-values-and-critical-values",
    "title": "Hypothesis Testing 1",
    "section": "p-values and Critical Values",
    "text": "p-values and Critical Values\n\n\nA p-value is the probability of observed or more extreme outcome given that the null hypothesis is true\nA critical value (\\(\\alpha\\)) is the threshold at which we will reject the null hypothesis\nIf the p-value is less than \\(\\alpha\\), we reject the null hypothesis\nA standard threshold for \\(\\alpha\\) is 0.05"
  },
  {
    "objectID": "slides/week-7.1.html#the-null-distribution",
    "href": "slides/week-7.1.html#the-null-distribution",
    "title": "Hypothesis Testing 1",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\n\nSince \\(H_0: p = 0.30\\), we need to simulate a null distribution where the probability of success (unemployment) for each trial (person in program) is 0.30\n\n\n\nWe want to know how likely we would be to get an unemployment rate of 0.25 in our sample of 60, if the true unemployment rate were 0.30"
  },
  {
    "objectID": "slides/week-7.1.html#what-do-we-expect",
    "href": "slides/week-7.1.html#what-do-we-expect",
    "title": "Hypothesis Testing 1",
    "section": "What do we expect?",
    "text": "What do we expect?\n\n\nSo the first step is to simulate our null distribution\nAnd the question is, when sampling from the null distribution, what is the expected proportion of unemployed?\nWe set up our simulator to select samples of 60 individuals with a 30% chance of being unemployed\nWe then calculate the proportion of unemployed in each sample"
  },
  {
    "objectID": "slides/week-7.1.html#simulation-1",
    "href": "slides/week-7.1.html#simulation-1",
    "title": "Hypothesis Testing 1",
    "section": "Simulation #1",
    "text": "Simulation #1\n\n\nsim1\n  employed unemployed \n        42         18 \n\n\n[1] 0.3"
  },
  {
    "objectID": "slides/week-7.1.html#simulation-2",
    "href": "slides/week-7.1.html#simulation-2",
    "title": "Hypothesis Testing 1",
    "section": "Simulation #2",
    "text": "Simulation #2\n\n\nsim2\n  employed unemployed \n        41         19 \n\n\n[1] 0.3166667"
  },
  {
    "objectID": "slides/week-7.1.html#simulation-3",
    "href": "slides/week-7.1.html#simulation-3",
    "title": "Hypothesis Testing 1",
    "section": "Simulation #3",
    "text": "Simulation #3\n\n\nsim3\n  employed unemployed \n        38         22 \n\n\n[1] 0.3666667"
  },
  {
    "objectID": "slides/week-7.1.html#we-need-to-do-this-many-times",
    "href": "slides/week-7.1.html#we-need-to-do-this-many-times",
    "title": "Hypothesis Testing 1",
    "section": "We need to do this many times…",
    "text": "We need to do this many times…"
  },
  {
    "objectID": "slides/week-7.1.html#tidymodels",
    "href": "slides/week-7.1.html#tidymodels",
    "title": "Hypothesis Testing 1",
    "section": "tidymodels",
    "text": "tidymodels\n\nWe can use the tidymodels package to help with this process…\n\n#load tidymodels\nlibrary(tidymodels)\n\n# simulate the distribution\nnull_dist &lt;- jobs_program |&gt;\n  specify(response = outcome, success = \"unemployed\") |&gt;\n  hypothesize(null = \"point\", p = c(\"unemployed\" = 0.30, \"employed\" = 0.70)) |&gt;\n  generate(reps = 2000, type = \"draw\") |&gt; \n  calculate(stat = \"prop\")"
  },
  {
    "objectID": "slides/week-7.1.html#what-is-being-stored-in-null_dist",
    "href": "slides/week-7.1.html#what-is-being-stored-in-null_dist",
    "title": "Hypothesis Testing 1",
    "section": "What is being stored in null_dist?",
    "text": "What is being stored in null_dist?\n\n\n\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 2,000 × 2\n   replicate  stat\n       &lt;dbl&gt; &lt;dbl&gt;\n 1         1 0.367\n 2         2 0.2  \n 3         3 0.283\n 4         4 0.2  \n 5         5 0.4  \n 6         6 0.317\n 7         7 0.3  \n 8         8 0.333\n 9         9 0.283\n10        10 0.25 \n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/week-7.1.html#the-null-distribution-1",
    "href": "slides/week-7.1.html#the-null-distribution-1",
    "title": "Hypothesis Testing 1",
    "section": "The null distribution",
    "text": "The null distribution\n\nWhere should this distribution be centered? Or, what should the mean be?\n\n\n\nnull_dist |&gt;\n  summarize(mean = mean(stat))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 0.301"
  },
  {
    "objectID": "slides/week-7.1.html#visualizing-the-null-distribution",
    "href": "slides/week-7.1.html#visualizing-the-null-distribution",
    "title": "Hypothesis Testing 1",
    "section": "Visualizing the null distribution",
    "text": "Visualizing the null distribution"
  },
  {
    "objectID": "slides/week-7.1.html#calculate-the-p-value",
    "href": "slides/week-7.1.html#calculate-the-p-value",
    "title": "Hypothesis Testing 1",
    "section": "Calculate the p-value",
    "text": "Calculate the p-value\n p-value–in what % of the simulations was the simulated sample proportion at least as extreme as the observed sample proportion?\n\nnull_dist |&gt;\n  # select out the value in the null distribution that are less that 0.25\n  filter(stat &lt;= (15/60)) |&gt;\n  # calculate the proportion - (number less divided by all values in null_dist)\n  summarise(p_value = n()/nrow(null_dist))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.238"
  },
  {
    "objectID": "slides/week-7.1.html#visualizing-the-p-value",
    "href": "slides/week-7.1.html#visualizing-the-p-value",
    "title": "Hypothesis Testing 1",
    "section": "Visualizing the p-value",
    "text": "Visualizing the p-value"
  },
  {
    "objectID": "slides/week-7.1.html#significance-level",
    "href": "slides/week-7.1.html#significance-level",
    "title": "Hypothesis Testing 1",
    "section": "“Significance” level",
    "text": "“Significance” level\n\n\nConventionally, people use a p-value of 0.05 as a cutoff (“signifigance level”) for determining “statistical significance”\n\nThat is, whether the null hypothesis should be rejected\nThat is, whether the data we gathered is very unlikely to have been generated due to chance\n\nAlways remember that this is a convention\n\np=0.049 is under the cutoff, while p=0.051 is not: are these really different?\n\nWhen people report “statistically significant” results, they mean that the p-value from their analysis is less than 0.05"
  },
  {
    "objectID": "slides/week-7.1.html#our-hypothetical-study",
    "href": "slides/week-7.1.html#our-hypothetical-study",
    "title": "Hypothesis Testing 1",
    "section": "Our Hypothetical Study",
    "text": "Our Hypothetical Study\n\n\nOur finding: if the true unemployment rate were 30 percent and we draw samples of 60, about 23 percent of the time we will get an unemployment rate lower than the one among the participants in the program (simply due to random chance)\nWhat should we conclude?"
  },
  {
    "objectID": "slides/week-7.1.html#conclusion",
    "href": "slides/week-7.1.html#conclusion",
    "title": "Hypothesis Testing 1",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nWe do NOT reject the null hypothesis: the unemployment rate in the sample could likely have been due to chance"
  },
  {
    "objectID": "slides/week-7.1.html#your-turn",
    "href": "slides/week-7.1.html#your-turn",
    "title": "Hypothesis Testing 1",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nWhat if the unemployment rate for the program was only 10%? - Would you reject the null hypothesis in this case?\n\nDemonstrate by calculating the p-value\n\nTry changing the true unemployment rate in the null distribution to 0.50 (50%)\n\nSimulate the null distribution\nWould you reject the null hypothesis if the observed unemployment rate was 23% in this case?"
  },
  {
    "objectID": "slides/week-7.2.html#hypotheses",
    "href": "slides/week-7.2.html#hypotheses",
    "title": "Hypothesis Testing 2",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\nNull hypothesis: there is no relationship between treatment and outcome, the difference is due to chance\nAlternative hypothesis: there is a relationship, the difference is not due to chance"
  },
  {
    "objectID": "slides/week-7.2.html#approach",
    "href": "slides/week-7.2.html#approach",
    "title": "Hypothesis Testing 2",
    "section": "Approach",
    "text": "Approach\n\n\nUnder the null hypothesis, treatment has NO impact on y (the outcome)\nThis means that if we were to change the values of the treatment variable, the values on y would stay the same"
  },
  {
    "objectID": "slides/week-7.2.html#approach-1",
    "href": "slides/week-7.2.html#approach-1",
    "title": "Hypothesis Testing 2",
    "section": "Approach",
    "text": "Approach\n\nSo…we can simulate the null distribution by:\n\nReshuffling the treatment variable\nCalculating the treatment effect\nRepeating many times\n\n\n\n\nThen we can ask: how likely would we be to observe the treatment effect in our data, if there is no effect of the treatment?"
  },
  {
    "objectID": "slides/week-7.2.html#résumé-experiment-example",
    "href": "slides/week-7.2.html#résumé-experiment-example",
    "title": "Hypothesis Testing 2",
    "section": "Résumé Experiment Example",
    "text": "Résumé Experiment Example\nBertrand and Mullainathan studied racial discrimination in responses to job applications in Chicago and Boston. They sent 4,870 résumés, randomly assigning names associated with different racial groups. - Data are in openintro package as an object called resume - I will save as myDat\n\nlibrary(openintro)\nmyDat &lt;- resume"
  },
  {
    "objectID": "slides/week-7.2.html#callbacks-by-race",
    "href": "slides/week-7.2.html#callbacks-by-race",
    "title": "Hypothesis Testing 2",
    "section": "Callbacks by Race",
    "text": "Callbacks by Race\n\nRemember, race of applicant is randomly assigned.\n\nlibrary(tidyverse)\n\nmns &lt;- myDat |&gt;\n  group_by(race) |&gt; \n  summarize(calls = mean(received_callback))\nmns\n\n# A tibble: 2 × 2\n  race   calls\n  &lt;chr&gt;  &lt;dbl&gt;\n1 black 0.0645\n2 white 0.0965"
  },
  {
    "objectID": "slides/week-7.2.html#section",
    "href": "slides/week-7.2.html#section",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Let’s save the means for white and black applicants.\n\n\nmean_white = mns$calls[2]\nmean_black = mns$calls[1]"
  },
  {
    "objectID": "slides/week-7.2.html#section-1",
    "href": "slides/week-7.2.html#section-1",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "And calculate the treatment effect. The treatment effect is the difference in means.\n\n\nteffect &lt;- mean_white - mean_black\nteffect\n\n[1] 0.03203285"
  },
  {
    "objectID": "slides/week-7.2.html#section-2",
    "href": "slides/week-7.2.html#section-2",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Before formal tests, let’s look at the data–the estimates and the confidence intervals…"
  },
  {
    "objectID": "slides/week-7.2.html#section-3",
    "href": "slides/week-7.2.html#section-3",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "First, let’s make the CIs for the white applicants.\n\n\nlibrary(tidymodels)\nboot_df_white &lt;- myDat |&gt;\n  filter(race == \"white\") |&gt; \n  specify(response = received_callback) |&gt;  \n  generate(reps = 15000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\nlower_bound_white &lt;- boot_df_white |&gt; summarize(lower_bound_white = quantile(stat, 0.025)) |&gt; pull() \nupper_bound_white &lt;- boot_df_white |&gt; summarize(upper_bound_white = quantile(stat, 0.975)) |&gt; pull()"
  },
  {
    "objectID": "slides/week-7.2.html#section-4",
    "href": "slides/week-7.2.html#section-4",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Now, let’s create the CIs for black applicants.\n\n\nboot_df_black &lt;- myDat |&gt;\n  filter(race == \"black\") |&gt; \n  specify(response = received_callback) |&gt;  \n  generate(reps = 15000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\nlower_bound_black &lt;- boot_df_black |&gt; summarize(lower_bound_black = quantile(stat, 0.025)) |&gt; pull() \nupper_bound_black &lt;- boot_df_black |&gt; summarize(upper_bound_black = quantile(stat, 0.975)) |&gt; pull()"
  },
  {
    "objectID": "slides/week-7.2.html#section-5",
    "href": "slides/week-7.2.html#section-5",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Now, let’s tidy the data for plotting.\n\n\nplotData &lt;- tibble(\n  race = c(\"Black\", \"White\"),\n  meanCalls = c(mean_black, mean_white),\n  lower95 = c(lower_bound_black, lower_bound_white),\n  upper95 = c(upper_bound_black, upper_bound_white)\n)\nplotData\n\n# A tibble: 2 × 4\n  race  meanCalls lower95 upper95\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Black    0.0645  0.0550  0.0743\n2 White    0.0965  0.0850  0.108"
  },
  {
    "objectID": "slides/week-7.2.html#plot",
    "href": "slides/week-7.2.html#plot",
    "title": "Hypothesis Testing 2",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "slides/week-7.2.html#plot-1",
    "href": "slides/week-7.2.html#plot-1",
    "title": "Hypothesis Testing 2",
    "section": "Plot",
    "text": "Plot\n\n\nggplot(plotData, aes(y = meanCalls, x = race, ymin = lower95, ymax = upper95)) +\n  geom_col(fill = \"steelblue4\") +\n  geom_errorbar(width = .05) +\n  theme_bw()  +\n  ylim(0, .15) +\n  labs(x = \"Race of Applicant\",\n       y = \"Call Back Rate\")"
  },
  {
    "objectID": "slides/week-7.2.html#is-this-evidence-of-racial-discrimination",
    "href": "slides/week-7.2.html#is-this-evidence-of-racial-discrimination",
    "title": "Hypothesis Testing 2",
    "section": "Is this evidence of racial discrimination?",
    "text": "Is this evidence of racial discrimination?\n\n\nWhat is the null hypothesis?\n\n\n\nWhat is the alternative hypothesis?\n\n\n\n\nHow can we formally test the null hypothesis to decide whether to reject it?"
  },
  {
    "objectID": "slides/week-7.2.html#formal-hypothesis-test",
    "href": "slides/week-7.2.html#formal-hypothesis-test",
    "title": "Hypothesis Testing 2",
    "section": "Formal Hypothesis Test",
    "text": "Formal Hypothesis Test\n\n\nCalculate the difference in means (White - Black)\nShuffle the race variable\nCalculate the difference in means for the shuffled data\nRepeat many times\nSimulates the null distribution of differences in callbacks"
  },
  {
    "objectID": "slides/week-7.2.html#hypothetical-original-data",
    "href": "slides/week-7.2.html#hypothetical-original-data",
    "title": "Hypothesis Testing 2",
    "section": "Hypothetical Original Data",
    "text": "Hypothetical Original Data\n\n\n\n\nApplicant\nRace\nCallback\n\n\n\n\nA\nBlack\nYes\n\n\nB\nBlack\nNo\n\n\nC\nBlack\nNo\n\n\nD\nWhite\nYes\n\n\nE\nWhite\nNo\n\n\nF\nWhite\nNo"
  },
  {
    "objectID": "slides/week-7.2.html#step-1-calculate-original-difference-in-callback-rates",
    "href": "slides/week-7.2.html#step-1-calculate-original-difference-in-callback-rates",
    "title": "Hypothesis Testing 2",
    "section": "Step 1: Calculate Original Difference in Callback Rates",
    "text": "Step 1: Calculate Original Difference in Callback Rates\n\n\nObjective: Understand initial association between race and callback rates"
  },
  {
    "objectID": "slides/week-7.2.html#step-2-shuffle-permute-the-race-variable",
    "href": "slides/week-7.2.html#step-2-shuffle-permute-the-race-variable",
    "title": "Hypothesis Testing 2",
    "section": "Step 2: Shuffle (Permute) the Race Variable",
    "text": "Step 2: Shuffle (Permute) the Race Variable\n\n\nMethod: Randomly reassign race labels, keeping callback outcomes fixed"
  },
  {
    "objectID": "slides/week-7.2.html#hypothetical-shuffled-data",
    "href": "slides/week-7.2.html#hypothetical-shuffled-data",
    "title": "Hypothesis Testing 2",
    "section": "Hypothetical Shuffled Data",
    "text": "Hypothetical Shuffled Data\n\n\n\n\nApplicant\nRace (Shuffled)\nCallback\n\n\n\n\nA\nWhite\nYes\n\n\nB\nBlack\nNo\n\n\nC\nWhite\nNo\n\n\nD\nWhite\nYes\n\n\nE\nBlack\nNo\n\n\nF\nBlack\nNo"
  },
  {
    "objectID": "slides/week-7.2.html#step-3-calculate-difference-in-callback-rates-again",
    "href": "slides/week-7.2.html#step-3-calculate-difference-in-callback-rates-again",
    "title": "Hypothesis Testing 2",
    "section": "Step 3: Calculate Difference in Callback Rates Again",
    "text": "Step 3: Calculate Difference in Callback Rates Again\n\n\nAfter Shuffling: Calculate the difference in callback rates between Black and White groups\nPurpose: Determine if observed difference is due to chance"
  },
  {
    "objectID": "slides/week-7.2.html#repeat-many-times",
    "href": "slides/week-7.2.html#repeat-many-times",
    "title": "Hypothesis Testing 2",
    "section": "Repeat Many Times",
    "text": "Repeat Many Times\n\n\nRepeat shuffling 5000 times to generate a distribution of differences by chance\nTest: Compare observed difference to null distribution to assess effect of race on callbacks\nIf observed difference is extreme (p-value is low), reject the null hypothesis"
  },
  {
    "objectID": "slides/week-7.2.html#simulating-with-tidymodels",
    "href": "slides/week-7.2.html#simulating-with-tidymodels",
    "title": "Hypothesis Testing 2",
    "section": "Simulating with tidymodels",
    "text": "Simulating with tidymodels\n\nIn real life we are going to use the tidymodels package to do the simulation for us.\n\nnull_dist &lt;- myDat |&gt;\n  specify(response = received_callback, explanatory = race) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(5000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", \n            order = c(\"white\", \"black\")) #"
  },
  {
    "objectID": "slides/week-7.2.html#visualize",
    "href": "slides/week-7.2.html#visualize",
    "title": "Hypothesis Testing 2",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/week-7.2.html#visualize-1",
    "href": "slides/week-7.2.html#visualize-1",
    "title": "Hypothesis Testing 2",
    "section": "Visualize",
    "text": "Visualize\n\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01, fill = \"steelblue4\") +\n  labs(title = \"Null distribution + Estimate\",\n       x = \"Estimated Difference under the Null\") +     \n  geom_vline(xintercept = teffect, linetype=\"dotted\", \n                color = \"black\", size=1) + theme_bw()"
  },
  {
    "objectID": "slides/week-7.2.html#calculate-the-p-value",
    "href": "slides/week-7.2.html#calculate-the-p-value",
    "title": "Hypothesis Testing 2",
    "section": "Calculate the p-value",
    "text": "Calculate the p-value\n\n\nnull_dist |&gt;\n  filter(stat &gt; teffect) |&gt;\n  summarise(p_value = n()/nrow(null_dist)) \n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0"
  },
  {
    "objectID": "slides/week-7.2.html#what-should-we-conclude",
    "href": "slides/week-7.2.html#what-should-we-conclude",
    "title": "Hypothesis Testing 2",
    "section": "What should we conclude?",
    "text": "What should we conclude?\n\n\nThe p-value is very small (below .05 threshold)\nTherefore, we reject the null hypothesis: the racial gap is extremely unlikely to have occurred due to chance alone\nThis is evidence of racial discrimination"
  },
  {
    "objectID": "slides/week-7.2.html#your-turn",
    "href": "slides/week-7.2.html#your-turn",
    "title": "Hypothesis Testing 2",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nUse the gender variable in the resume data to assess whether there is gender discrimination in call backs\nPlot means and 95% confidence intervals for the call back rate for men and women\nWrite the null and alternative hypotheses\nSimulate the null distribution\nVisualize the null distribution and the gender gap\nCalculate the p-value\nWhat do you conclude from your test?"
  },
  {
    "objectID": "slides/week-2.1.html#getting-started-with-data",
    "href": "slides/week-2.1.html#getting-started-with-data",
    "title": "Bar Charts and Histograms",
    "section": "Getting Started with Data",
    "text": "Getting Started with Data\n\n\nTabular data is data that is organized into rows and columns\n\na.k.a. rectangular data\n\nA data frame is a special kind of tabular data used in data science\nA variable is something you can measure\nAn observation is a single unit or case in your data set\nThe unit of analysis is the level at which you are measuring\n\nIn a cross-section: country, state, county, city, individual, etc.\nIn a time-series: year, month, day, etc."
  },
  {
    "objectID": "slides/week-2.1.html#example",
    "href": "slides/week-2.1.html#example",
    "title": "Bar Charts and Histograms",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/week-2.1.html#some-basic-r-code",
    "href": "slides/week-2.1.html#some-basic-r-code",
    "title": "Bar Charts and Histograms",
    "section": "Some Basic R Code",
    "text": "Some Basic R Code\n\n\n&lt;- is the assignment operator\n\nUse it to assign values to objects\n\n# is the comment operator\n\nUse it to comment out code or add comments\nDifferent function than in markdown text\n\nTo call a library, use library() and name of library\n\nname of library does not have to be in quotes, e.g. library(readr)\nonly when you install it, e.g. install.packages(\"readr\")"
  },
  {
    "objectID": "slides/week-2.1.html#read-data-into-r",
    "href": "slides/week-2.1.html#read-data-into-r",
    "title": "Bar Charts and Histograms",
    "section": "Read Data into R",
    "text": "Read Data into R\n\n\n# load libraries\nlibrary(readr)\nlibrary(dplyr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")"
  },
  {
    "objectID": "slides/week-2.1.html#viewing-the-data-in-r",
    "href": "slides/week-2.1.html#viewing-the-data-in-r",
    "title": "Bar Charts and Histograms",
    "section": "Viewing the Data in R",
    "text": "Viewing the Data in R\n\nUse glimpse() to see the columns and data types:\n\n# load libraries\nlibrary(readr)\nlibrary(dplyr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nglimpse(dem_summary)\n\nRows: 6\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\", \"Afri…\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319\n$ flfp      &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.21568"
  },
  {
    "objectID": "slides/week-2.1.html#try-it-yourself",
    "href": "slides/week-2.1.html#try-it-yourself",
    "title": "Bar Charts and Histograms",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\n\nOpen the CSV file to see what it looks like\nThen use this code to read it into R and view it\n\n\n# load libraries\nlibrary(readr)\nlibrary(dplyr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nglimpse(dem_summary)"
  },
  {
    "objectID": "slides/week-2.1.html#the-grammar-of-graphics",
    "href": "slides/week-2.1.html#the-grammar-of-graphics",
    "title": "Bar Charts and Histograms",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\nData viz has a language with its own grammar\nBasic components include:\n\nData we are trying to visualize\nAesthetics (dimensions)\nGeom (e.g. bar, line, scatter plot)\nColor scales\nThemes\nAnnotations"
  },
  {
    "objectID": "slides/week-2.1.html#section-1",
    "href": "slides/week-2.1.html#section-1",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Let’s start with the first two, the data and the aesthetic…\n\n\nlibrary(readr)\nlibrary(ggplot2)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nggplot(dem_summary, aes(x = region, y = polyarchy))"
  },
  {
    "objectID": "slides/week-2.1.html#section-2",
    "href": "slides/week-2.1.html#section-2",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "This gives us the axes without any visualization:"
  },
  {
    "objectID": "slides/week-2.1.html#section-3",
    "href": "slides/week-2.1.html#section-3",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Now let’s add a geom. In this case we want a bar chart so we add geom_col().\n\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + \n  geom_col()"
  },
  {
    "objectID": "slides/week-2.1.html#section-4",
    "href": "slides/week-2.1.html#section-4",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "That gets the idea across but looks a little depressing, so…"
  },
  {
    "objectID": "slides/week-2.1.html#section-5",
    "href": "slides/week-2.1.html#section-5",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "…let’s change the color of the bars by specifying fill = \"steelblue\".\n\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + \n  geom_col(fill = \"steelblue\")"
  },
  {
    "objectID": "slides/week-2.1.html#section-6",
    "href": "slides/week-2.1.html#section-6",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Note how color of original bars is simply overwritten:"
  },
  {
    "objectID": "slides/week-2.1.html#section-7",
    "href": "slides/week-2.1.html#section-7",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Now let’s add some labels with the labs() function:\n\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + \n  geom_col(fill = \"steelblue\") +\n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-2.1.html#section-8",
    "href": "slides/week-2.1.html#section-8",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "And that gives us…"
  },
  {
    "objectID": "slides/week-2.1.html#section-9",
    "href": "slides/week-2.1.html#section-9",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Next, we reorder the bars with fct_reorder() from the forcats package.\n\n\nlibrary(forcats)\n\nggplot(dem_summary, aes(x = fct_reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\nNote that we could also use the base R reorder() function here."
  },
  {
    "objectID": "slides/week-2.1.html#section-10",
    "href": "slides/week-2.1.html#section-10",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "This way, we get a nice, visually appealing ordering of the bars according to levels of democracy…"
  },
  {
    "objectID": "slides/week-2.1.html#section-11",
    "href": "slides/week-2.1.html#section-11",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Now let’s change the theme to theme_minimal().\n\n\nggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-2.1.html#section-12",
    "href": "slides/week-2.1.html#section-12",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Gives us a clean, elegant look."
  },
  {
    "objectID": "slides/week-2.1.html#section-13",
    "href": "slides/week-2.1.html#section-13",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Note that you can also save your plot as an object to modify later.\n\n\ndem_bar_chart &lt;- ggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\")"
  },
  {
    "objectID": "slides/week-2.1.html#section-14",
    "href": "slides/week-2.1.html#section-14",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Which gives us…\n\ndem_bar_chart"
  },
  {
    "objectID": "slides/week-2.1.html#section-15",
    "href": "slides/week-2.1.html#section-15",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Now let’s add back our labels…\n\n\ndem_bar_chart &lt;- dem_bar_chart +\n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-2.1.html#section-16",
    "href": "slides/week-2.1.html#section-16",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "So now we have…\n\ndem_bar_chart"
  },
  {
    "objectID": "slides/week-2.1.html#section-17",
    "href": "slides/week-2.1.html#section-17",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "And now we’ll add back our theme…\n\n\ndem_bar_chart &lt;- dem_bar_chart + theme_minimal()"
  },
  {
    "objectID": "slides/week-2.1.html#section-18",
    "href": "slides/week-2.1.html#section-18",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Voila!\n\ndem_bar_chart"
  },
  {
    "objectID": "slides/week-2.1.html#section-19",
    "href": "slides/week-2.1.html#section-19",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Change the theme. There are many themes to choose from.\n\ndem_bar_chart + theme_bw()"
  },
  {
    "objectID": "slides/week-2.1.html#your-turn",
    "href": "slides/week-2.1.html#your-turn",
    "title": "Bar Charts and Histograms",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nglimpse() the data\nFind a new variable to visualize\nMake a bar chart with it\nChange the color of the bars\nOrder the bars\nAdd labels\nAdd a theme\nTry saving your plot as an object\nThen change the labels and/or theme"
  },
  {
    "objectID": "slides/week-2.1.html#purpose-of-histograms",
    "href": "slides/week-2.1.html#purpose-of-histograms",
    "title": "Bar Charts and Histograms",
    "section": "Purpose of Histograms",
    "text": "Purpose of Histograms\n\nHistograms are used to visualize the distribution of a single variable\nThey are used for continuous variables (e.g., income, age, etc.)\nA continuous variable is one that can take on any value within a range (e.g., 0.5, 1.2, 3.7, etc.)\nA discrete variable is one that can only take on certain values (e.g., 1, 2, 3, etc.)\nx-axis represents value of variable of interest\ny-axis represents the frequency of that value"
  },
  {
    "objectID": "slides/week-2.1.html#example-1",
    "href": "slides/week-2.1.html#example-1",
    "title": "Bar Charts and Histograms",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/week-2.1.html#histogram-code",
    "href": "slides/week-2.1.html#histogram-code",
    "title": "Bar Charts and Histograms",
    "section": "Histogram Code",
    "text": "Histogram Code\n\n\n# load data\ndem_women &lt;- read_csv(\"data/dem_women.csv\")\n\n# filter to 2022\ndem_women_2022 &lt;- dem_women |&gt;\n  filter(year == 2022) \n\n# create histogram\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-2.1.html#change-number-of-bins",
    "href": "slides/week-2.1.html#change-number-of-bins",
    "title": "Bar Charts and Histograms",
    "section": "Change Number of Bins",
    "text": "Change Number of Bins\n\nChange number of bins (bars) using bins or binwidth arguments (default number of bins = 30):\n\n\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(bins = 50, fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-2.1.html#section-20",
    "href": "slides/week-2.1.html#section-20",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "At 50 bins…"
  },
  {
    "objectID": "slides/week-2.1.html#section-21",
    "href": "slides/week-2.1.html#section-21",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "At 100 bins…probably too many!"
  },
  {
    "objectID": "slides/week-2.1.html#section-22",
    "href": "slides/week-2.1.html#section-22",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Using binwidth instead of bins…\n\n\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-2.1.html#section-23",
    "href": "slides/week-2.1.html#section-23",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Setting binwidth to 2…"
  },
  {
    "objectID": "slides/week-2.1.html#change-from-count-to-density",
    "href": "slides/week-2.1.html#change-from-count-to-density",
    "title": "Bar Charts and Histograms",
    "section": "Change from Count to Density",
    "text": "Change from Count to Density\n\n\nggplot(dem_women_2022, aes(after_stat(density), x = flfp)) +\n  geom_histogram(fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Density\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-2.1.html#section-24",
    "href": "slides/week-2.1.html#section-24",
    "title": "Bar Charts and Histograms",
    "section": "",
    "text": "Which gives us…"
  },
  {
    "objectID": "slides/week-2.1.html#your-turn-1",
    "href": "slides/week-2.1.html#your-turn-1",
    "title": "Bar Charts and Histograms",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nPick a variable that you want to explore the distribution of\nMake a histogram\n\nOnly specify x = in aes()\nSpecify geom as geom_histogram\n\nChoose color for bars\nChoose appropriate labels\nChange number of bins\nChange from count to density"
  },
  {
    "objectID": "slides/week-2.2.html#huntingtons-three-waves",
    "href": "slides/week-2.2.html#huntingtons-three-waves",
    "title": "Line Charts & Scatter Plots",
    "section": "Huntington’s Three Waves",
    "text": "Huntington’s Three Waves"
  },
  {
    "objectID": "slides/week-2.2.html#line-chart",
    "href": "slides/week-2.2.html#line-chart",
    "title": "Line Charts & Scatter Plots",
    "section": "Line Chart",
    "text": "Line Chart"
  },
  {
    "objectID": "slides/week-2.2.html#section",
    "href": "slides/week-2.2.html#section",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Here is the code…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-2.2.html#section-1",
    "href": "slides/week-2.2.html#section-1",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Use geom_line() to specify a line chart…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-2.2.html#section-2",
    "href": "slides/week-2.2.html#section-2",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Add third dimension to the aes() call for line color…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-2.2.html#section-3",
    "href": "slides/week-2.2.html#section-3",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Modify the legend title…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-2.2.html#section-4",
    "href": "slides/week-2.2.html#section-4",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Use a colorblind safe color scheme like viridis…"
  },
  {
    "objectID": "slides/week-2.2.html#section-5",
    "href": "slides/week-2.2.html#section-5",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Here scale_color_viridis_d() specifies the viridis color scheme…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  ) +\n  scale_color_viridis_d(option = \"mako\", end = .8) # use viridis color palette"
  },
  {
    "objectID": "slides/week-2.2.html#palettes",
    "href": "slides/week-2.2.html#palettes",
    "title": "Line Charts & Scatter Plots",
    "section": "Palettes",
    "text": "Palettes\n\n\nThere are a number of viridis palettes\nSee this reference to view different palettes and options\nYou can also use scale_color_viridis_c() to specify a continuous color scale\nAlso check out the paletteer package for easy access to many more palettes"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn",
    "href": "slides/week-2.2.html#your-turn",
    "title": "Line Charts & Scatter Plots",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nSee table three of this article\nSelect three countries to visualize\nAdjust setup code to filter data on those countries\nVisualize with geom_line()\nUse scale_color_viridis_d() to specify a viridis color scheme"
  },
  {
    "objectID": "slides/week-2.2.html#scatter-plot",
    "href": "slides/week-2.2.html#scatter-plot",
    "title": "Line Charts & Scatter Plots",
    "section": "Scatter Plot",
    "text": "Scatter Plot"
  },
  {
    "objectID": "slides/week-2.2.html#scatter-plot-1",
    "href": "slides/week-2.2.html#scatter-plot-1",
    "title": "Line Charts & Scatter Plots",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nUse geom_point()…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) +\n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#scatter-plot-2",
    "href": "slides/week-2.2.html#scatter-plot-2",
    "title": "Line Charts & Scatter Plots",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nFour dimensions…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#scatter-plot-3",
    "href": "slides/week-2.2.html#scatter-plot-3",
    "title": "Line Charts & Scatter Plots",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nStretch axis on log scale and use scales package to adjust labels…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#scatter-plot-4",
    "href": "slides/week-2.2.html#scatter-plot-4",
    "title": "Line Charts & Scatter Plots",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nChange legend titles…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn-1",
    "href": "slides/week-2.2.html#your-turn-1",
    "title": "Line Charts & Scatter Plots",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nThere are four variables in dem_summary_ctry\nPick one related to women’s empowerment\nVisualize it on the y-axis with gdp_pc or polyarchy on the x-axis\nChange labels and legend titles to match your visualization\nInterpret your plot"
  },
  {
    "objectID": "slides/week-2.2.html#add-a-trend-line",
    "href": "slides/week-2.2.html#add-a-trend-line",
    "title": "Line Charts & Scatter Plots",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line"
  },
  {
    "objectID": "slides/week-2.2.html#add-a-trend-line-1",
    "href": "slides/week-2.2.html#add-a-trend-line-1",
    "title": "Line Charts & Scatter Plots",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line\n\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#add-a-trend-line-2",
    "href": "slides/week-2.2.html#add-a-trend-line-2",
    "title": "Line Charts & Scatter Plots",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line\n\nTaking out size and adding color to geom_point() call…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) + \n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#add-a-trend-line-3",
    "href": "slides/week-2.2.html#add-a-trend-line-3",
    "title": "Line Charts & Scatter Plots",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line\n\nChanging legend titles…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn-2",
    "href": "slides/week-2.2.html#your-turn-2",
    "title": "Line Charts & Scatter Plots",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nAdd a trendline to your plot\nChange the labels accordingly\nTry using method = \"loess\" instead of a “lm”"
  },
  {
    "objectID": "slides/week-2.2.html#facet-wrapping",
    "href": "slides/week-2.2.html#facet-wrapping",
    "title": "Line Charts & Scatter Plots",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping"
  },
  {
    "objectID": "slides/week-2.2.html#facet-wrapping-1",
    "href": "slides/week-2.2.html#facet-wrapping-1",
    "title": "Line Charts & Scatter Plots",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping\n\nUse facet_wrap() with ~ before variable you want to wrap on…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-2.2.html#facet-wrapping-2",
    "href": "slides/week-2.2.html#facet-wrapping-2",
    "title": "Line Charts & Scatter Plots",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping\n\nWhat else changes? Back down to two dimensions…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-2.2.html#facet-wrapping-3",
    "href": "slides/week-2.2.html#facet-wrapping-3",
    "title": "Line Charts & Scatter Plots",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping\n\nDon’t forget to take the legend title out of the captions…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn-3",
    "href": "slides/week-2.2.html#your-turn-3",
    "title": "Line Charts & Scatter Plots",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nFacet wrap your scatter plot\nUse scales = \"free\" in facet_wrap call to fix the West\n\nfacet_wrap(~ region, scales = \"free\")"
  },
  {
    "objectID": "slides/week-2.2.html#labeling-points",
    "href": "slides/week-2.2.html#labeling-points",
    "title": "Line Charts & Scatter Plots",
    "section": "Labeling Points",
    "text": "Labeling Points"
  },
  {
    "objectID": "slides/week-2.2.html#labeling-points-1",
    "href": "slides/week-2.2.html#labeling-points-1",
    "title": "Line Charts & Scatter Plots",
    "section": "Labeling Points",
    "text": "Labeling Points\n\nFilter for Asia, add labels with geom_text()…\n\ndem_summary_ctry |&gt; \n  filter(region == \"Asia\") |&gt;\n  ggplot(aes(x = gdp_pc, y = polyarchy)) + \n    geom_point() + \n    geom_text(aes(label = country), size = 2, vjust = 2) +\n    geom_smooth(method = \"lm\", linewidth = 1) +\n    scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n      labs(\n        x= \"GDP Per Capita\", \n        y = \"Polyarchy Score\",\n        title = \"Wealth and democracy in Asia, 1990 - present\", \n        caption = \"Source: V-Dem Institute\"\n        )"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn-4",
    "href": "slides/week-2.2.html#your-turn-4",
    "title": "Line Charts & Scatter Plots",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nFilter for Asia or another region\nUse geom_text() to add labels to your points\nPlay with size and vjust paramters"
  },
  {
    "objectID": "slides/week-2.2.html#make-it-interactive",
    "href": "slides/week-2.2.html#make-it-interactive",
    "title": "Line Charts & Scatter Plots",
    "section": "Make it Interactive",
    "text": "Make it Interactive"
  },
  {
    "objectID": "slides/week-2.2.html#section-6",
    "href": "slides/week-2.2.html#section-6",
    "title": "Line Charts & Scatter Plots",
    "section": "",
    "text": "Use plotly to make any plot interactive…\n\nlibrary(plotly)\n\nmodernization_plot &lt;- ggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  aes(label = country) +\n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)\n\nggplotly(modernization_plot, tooltip = c(\"country\", \"gdp_pc\", \"polyarchy\"))"
  },
  {
    "objectID": "slides/week-4.1.html#messages-warnings-and-errors",
    "href": "slides/week-4.1.html#messages-warnings-and-errors",
    "title": "Data Viz Review",
    "section": "Messages, Warnings and Errors",
    "text": "Messages, Warnings and Errors\n\n\nMessages tell you what R is doing\nWarnings tell you that something might be wrong\nErrors tell you that something is definitely wrong\n\nLocate the error line number in the console and check your code\nError line tells you about where the error occurred, not exact\nErrors are normal, don’t freak out!\nIn fact, you should practice making errors to learn how to fix them"
  },
  {
    "objectID": "slides/week-4.1.html#section",
    "href": "slides/week-4.1.html#section",
    "title": "Data Viz Review",
    "section": "",
    "text": "Your boss wants you to…\n\nVisualize trends in liberal democracy since the end of WWII for countries in South Asia\nSee how regions compare in terms of levels of corruption in 2015\nView the relationship between GDP per capita and electoral democracy in 2020"
  },
  {
    "objectID": "slides/week-4.1.html#could-you-reproduce-it",
    "href": "slides/week-4.1.html#could-you-reproduce-it",
    "title": "Data Viz Review",
    "section": "Could You Reproduce It?",
    "text": "Could You Reproduce It?\n\n\nHave a look at my “Democracy Around the World” Shiny app\nCan you reproduce the visualizations in the app using the V-Dem data?\nWhat are the steps you would take to do so?"
  },
  {
    "objectID": "slides/week-4.1.html#homework-1",
    "href": "slides/week-4.1.html#homework-1",
    "title": "Data Viz Review",
    "section": "Homework 1",
    "text": "Homework 1\n\n\nFind Homework 1 on the course website\nGet started today!"
  },
  {
    "objectID": "slides/week-6.2.html#section",
    "href": "slides/week-6.2.html#section",
    "title": "Confidence Intervals",
    "section": "",
    "text": "On December 19, 2014, the front page of Spanish national newspaper El País read “Catalan public opinion swings toward ‘no’ for independence, says survey”.1\n\n\n\n\n\n\n\n\n\nAlberto Cairo. The truthful art: Data, charts, and maps for communication. New Riders, 2016."
  },
  {
    "objectID": "slides/week-6.2.html#section-1",
    "href": "slides/week-6.2.html#section-1",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The probability of the tiny difference between the ‘No’ and ‘Yes’ being just due to random chance is very high.1\n\n\n\n\n\n\n\n\n\nAlberto Cairo. “Uncertainty and Graphicacy”, 2017."
  },
  {
    "objectID": "slides/week-6.2.html#characterizing-uncertainty",
    "href": "slides/week-6.2.html#characterizing-uncertainty",
    "title": "Confidence Intervals",
    "section": "Characterizing Uncertainty",
    "text": "Characterizing Uncertainty\n\n\nWe know from previous section that even unbiased procedures do not get the “right” answer every time\nWe also know that our estimates might vary from sample to sample due to random chance\nTherefore we want to report on our estimate and our level of uncertainty"
  },
  {
    "objectID": "slides/week-6.2.html#characterizing-uncertainty-1",
    "href": "slides/week-6.2.html#characterizing-uncertainty-1",
    "title": "Confidence Intervals",
    "section": "Characterizing Uncertainty",
    "text": "Characterizing Uncertainty\n\n\nWith M&Ms, we knew the population parameter\nIn real life, we do not!\nWe want to generate an estimate and characterize our uncertainty with a range of possible estimates"
  },
  {
    "objectID": "slides/week-6.2.html#solution-create-a-confidence-interval",
    "href": "slides/week-6.2.html#solution-create-a-confidence-interval",
    "title": "Confidence Intervals",
    "section": "Solution: Create a Confidence Interval",
    "text": "Solution: Create a Confidence Interval\n\n\nA plausible range of values for the population parameter is a confidence interval.\n\n\n\n95 percent confidence interval is standard\n\nWe are 95% confident that the parameter value falls within the range given by the confidence interval"
  },
  {
    "objectID": "slides/week-6.2.html#ways-to-estimate",
    "href": "slides/week-6.2.html#ways-to-estimate",
    "title": "Confidence Intervals",
    "section": "Ways to Estimate",
    "text": "Ways to Estimate\n\n\nTake advantage of Central Limit Theorem to estimate using math\nUse simulation, bootstrapping"
  },
  {
    "objectID": "slides/week-6.2.html#with-math",
    "href": "slides/week-6.2.html#with-math",
    "title": "Confidence Intervals",
    "section": "With Math…",
    "text": "With Math…\n\\[CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\n\n\\(\\bar{x}\\) is the sample mean,\n\\(Z\\) is the Z-score corresponding to the desired level of confidence\n\\(\\sigma\\) is the population standard deviation, and\n\\(n\\) is the sample size"
  },
  {
    "objectID": "slides/week-6.2.html#section-2",
    "href": "slides/week-6.2.html#section-2",
    "title": "Confidence Intervals",
    "section": "",
    "text": "This part here represents the standard error:\n\\[\\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\n\nStandard deviation of the sampling distribution\nCharacterizes the spread of the sampling distribution\nThe bigger this is the bigger the CIs are going to be"
  },
  {
    "objectID": "slides/week-6.2.html#central-limit-theorem",
    "href": "slides/week-6.2.html#central-limit-theorem",
    "title": "Confidence Intervals",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\n\nThis way of doing things depends on the Central Limit Theorem\nAs sample size gets bigger, the spread of the sampling distribution gets narrower\nThe shape of the sampling distributions becomes more normally distributed"
  },
  {
    "objectID": "slides/week-6.2.html#section-3",
    "href": "slides/week-6.2.html#section-3",
    "title": "Confidence Intervals",
    "section": "",
    "text": "\\[CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\nThis is therefore a parametric method of calculating the CI. It depends on assumptions about the normality of the distribution."
  },
  {
    "objectID": "slides/week-6.2.html#bootstrapping",
    "href": "slides/week-6.2.html#bootstrapping",
    "title": "Confidence Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n\nPulling oneself up from their bootstraps …\nUse the data we have to estimate the sampling distribution\nWe call this the bootstrap distribution\nThis is a nonparametric method\nIt does not depend on assumptions about normality"
  },
  {
    "objectID": "slides/week-6.2.html#bootstrap-process",
    "href": "slides/week-6.2.html#bootstrap-process",
    "title": "Confidence Intervals",
    "section": "Bootstrap Process",
    "text": "Bootstrap Process\n\n\nTake a bootstrap sample - a random sample taken with replacement from the original sample, of the same size as the original sample\n\n\n\nCalculate the bootstrap statistic - a statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples\n\n\n\n\nRepeat steps (1) and (2) many times to create a bootstrap distribution - a distribution of bootstrap statistics\n\n\n\n\nCalculate the bounds of the XX% confidence interval as the middle XX% of the bootstrap distribution (usually 95 percent confidence interval)"
  },
  {
    "objectID": "slides/week-6.2.html#russia",
    "href": "slides/week-6.2.html#russia",
    "title": "Confidence Intervals",
    "section": "Russia",
    "text": "Russia\n\nWhat Proportion of Russians believe their country interfered in the 2016 presidential elections in the US?\n\nPew Research survey\n506 subjects\nData available in the openintro package"
  },
  {
    "objectID": "slides/week-6.2.html#section-4",
    "href": "slides/week-6.2.html#section-4",
    "title": "Confidence Intervals",
    "section": "",
    "text": "For this example, we will use data from the Open Intro package. Install that package before running this code chunk.\n\n\n#install.packages(\"openintro\")\nlibrary(openintro)\n\nglimpse(russian_influence_on_us_election_2016)\n\nRows: 506\nColumns: 1\n$ influence_2016 &lt;chr&gt; \"Did not try\", \"Did not try\", \"Did not try\", \"Don't kno…"
  },
  {
    "objectID": "slides/week-6.2.html#section-5",
    "href": "slides/week-6.2.html#section-5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Let’s use mutate() to recode the qualitative variable as a numeric one…\n\n\nrussiaData &lt;- russian_influence_on_us_election_2016 |&gt; \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))"
  },
  {
    "objectID": "slides/week-6.2.html#section-6",
    "href": "slides/week-6.2.html#section-6",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Now let’s calculate the mean and standard deviation of the try_influence variable…\n\n\nrussiaData |&gt;\n  summarize( \n          mean = mean(try_influence),\n          sd = sd(try_influence)\n  )\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.150 0.358"
  },
  {
    "objectID": "slides/week-6.2.html#section-7",
    "href": "slides/week-6.2.html#section-7",
    "title": "Confidence Intervals",
    "section": "",
    "text": "And finally let’s draw a bar plot…\n\n\nggplot(russiaData, aes(x = try_influence)) +\n  geom_bar(fill = \"steelblue\", width = .75) +\n  labs(\n    title = \"Did Russia try to influence the U.S. election?\",\n    x = \"0 = 'No', 1 = 'Yes'\",\n    y = \"Frequncy\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-6.2.html#bootstrap-with-tidymodels",
    "href": "slides/week-6.2.html#bootstrap-with-tidymodels",
    "title": "Confidence Intervals",
    "section": "Bootstrap with tidymodels",
    "text": "Bootstrap with tidymodels\nInstall tidymodels before running this code chunk…\n\n#install.packages(\"tidymodels\")\nlibrary(tidymodels)\n\nset.seed(66)\nboot_df &lt;- russiaData |&gt;\n  # specify the variable of interest\n  specify(response = try_influence) |&gt;\n  # generate 15000 bootstrap samples\n  generate(reps = 15000, type = \"bootstrap\") |&gt;\n  # calculate the mean of each bootstrap sample\n  calculate(stat = \"mean\")\n\nglimpse(boot_df)\n\nRows: 15,000\nColumns: 2\n$ replicate &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ stat      &lt;dbl&gt; 0.1146245, 0.1442688, 0.1343874, 0.1877470, 0.1521739, 0.138…"
  },
  {
    "objectID": "slides/week-6.2.html#section-9",
    "href": "slides/week-6.2.html#section-9",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Calculate the confidence interval. A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution.\n\n\nboot_df |&gt;\n  summarize(lower = quantile(stat, 0.025),\n            upper = quantile(stat, 0.975))\n\n# A tibble: 1 × 2\n  lower upper\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.119 0.182"
  },
  {
    "objectID": "slides/week-6.2.html#section-10",
    "href": "slides/week-6.2.html#section-10",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Create upper and lower bounds for visualization.\n\n\n# for using these values later\nlower_bound &lt;- boot_df |&gt; summarize(lower_bound = quantile(stat, 0.025)) |&gt; pull() \nupper_bound &lt;- boot_df |&gt; summarize(upper_bound = quantile(stat, 0.975)) |&gt; pull()"
  },
  {
    "objectID": "slides/week-6.2.html#section-11",
    "href": "slides/week-6.2.html#section-11",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Visualize with a histogram\n\n\nggplot(data = boot_df, mapping = aes(x = stat)) +\n  geom_histogram(binwidth =.01, fill = \"steelblue4\") +\n  geom_vline(xintercept = c(lower_bound, upper_bound), color = \"darkgrey\", size = 1, linetype = \"dashed\") +\n  labs(title = \"Bootstrap distribution of means\",\n       subtitle = \"and 95% confidence interval\",\n       x = \"Estimate\",\n       y = \"Frequency\") +\n  theme_bw()"
  },
  {
    "objectID": "slides/week-6.2.html#interpret-the-confidence-interval",
    "href": "slides/week-6.2.html#interpret-the-confidence-interval",
    "title": "Confidence Intervals",
    "section": "Interpret the confidence interval",
    "text": "Interpret the confidence interval\n\nThe 95% confidence interval was calculated as (lower_bound, upper_bound). Which of the following is the correct interpretation of this interval?\n\n(a) 95% of the time the percentage of Russian who believe that Russia interfered in the 2016 US elections is between lower_bound and upper_bound.\n(b) 95% of all Russians believe that the chance Russia interfered in the 2016 US elections is between lower_bound and upper_bound.\n(c) We are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 US election is between lower_bound and upper_bound.\n(d) We are 95% confident that the proportion of Russians who supported interfering in the 2016 US elections is between lower_bound and upper_bound."
  },
  {
    "objectID": "slides/week-6.2.html#your-turn",
    "href": "slides/week-6.2.html#your-turn",
    "title": "Confidence Intervals",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nChange the reps argument in the generate() function to 1000. What happens to the width of the confidence interval?\nChange the reps argument in the generate() function to 5000. What happens to the width of the confidence interval?\nChange the reps argument in the generate() function to 10000. What happens to the width of the confidence interval?\nHow does the width of the confidence interval change as the number of bootstrap samples increases?\nHow would you interpret this finding?"
  },
  {
    "objectID": "slides/week-6.2.html#bias-vs-precision",
    "href": "slides/week-6.2.html#bias-vs-precision",
    "title": "Confidence Intervals",
    "section": "Bias vs Precision",
    "text": "Bias vs Precision\n\nA procedure is unbiased if it generates the “right” answer, on average\nPrecision refers to variability: procedures with less sampling variability will be more precise\n\nall else equal, a greater sample size will increase precision\n\nWhen we increase the sample size (number of reps), we increase precision\nAs a result our confidence interval will be narrower"
  },
  {
    "objectID": "slides/week-6.2.html#why-did-we-do-these-simulations",
    "href": "slides/week-6.2.html#why-did-we-do-these-simulations",
    "title": "Confidence Intervals",
    "section": "Why did we do these simulations?",
    "text": "Why did we do these simulations?\n\n\nThey provide a foundation for statistical inference and for characterizing uncertainty in our estimates\nThe best research designs will try to maximize or achieve good balance on bias vs precision"
  },
  {
    "objectID": "slides/week-14.1.html#advanced-html",
    "href": "slides/week-14.1.html#advanced-html",
    "title": "Authoring in Quarto",
    "section": "Advanced HTML",
    "text": "Advanced HTML\n\n\nSo far we have been using the basic HTML elements to create our documents\nQuarto supports a wide range of HTML elements and attributes\nWe can use these to create more complex and interactive documents"
  },
  {
    "objectID": "slides/week-14.1.html#formatting-options",
    "href": "slides/week-14.1.html#formatting-options",
    "title": "Authoring in Quarto",
    "section": "Formatting Options",
    "text": "Formatting Options\n\nWe can make our documents have a more sophisticated feel by specifying various options in the YAML header\nExamples include title, subtitle, date, abstract, toc, etc.\nWe can also specify font styles and colors and control other aspects of the formatting\nSee this page to explore the various formatting options"
  },
  {
    "objectID": "slides/week-14.1.html#themes",
    "href": "slides/week-14.1.html#themes",
    "title": "Authoring in Quarto",
    "section": "Themes",
    "text": "Themes\n\n\nQuarto supports a wide range of themes\nWe can use these to change the look and feel of our documents\nCheck out this page to explore the various themes available in Quarto"
  },
  {
    "objectID": "slides/week-14.1.html#generating-lorem-ipsum",
    "href": "slides/week-14.1.html#generating-lorem-ipsum",
    "title": "Authoring in Quarto",
    "section": "Generating Lorem Ipsum",
    "text": "Generating Lorem Ipsum\n\n\nFor working on your document’s format it is helpful to have random text\nThe lorem package is helpful for this\nYou can also install an R Studio Addin"
  },
  {
    "objectID": "slides/week-14.1.html#your-turn",
    "href": "slides/week-14.1.html#your-turn",
    "title": "Authoring in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nStart a new Quarto project\nCreate a new Quarto document in your project folder\nGenerate some sections and some random text using the lorem package\nAdd a YAML header with the following options:\n\ntitle:\nsubtitle:\ndate:\ndate-format:\ntheme:\ntoc:\n\nTry adding at least one more option to your YAML header\nRender the document and see what it looks like\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-14.1.html#citations",
    "href": "slides/week-14.1.html#citations",
    "title": "Authoring in Quarto",
    "section": "Citations",
    "text": "Citations\n\n\nWith Quarto you can easily add citations to your document\nThen a bibliography will be automatically generated\nToo add a citation you can use the @ symbol followed by the citation key\nIn order to do this you need to know a little bit about BibTeX and/or CSL"
  },
  {
    "objectID": "slides/week-14.1.html#bibtex",
    "href": "slides/week-14.1.html#bibtex",
    "title": "Authoring in Quarto",
    "section": "BibTeX",
    "text": "BibTeX\n\n\nBibTeX is a reference management software package\nWhen we retrieve a citation for use in Quarto it should be in BibTeX format\nBibTeX is a plain text format that is easy to read and write\nTo work with BibTeX you need a reference manager like Zotero or Mendeley\nWe are going to do Zotero"
  },
  {
    "objectID": "slides/week-14.1.html#zotero",
    "href": "slides/week-14.1.html#zotero",
    "title": "Authoring in Quarto",
    "section": "Zotero",
    "text": "Zotero\n\n\nGo to Zotero\nCreate an account\nDownload the Zotero app\nInstall the Zotero Connector for your browser\nInstall the Better BibTeX extension\n\nFollow these instructions"
  },
  {
    "objectID": "slides/week-14.1.html#downloading-citations",
    "href": "slides/week-14.1.html#downloading-citations",
    "title": "Authoring in Quarto",
    "section": "Downloading Citations",
    "text": "Downloading Citations\n\n\nOpen Zotero\nGo to google scholar\nSearch for a paper on a topic related to your research\nClick the Zotero Connector to download the citation"
  },
  {
    "objectID": "slides/week-14.1.html#adding-citations",
    "href": "slides/week-14.1.html#adding-citations",
    "title": "Authoring in Quarto",
    "section": "Adding Citations",
    "text": "Adding Citations\n\n\nExport the bibliography as a .bib file from Zotero\nAdd the .bib file to your Quarto project folder\nAdd bibliography: and the name of the file to your YAML header\n\ne.g. bibliography: mybib.bib"
  },
  {
    "objectID": "slides/week-14.1.html#citations-in-text",
    "href": "slides/week-14.1.html#citations-in-text",
    "title": "Authoring in Quarto",
    "section": "Citations in Text",
    "text": "Citations in Text\n\n\nNow you can add citations to your text\nUse the @ symbol followed by the citation key\nFor example @smith2020 will add a citation to the paper by Smith (2020) assuming you have a citation in your .bib file with the key smith2020\nNow you can render your document and see the bibliography at the end"
  },
  {
    "objectID": "slides/week-14.1.html#your-turn-1",
    "href": "slides/week-14.1.html#your-turn-1",
    "title": "Authoring in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nAdd a bibliography to your Quarto document\nAdd a citation to your text\nRender the document and see the bibliography at the end\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-14.1.html#footnotes",
    "href": "slides/week-14.1.html#footnotes",
    "title": "Authoring in Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also add footnotes to your document\nUse the ^[] or [^1] syntax to add a footnote\nThere are different ways to add footnotes\nI find that inline notes are the easiest to work with\n\nFor example ^[This is a footnote]"
  },
  {
    "objectID": "slides/week-14.1.html#scholarly-writing",
    "href": "slides/week-14.1.html#scholarly-writing",
    "title": "Authoring in Quarto",
    "section": "Scholarly Writing",
    "text": "Scholarly Writing\n\n\nQuarto is a great tool for scholarly writing\nThere are lots of formatting options for scholarly papers specifically\nCheck out this section of the Quarto guide for more details"
  },
  {
    "objectID": "slides/week-14.1.html#publishing-on-quarto-pub",
    "href": "slides/week-14.1.html#publishing-on-quarto-pub",
    "title": "Authoring in Quarto",
    "section": "Publishing on Quarto Pub",
    "text": "Publishing on Quarto Pub\n\n\nQuarto Pub is a platform for publishing Quarto documents\nYou can publish your documents on Quarto Pub and share them with others\nQuarto Pub is free to use\nSign up for an account, then publish with quarto publish quarto-pub name-of-document.qmd in the terminal"
  },
  {
    "objectID": "slides/week-14.1.html#your-turn-2",
    "href": "slides/week-14.1.html#your-turn-2",
    "title": "Authoring in Quarto",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nAdd a footnote to your Quarto document\nExplore the scholarly writing options in Quarto\nPublish your document on Quarto Pub\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-9.2.html#modeling",
    "href": "slides/week-9.2.html#modeling",
    "title": "Modeling",
    "section": "Modeling",
    "text": "Modeling\n\nUse models to explain the relationship between variables and to make predictions\nExplaining relationships [usually interested in causal relationships, but not always]\n\nDoes oil wealth impact regime type?\n\nPredictive modeling\n\nWhere is violence most likely to happen in [country X] during their next election?\nIs this email spam?"
  },
  {
    "objectID": "slides/week-9.2.html#modeling-1",
    "href": "slides/week-9.2.html#modeling-1",
    "title": "Modeling",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "slides/week-9.2.html#modeling-2",
    "href": "slides/week-9.2.html#modeling-2",
    "title": "Modeling",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "slides/week-9.2.html#pull-in-the-vdem-data",
    "href": "slides/week-9.2.html#pull-in-the-vdem-data",
    "title": "Modeling",
    "section": "Pull in the VDEM Data",
    "text": "Pull in the VDEM Data\n\nWhat is this code doing?\n\nlibrary(vdemdata)\n\nmodelData &lt;- vdem |&gt;\n  filter(year == 2019) |&gt; \n  select(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc) |&gt;\n  mutate(log_wealth = log(wealth))\n\nglimpse(modelData)\n\nRows: 179\nColumns: 4\n$ country    &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\", \"So…\n$ lib_dem    &lt;dbl&gt; 0.433, 0.593, 0.875, 0.870, 0.614, 0.601, 0.754, 0.267, 0.1…\n$ wealth     &lt;dbl&gt; 16.814, 11.752, 48.804, 56.110, 5.608, 11.345, 39.061, 5.69…\n$ log_wealth &lt;dbl&gt; 2.8222119, 2.4640234, 3.8878123, 4.0273140, 1.7241941, 2.42…"
  },
  {
    "objectID": "slides/week-9.2.html#plot-the-relationship",
    "href": "slides/week-9.2.html#plot-the-relationship",
    "title": "Modeling",
    "section": "Plot the Relationship",
    "text": "Plot the Relationship"
  },
  {
    "objectID": "slides/week-9.2.html#plot-the-relationship-1",
    "href": "slides/week-9.2.html#plot-the-relationship-1",
    "title": "Modeling",
    "section": "Plot the Relationship",
    "text": "Plot the Relationship"
  },
  {
    "objectID": "slides/week-9.2.html#plot-the-relationship-2",
    "href": "slides/week-9.2.html#plot-the-relationship-2",
    "title": "Modeling",
    "section": "Plot the Relationship",
    "text": "Plot the Relationship\n\n\nggplot(modelData, aes(x = wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  theme_bw()"
  },
  {
    "objectID": "slides/week-9.2.html#using-the-scales-package",
    "href": "slides/week-9.2.html#using-the-scales-package",
    "title": "Modeling",
    "section": "Using the Scales Package",
    "text": "Using the Scales Package"
  },
  {
    "objectID": "slides/week-9.2.html#using-the-scales-package-1",
    "href": "slides/week-9.2.html#using-the-scales-package-1",
    "title": "Modeling",
    "section": "Using the Scales Package",
    "text": "Using the Scales Package\n\n\nggplot(modelData, aes(x = wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  scale_x_log10(label = scales::label_dollar(suffix = \"k\")) +\n  labs(\n    title = \"Wealth and Democracy, 2019\",\n    x = \"GPD per capita\", \n    y = \"Liberal Democracy Index\") +\n  theme_bw()"
  },
  {
    "objectID": "slides/week-9.2.html#models-as-functions",
    "href": "slides/week-9.2.html#models-as-functions",
    "title": "Modeling",
    "section": "Models as Functions",
    "text": "Models as Functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs\n\nPlug in the inputs and receive back the output\n\nExample: The formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\).\n\nIf \\(x\\) is \\(5\\), \\(y\\) is \\(22\\),\n\\(y = 3 \\times 5 + 7 = 22\\)"
  },
  {
    "objectID": "slides/week-9.2.html#quant-lingo",
    "href": "slides/week-9.2.html#quant-lingo",
    "title": "Modeling",
    "section": "Quant Lingo",
    "text": "Quant Lingo\n\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis in the plot\n\nDependent variable\nOutcome variable\nY variable\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis in the plot\n\nIndependent variables\nPredictors"
  },
  {
    "objectID": "slides/week-9.2.html#section",
    "href": "slides/week-9.2.html#section",
    "title": "Modeling",
    "section": "",
    "text": "Linear model with one explanatory variable…\n\n\\(Y = a + bX\\)\n\\(Y\\) is the outcome variable\n\\(X\\) is the explanatory variable\n\\(a\\) is the intercept: the predicted value of \\(Y\\) when \\(X\\) is equal to 0\n\\(b\\) is the slope of the line [remember rise over run!]"
  },
  {
    "objectID": "slides/week-9.2.html#quant-lingo-1",
    "href": "slides/week-9.2.html#quant-lingo-1",
    "title": "Modeling",
    "section": "Quant Lingo",
    "text": "Quant Lingo\n\n\nPredicted value: Output of the model function\n\nThe model function gives the typical (expected) value of the response variable conditioning on the explanatory variables\nWe often call this \\(\\hat{Y}\\) to differentiate the predicted value from an observed value of Y in the data\n\nResiduals: A measure of how far each case is from its predicted value (based on a particular model)\n\nResidual = Observed value (\\(Y\\)) - Predicted value (\\(\\hat{Y}\\))\nHow far above/below the expected value each case is"
  },
  {
    "objectID": "slides/week-9.2.html#residuals",
    "href": "slides/week-9.2.html#residuals",
    "title": "Modeling",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/week-9.2.html#linear-model",
    "href": "slides/week-9.2.html#linear-model",
    "title": "Modeling",
    "section": "Linear Model",
    "text": "Linear Model\n\\(\\hat{Y} = a  + b \\times X\\)\n\\(\\hat{Y} = 0.13  + 0.12 \\times X\\)"
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation",
    "href": "slides/week-9.2.html#linear-model-interpretation",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\n\\(\\hat{Y} = a  + b \\times X\\)\n\\(\\hat{Y} = 0.13  + 0.12 \\times X\\)\nWhat is the interpretation of our estimate of \\(a\\)?\n\n\n\\(\\hat{Y} = 0.13  + 0.12 \\times 0\\)\n\\(\\hat{Y} = 0.13\\)\n\\(a\\) is our predicted level of democracy when GDP per capita is 0."
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation-1",
    "href": "slides/week-9.2.html#linear-model-interpretation-1",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\n\\(\\hat{Y} = a  + b \\times X\\)\n\\(\\hat{Y} = 0.13  + 0.12 \\times X\\)\nWhat is interpretation of our estimate of \\(b\\)?\n\n\n\\(\\hat{Y} = a  + \\frac{Rise}{Run} \\times X\\)\n\\(\\hat{Y} = a  + \\frac{Change Y}{Change X} \\times X\\)"
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation-2",
    "href": "slides/week-9.2.html#linear-model-interpretation-2",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\n\\(b = \\frac{Change Y}{Change X}\\)\n\\(0.12 = \\frac{Change Y}{Change X}\\)\n\\({Change Y} = 0.12 * {ChangeX}\\)\n\n\nWhen \\(ChangeX = 1\\):\n\\({Change Y = 0.12}\\)\n\n\n\n\\(b\\) is the predicted change in \\(Y\\) associated with a ONE unit change in X."
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation-3",
    "href": "slides/week-9.2.html#linear-model-interpretation-3",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation"
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation-4",
    "href": "slides/week-9.2.html#linear-model-interpretation-4",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation"
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation-5",
    "href": "slides/week-9.2.html#linear-model-interpretation-5",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation"
  },
  {
    "objectID": "slides/week-9.2.html#linear-model-interpretation-6",
    "href": "slides/week-9.2.html#linear-model-interpretation-6",
    "title": "Modeling",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\nIs this the causal effect of GDP per capita on liberal democracy?\n\n\nNo! It is only the association…\n\n\n\nTo identify causality we need other methods (beyond the scope of this course)."
  },
  {
    "objectID": "slides/week-9.2.html#your-task",
    "href": "slides/week-9.2.html#your-task",
    "title": "Modeling",
    "section": "Your Task",
    "text": "Your Task\n\nAn economist is interested in the relationship between years of education and hourly wages. They estimate a linear model with estimates of \\(a\\) and \\(b\\) as follows:\n\n\\(\\hat{Y} = 9 + 1.60*{YrsEdu}\\)\n\n1. Interpret \\(a\\) and \\(b\\)\n2. What is the predicted hourly wage for those with 10 years of education?"
  },
  {
    "objectID": "slides/week-9.2.html#next-step",
    "href": "slides/week-9.2.html#next-step",
    "title": "Modeling",
    "section": "Next step",
    "text": "Next step\n\n\nLinear model with one predictor: \\(Y = a + bX\\)\nFor any given data…\nHow do we figure out what the best values are for \\(a\\) and \\(b\\)??"
  },
  {
    "objectID": "project/project-description.html",
    "href": "project/project-description.html",
    "title": "Final Project",
    "section": "",
    "text": "Your goal in this project is to apply the skills and tools we have learned this semester to answer an International Affairs related question that is of interest to you.\nYou are free to select your own question and your own dataset. This is your chance to practice the skills we are using, apply them to a question of interest to you, and to work on generating conclusions from data that can inform policy decision-making in International Affairs.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#overview",
    "href": "project/project-description.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "Your goal in this project is to apply the skills and tools we have learned this semester to answer an International Affairs related question that is of interest to you.\nYou are free to select your own question and your own dataset. This is your chance to practice the skills we are using, apply them to a question of interest to you, and to work on generating conclusions from data that can inform policy decision-making in International Affairs.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#timeline",
    "href": "project/project-description.html#timeline",
    "title": "Final Project",
    "section": "Timeline",
    "text": "Timeline\n\nIn class on April 23, please come to class ready to discuss your research question and dataset (details below). This will be an opportunity for you to make sure you are on the right track with the question you have selected and the data you are working with.\nOn April 23, we will also discuss some advanced formatting techniques and how to post your project to Quarto Pub.\nThe project is due on May 10 by 11:59 p.m., however you are free to submit the project anytime before then.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#group-or-individual-work",
    "href": "project/project-description.html#group-or-individual-work",
    "title": "Final Project",
    "section": "Group or Individual Work",
    "text": "Group or Individual Work\n\nYou may work in groups of up to 3 members. If you work in a group, you must also submit a paragraph indicating how each of the group members participated and what aspects of the project they contributed to. Our expectation is that all group members will actively contribute.\nYou may also choose to work on your own.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#select-a-dataset",
    "href": "project/project-description.html#select-a-dataset",
    "title": "Final Project",
    "section": "Select a Dataset",
    "text": "Select a Dataset\n\nIn this project, you will select your own dataset for analysis. This should be a dataset that is relevant to your interests.\n\nA list of possible datasets is provided here\nYou may select one of these, or find another data source on your own.\nIf you have trouble reading a dataset into R, please let us know.\n\nIf you are not sure what to do, it is completely fine for you to use the vdem data for this project. You can also extend on your analysis from the GoBifo program that we analyzed for Homework 2.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#identify-a-research-question",
    "href": "project/project-description.html#identify-a-research-question",
    "title": "Final Project",
    "section": "Identify a Research Question",
    "text": "Identify a Research Question\n\nSelect a research question that you can answer with your data.\nThe question can be policy-related, or related to social science arguments.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#components-of-your-project",
    "href": "project/project-description.html#components-of-your-project",
    "title": "Final Project",
    "section": "Components of your Project",
    "text": "Components of your Project\nEach project should include the following:\n\nA brief introduction to your research question and why it is important. Your introduction should cite relevant literature that helps to motivate your research.\nA description of the data you are using, including the source of the data and the variables you are using.\nA description of the outcome variable you are trying to explain and why it is a good measure of the concept that you want to study.\nIdentify the explanatory variables that you will use to explain the outcome variable and discuss why they are relevant to your explanation. Is there one variable that you think is the most important? If so, why? Briefly justify your choices by referencing relevant materials.\nA brief summary of the literature related to your topic A clear research question that you are trying to answer.\nA description of the data you are using, including the source of the data and the variables you are using.\nAt least one data visualization illustrating the distribution of one of your important variables, with appropriate interpretation. Usually, this would be the outcome variable you are trying to explain.\nAt least one other relevant visualization pertaining to your outcome variable (scatter plot, line chart, column chart) that tells us something substantive about that variable from the standpoint of your analysis.\nChoose one of the methods that we learned about in class to analyze the relationship between your outcome variable and your explanatory variables. This could be a linear regression, a logistic regression, or a hypothesis test involving bootstrapping methods.\nInclude a visualization (table or bar plot) that would illustrate whether your explanatory variables are statistically significant in explaining your outcome variable.\nWhen interpreting your results, you should include an accurate interpretation of the p-value and associated hypothesis test.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#policy-implications",
    "href": "project/project-description.html#policy-implications",
    "title": "Final Project",
    "section": "Policy Implications",
    "text": "Policy Implications\n\nEach project should accurately summarize the findings and discuss policy implications.\nYou should also have a clear answer to your question that is clearly communicated.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "project/project-description.html#product",
    "href": "project/project-description.html#product",
    "title": "Final Project",
    "section": "Product",
    "text": "Product\nYou have two choices for your final product:\n\nA 3-5 page single-spaced policy memo that motivates your question, introduces the data, presents your findings and highlights important policy implications (or implications for general knowledge about politics or international affairs). You should render and submit your policy memo in HTML format.\nA 3-5 minute recorded presentation on Zoom, Vimeo or some similar tool with a revealjs slide deck. The presentation should motivate your question, introduces the data, presents your findings and highlights important policy implications (or implications for general knowledge about politics or international affairs).\n\n\n\n\n\n\n\nSubmission Instructions\n\n\n\n\nUpload your document or presentation to Quarto Pub and submit the link to the Discord server.\nIf you are doing a presentation, submit the link to your presentation as well as the link to your slide deck.\nUpload your project folder with the related QMD file to blackboard as per the usual routine for the homework assignments.",
    "crumbs": [
      "Final Project",
      "Final Project"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Support",
    "section": "",
    "text": "Writing Center GW’s Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online here.\nAcademic Commons Academic Commons provides tutoring and other academic support resources to students in many courses. Students can schedule virtual one-on-one appointments or attend virtual drop-in sessions. Students may schedule an appointment, review the tutoring schedule, access other academic support resources, or obtain assistance here.",
    "crumbs": [
      "Course Information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Support",
    "section": "",
    "text": "Writing Center GW’s Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online here.\nAcademic Commons Academic Commons provides tutoring and other academic support resources to students in many courses. Students can schedule virtual one-on-one appointments or attend virtual drop-in sessions. Students may schedule an appointment, review the tutoring schedule, access other academic support resources, or obtain assistance here.",
    "crumbs": [
      "Course Information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#support-for-students-outside-the-classroom",
    "href": "course-support.html#support-for-students-outside-the-classroom",
    "title": "Support",
    "section": "Support for students outside the classroom",
    "text": "Support for students outside the classroom\nDisability Support Services (DSS) 202-994-8250 Any student who may need an accommodation based on the potential impact of a disability should contact Disability Support Services to establish eligibility and to coordinate reasonable accommodations.\nCounseling and Psychological Services 202-994-5300 GW’s Colonial Health Center offers counseling and psychological services, supporting mental health and personal development by collaborating directly with students to overcome challenges and difficulties that may interfere with academic, emotional, and personal success.\nGW aims to create a community that cares for each other.The CARE Team fosters this goal by creating a pathway through which students who may need additional support can be identified and referred to the most appropriate services. Through the CARE Team, students are given the support they need to persist and succeed at GW and beyond.\nSafety and Security:\n\nIn an emergency: call GWPD 202-994-6111 or 911.\nFor situation-specific actions: review the Emergency Response Handbook\nStay informed: safety.gwu.edu/stay-informed",
    "crumbs": [
      "Course Information",
      "Support"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "",
    "text": "Location: Phillips Hall, Rm. 209\n\nInstructor: Prof. Emmanuel Teitelbaum\nEmail: ejt@gwu.edu\nOffice Hours: Tuesdays, 3-5 p.m.\n\nCredit Hours: 3.0.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "",
    "text": "Location: Phillips Hall, Rm. 209\n\nInstructor: Prof. Emmanuel Teitelbaum\nEmail: ejt@gwu.edu\nOffice Hours: Tuesdays, 3-5 p.m.\n\nCredit Hours: 3.0.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#why-are-you-here-what-our-course-is-about",
    "href": "course-syllabus.html#why-are-you-here-what-our-course-is-about",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Why Are You Here: What Our Course Is About",
    "text": "Why Are You Here: What Our Course Is About\nThis course will focus on developing data analysis and data science skills that are relevant for international affairs practitioners. Students will develop coding, data visualization, data analysis, and data presentation skills that will allow them to use data to answer questions related to international affairs and to contribute to data driven policy and programmatic decision making.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-objectives-what-youll-be-able-to-do",
    "href": "course-syllabus.html#course-objectives-what-youll-be-able-to-do",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Course Objectives: What You’ll Be Able To Do",
    "text": "Course Objectives: What You’ll Be Able To Do\nCourse and module objectives are guides to gauge your skill and knowledge development. By the end of this course, you should be able to:\n\nAnalyze data sets using modern computational tools (using R).\nIdentify and access data sets, and apply data wrangling concepts to optimize data for analysis and modeling purposes.\nUse data visualization to summarize simple and complex relationships between and among variables in a data set.\nUse tools of statistical inference to test hypotheses\nDevelop and use models for prediction/forecasting, and for causal inference\nCreate professional and reproducible data analysis outputs (reports, websites, slides, and so on) using Quarto",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#aiming-for-success",
    "href": "course-syllabus.html#aiming-for-success",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Aiming For Success",
    "text": "Aiming For Success\nI care about your learning and also about this subject matter, and I am here to help you have a meaningful learning experience. I expect you to take ownership of your learning: you can get more out of the course by thoughtfully participating in discussions, actively taking notes on readings and lectures, and giving your best effort overall. I will hold you to the highest standards for academic honesty and integrity in your work. I will also encourage you to collaborate and learn from your peers through thoughtful and respectful discussion. I must highlight that communication is vital, so I hope you feel comfortable reaching out to me if you are struggling or have concerns or need accommodations beyond accessibility. We can determine strategies to set you up for success. Finally, I look forward to collaborating with you in this course to create a meaningful experience for everyone.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#policy-on-ai-tools",
    "href": "course-syllabus.html#policy-on-ai-tools",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Policy on AI Tools",
    "text": "Policy on AI Tools\nThe use of LLMs can be a valuable tool for students in this course, but it is important to use it ethically and appropriately. By following the guidelines and expectations outlined in this syllabus policy, students can maximize the benefits of an AI-integrated workflow while also demonstrating their own critical thinking, research, and programming skills.\nAs part of this course, students will have the option to use LLMs to assist in the writing of their R scripts for specific course assignments. LLMs can provide students with a starting point for their code but sometimes the code is incomplete or error-prone. It is important to recognize that AI is a tool and not a substitute for critical thinking, research, or programming skills.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Office Hours",
    "text": "Office Hours\nAnother way we can work toward your success in the course is through office/student hours. Please make an appointment to talk with me during this time. You can work with me to:\n\nClarify any questions about the syllabus or course content\nReview an assignment you’ve completed and have questions about\nStep through practice problems or questions\nGet study strategies\nDiscuss grades\n\nTo make the meeting more effective, you can:\n\nGather materials (assignments, notes, etc.) ready in advance\nBe ready to take notes during office hours\nAsk follow up questions if you need clarification\nConfirm any action plan at the close of the meeting\nFollowing through on any action plan",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAcademic\nNone\n\n\nTechnological\n\nConfiguration and software\nTo fully participate in our course, you will need regular access to broadband Internet access as well as other technology components. Please consult GW Online’s Technical Requirements and Support for details on recommended configurations and software available to you. You will need to use the following tools and platforms:\n\nRStudio: an IDE for generating data visualizations using the programming language, R.\nR\n\nBoth of these can be downloaded from https://posit.co/downloads/\nFor our course, you should be able to:\n\nAccess and use GW’s Blackboard system.\nUse your GW email for university-related communications per university policy.\nUse productivity software (e.g., Office 365, Google Suite) to collaborate with peers and submit assignments.\nUse web conferencing tools (e.g., Zoom, Webex) to collaborate with peers and me.\nUse a mobile device and/or computer to upload documents, images, and recordings.\nSeek technology help and tools by contacting GW Information Technology | (202)-994-4948 | ithelp@gwu.edu.\n\nIf you need assistance with technology tools we’ll use in this course, please visit the Technology Support link in the left navigation menu in our course on Blackboard.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#materials-youll-need",
    "href": "course-syllabus.html#materials-youll-need",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Materials You’ll Need",
    "text": "Materials You’ll Need\nYou will need to download the RStudio desktop application. You can find our weekly assigned readings through GW libraries. You must use your GW credentials to access these readings. Other course materials will be provided in our Blackboard course modules.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-credit-hour-policy",
    "href": "course-syllabus.html#course-credit-hour-policy",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Course Credit Hour Policy",
    "text": "Course Credit Hour Policy\n37.5 hours of work per semester is required for one credit hour. These hours will consist of 50 minutes of direct or guided interaction plus 100 minutes of independent learning per week during the course of a normal 15-week semester, which includes one week for exams.\n\nHow this applies to you\nUse the credit hour policy to plan and manage your workload and time spent on this course. You should expect to spend approximately 5 hours (300 minutes) per week on this course outside of class time.\nPlease contact me if you are having difficulty managing your workload, and we can discuss strategies to help you succeed in the course.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#how-you-will-learn-and-demonstrate-knowledge",
    "href": "course-syllabus.html#how-you-will-learn-and-demonstrate-knowledge",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "How You Will Learn and Demonstrate Knowledge",
    "text": "How You Will Learn and Demonstrate Knowledge\nMy aim is to provide you opportunities for active learning and skills development that help you meet course learning objectives and also grow in your knowledge of this field.\n\nInstruction\nI’ve designed the following instructional components to support your learning and growth in the course.\n\nClass Sessions: Class sessions will involve a mix of lecture and active learning activities that ask you to practice the skills we are learning in a given session. Please come prepared to actively participate in class.\nReadings: Each week you will be responsible for various reading assignments. The readings contain essential content for completing the course assignments.\n\n\n\nAssessment\nThe following assessments help you gauge and demonstrate your progress in the course and support you in meeting course learning objectives.\n\nLabs: In this course, you will have frequent opportunities to apply your knowledge via lab assignments\n3 Homework Assignments: You will develop various skills through three assignments that ask you to apply the data analysis tools we are learning in the course. Thoughtfully and thoroughly completing these assignments helps you meet course objectives.\nGroup Project: In your final project, you will work in teams to apply your new data analysis skills in an international affairs project. You are free to choose your own partners or to do the project solo.\n\nYou’ll find support for Blackboard and other tools used for course activities and assignments under the Technology Support link in the left navigation menu in our course on Blackboard.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#demonstrating-academic-integrity",
    "href": "course-syllabus.html#demonstrating-academic-integrity",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Demonstrating Academic Integrity",
    "text": "Demonstrating Academic Integrity\nAll of us in the course will comply with the GW Code of Academic Integrity. It states that “we, the Students, Faculty, Librarians, Staff, and Administration of The George Washington University, believing academic integrity to be central to the mission of the University, commit ourselves to promoting high standards for the integrity of academic work. Commitment to academic integrity upholds educational equity, development, and dissemination of meaningful knowledge, and mutual respect that our community values and nurtures. The George Washington University Code of Academic Integrity is established to further this commitment.”\nAcademic dishonesty is defined as cheating of any kind, including misrepresenting one’s own work, taking credit for the work of others without crediting them and without appropriate authorization, and the fabrication of information. For details and complete code, see the Code of Academic Integrity.Common examples of academic dishonesty include cheating, fabrication, plagiarism, falsification, forgery of University academic documents, and facilitating academic dishonesty by others. Learn more about avoiding these:\n\nGW guidance for students on academic integrity.\nPlagiarism: What is it and how to avoid it from GW Libraries.\nMaintaining academic honesty can be a challenging skill to learn. If you have questions about maintaining our course standards, please talk with me early on.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#assignments",
    "href": "course-syllabus.html#assignments",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Assignments",
    "text": "Assignments\n\nLabs (20%)\n3 Homework assignments (15% each = 45%)\nFinal Group Project (35%)\n\n\nLabs\nStudents will approximately seven labs. These exercises will be oriented towards strengthening the student’s ability to use the tools we will be learning in the course. The labs will help to ensure that you keep up with the material consistently throughout the semester.\n\n\nCoding Homework Assignments\nStudents will complete three coding homework assignments. These assignments are designed to provide students with the opportunity to apply their newly-acquired skills to an international affairs related question or problem.\n\n\nFinal Group Project\nEach student will complete a final project that will be developed throughout the semester. We will provide more detail and time for you to work with your group in class.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-grading",
    "href": "course-syllabus.html#course-grading",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Course Grading",
    "text": "Course Grading\nThe grading scale below maps your final point or percentage total to your final letter grade for the course.\n\n\n\nRange\nLetter Grade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n84-86\nB\n\n\n80-83\nB-\n\n\n77-79\nC+\n\n\n74-76\nC\n\n\n70-73\nC-\n\n\n&lt;=69\nF",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#late-work",
    "href": "course-syllabus.html#late-work",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Late Work",
    "text": "Late Work\nWe understand that sometimes emergencies arise that might prevent you from submitting work on time. If you think you might miss an assignment deadline, it is your responsibility to contact me via email. We know life happens sometimes and will be as accommodating as possible, but it is important that you communicate with me in advance. If we do not hear from you, I will deduct 1 percentage points from your grade for every 24 hours the assignment is late.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#incomplete-grades",
    "href": "course-syllabus.html#incomplete-grades",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Incomplete Grades",
    "text": "Incomplete Grades\nIncomplete grades may be given to graduate students only if for reasons beyond the student’s control (such as medical or family emergency) they are unable to complete the final work of the course. Faculty should not assign an Incomplete grade if not asked by the student.\nFor further information, please consult with your advisor.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-communication",
    "href": "course-syllabus.html#course-communication",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Course Communication",
    "text": "Course Communication\nI try to respond to emails within 24 hours except on weekends. It is usually preferred to talk to me in person, however, either after class or during office hours.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#netiquette",
    "href": "course-syllabus.html#netiquette",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Netiquette",
    "text": "Netiquette\nBehind every name there is a person.\nTo ensure safe and meaningful learning experiences for everyone, we all agree to:\n\nRemain professional, respectful, and courteous at all times on all platforms.\nKeep in mind this is a college class. Something that would be inappropriate in an in-person classroom is also inappropriate in an online classroom.\nWhen upset, we’ll wait a day or two prior to posting. Messages posted or emailed in anger are often regretted later.\nAsk one another for clarification if we find a communication offensive or difficult to understand.\nAvoid sweeping generalizations. Back up our stated opinions with facts and reliable sources.\nUnderstand that we may disagree and that exposure to other people’s opinions is part of the learning experience.\nJust as we would like our privacy respected, we will respect the privacy of other course participants and what they share.\n\nI (the instructor) reserve the right to delete any post or communication in our course that is deemed inappropriate without prior notification to the student. This includes anything containing language that is offensive, rude, profane, racist, or hateful. Items that are seriously off-topic or serve no purpose other than to vent frustration will also be removed.\nUsing outside communication apps\nI am aware that you and your peers might communicate using tools outside of GW’s Blackboard, my course website, our course Discord channel, or email systems. Rules of netiquette and appropriate communication extend to these tools as well as to Blackboard. If you see any tool being used inappropriately (i.e., any communication containing language that is offensive, rude, profane, racist, or hateful; uses that promote cheating of any kind), contact me as soon as possible to speak privately about it.\n(Adapted from Lake Superior Connect, Creative Commons Attribution 3.0)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#policies",
    "href": "course-syllabus.html#policies",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Policies",
    "text": "Policies\nTo make this a meaningful learning experience for everyone, please read and understand the following policies. All GW policies can be found on the GW Office of Ethics, Compliance, and Privacy site. All GW community members are responsible for adhering to and activating in accordance with all university policies. Please contact me if you have any questions.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accessibility-and-accommodations",
    "href": "course-syllabus.html#accessibility-and-accommodations",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Accessibility and Accommodations",
    "text": "Accessibility and Accommodations\n\nGW’s Disability Support Services\nIf you are a student with a disability, or think you may have a disability, you can let me know, and/or you can talk to GW’s Office of Disability Support Services (DSS). DSS works with both students with disabilities and instructors to identify reasonable accommodations. Contact the DSS office at (202) 994-8250, by email on dss@gwu.edu, or in-person in Rome Hall Suite 102 to establish eligibility and to coordinate reasonable accommodations. If you have already been approved for accommodations, please send me your accommodation letter and meet with me so we can develop an implementation plan together.\nHow are course technology tools accessible to everyone? To find out, access Technology Support Technology Tools Policies in the Blackboard course menu.\n\n\nAccommodations Beyond Disability\nEveryone has different needs for learning. If you don’t have a documented disability but feel that you would benefit from learning support for other reasons, please don’t hesitate to talk to me. If you have substantial non-academic obligations or other concerns (e.g., food insecurity, work, childcare, athletic commitments, language barriers, financial issues, technology access, commuting, etc.) that make learning difficult, please contact me. I’ll keep this information confidential, and together, we can brainstorm ways to meet your needs.\n\n\nOther Needs\nAny student who has difficulty affording groceries or accessing sufficient food to eat every day, or who lacks a safe and stable place to live, and believes this may affect their performance in the course, is urged to contact GW’s Office of Student Financial Assistance for support. Furthermore, please notify me if you are comfortable doing so. Some other resources to support you are found under the course menu item Student Resources and include support for academic achievement and personal well-being. (Adapted from Goldrick-Rab, 2017)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#counseling-and-psychological-services",
    "href": "course-syllabus.html#counseling-and-psychological-services",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Counseling and Psychological Services",
    "text": "Counseling and Psychological Services\nGW’s Health Center offers counseling and psychological services to GW students. Please note that staff is licensed to offer short term therapy to students in Washington, DC, Maryland, and Virginia. If you are living outside these regions, the office may be able to refer you elsewhere. Assistance and referrals 24 hours a day, 365 days a year and can be reached on (202) 994-5300.\nThe Center provides assistance and referral to address students’ personal, social, career, and study skills problems. Services for students include: crisis and emergency mental health consultations, confidential assessment, counseling services (individual and small group), and referrals.\nVirtual Workshops are open to any student regardless of geographic location. These can be exceptionally valuable and help you build essential skills and cope with common ongoing mental health concerns. Please contact the GW Health Center on (202) 994-5300 for more information.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#religious-observances",
    "href": "course-syllabus.html#religious-observances",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Religious Observances",
    "text": "Religious Observances\nAs members of the GW community, you have the right to observe religious holidays. University policy requires that students notify their instructors during the first week of the semester if they plan to be absent from class on days of religious observance. For further details, please consult the university policy on religious holiday observance.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#emergency-preparedness-and-response-procedures",
    "href": "course-syllabus.html#emergency-preparedness-and-response-procedures",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Emergency Preparedness and Response Procedures",
    "text": "Emergency Preparedness and Response Procedures\nThe University has asked all faculty to inform students of these procedures, prepared by the GW Office of Public Safety and Emergency Management in collaboration with the Office of the Executive Vice President for Academic Affairs.\n\nTo Report an Emergency or Suspicious Activity\nCall the University Police Department at 202-994-6111 (Foggy Bottom) or 202-242-6111 (Mount Vernon).\n\n\nShelter in Place – General Guidance\nAlthough it is unlikely that we will ever need to shelter in place, it is helpful to know what to do just in case. No matter where you are, the basic steps of shelter in place will generally remain the same.\n\nIf you are inside, stay where you are unless the building you are in is affected. If it is affected, you should evacuate. If you are outdoors, proceed into the closest building or follow instructions from emergency personnel on the scene.\nLocate an interior room to shelter inside. If possible, it should be above ground level and have the fewest number of windows. If sheltering in a room with windows, move away from the windows. If there is a large group of people inside a particular building, several rooms may be necessary.\nShut and lock all windows (for a tighter seal) and close exterior doors.\nTurn off air conditioners, heaters, and fans. Close vents to ventilation systems as you are able. (University staff will turn off ventilation systems as quickly as possible).\nMake a list of the people with you and ask someone to call the list in to UPD so they know where you are sheltering and who is with you. If only students are present, one of the students should call in the list.\nAwait further instructions. If possible, visit GW Campus Advisories for incident updates or call the GW Information Line 202-994-5050.\nMake yourself comfortable and look after one other. You will get word as soon as it is safe to come out.\n\n\n\nEvacuation\nAn evacuation will be considered if the building we are in is affected or we must move to a location of greater safety. We will always evacuate if the fire alarm sounds. In the event of an evacuation, please gather your personal belongings quickly (purse, keys, GWorld card, etc.) and proceed to the nearest exit. Every classroom has a map at the door designating both the shortest egress and an alternate egress. Anyone who is physically unable to walk down the stairs should wait in the stairwell, behind the closed doors. Firemen will check the stairwells upon entering the building. Once you have evacuated the building, proceed to our primary rendezvous location: the court yard area between the GW Hospital and Ross Hall. In the event that this location is unavailable, we will meet on the ground level of the Visitors Parking Garage (I Street entrance, at 22nd Street). From our rendezvous location, we will await instructions to re-enter the School.\n\n\nAlert DC\nAlert DC provides free notification by e-mail or text message during an emergency. Visit GW Campus Advisories for a link and instructions on how to sign up for alerts pertaining to GW. If you receive an Alert DC notification during class, you are encouraged to share the information immediately.\n\n\nGW Alert\nGW Alert provides popup notification to desktop and laptop computers during an emergency. In the event that we receive an alert to the computer in our classroom, we will follow the instructions given. You are also encouraged to download this application to your personal computer. Visit GW Campus Advisories to learn how.\n\n\nAdditional Information\nAdditional information about emergency preparedness and response at GW or the University’s operating status can be found on GW Campus Advisories or by calling the GW Information Line at 202-994-5050.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#key-dates",
    "href": "course-syllabus.html#key-dates",
    "title": "IAFF 6501: Quantitative Analysis for IA Practitioners",
    "section": "Key Dates",
    "text": "Key Dates\nPlease defer to the due dates listed on the course website. You can also view due dates in the gradebook and under each individual course assignment item in Blackboard Ultra.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "modules/module-9.1.html",
    "href": "modules/module-9.1.html",
    "title": "Module 9.1",
    "section": "",
    "text": "Statistical modeling is one of the most powerful tools in data science. We use models for two primary purposes: to explore relationships between variables and to make predictions. Sometimes we are interested in understanding causal relationships (Does oil wealth impact regime type?), while other times we focus on predictive accuracy (Where is violence most likely to happen during an election? Is this email spam?).\nIn this module, we will explore linear regression, one of the foundational techniques in statistical modeling. We will learn how to quantify relationships between variables using correlation coefficients, fit linear models to data, and interpret the results. Throughout, we will work with real data examining the relationship between a country’s wealth and its level of democracy.\nBy the end of this module, you will be able to calculate correlations, fit simple linear regression models, and interpret the coefficients in meaningful ways. You will also understand the crucial distinction between correlation and causation.",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#overview",
    "href": "modules/module-9.1.html#overview",
    "title": "Module 9.1",
    "section": "",
    "text": "Statistical modeling is one of the most powerful tools in data science. We use models for two primary purposes: to explore relationships between variables and to make predictions. Sometimes we are interested in understanding causal relationships (Does oil wealth impact regime type?), while other times we focus on predictive accuracy (Where is violence most likely to happen during an election? Is this email spam?).\nIn this module, we will explore linear regression, one of the foundational techniques in statistical modeling. We will learn how to quantify relationships between variables using correlation coefficients, fit linear models to data, and interpret the results. Throughout, we will work with real data examining the relationship between a country’s wealth and its level of democracy.\nBy the end of this module, you will be able to calculate correlations, fit simple linear regression models, and interpret the coefficients in meaningful ways. You will also understand the crucial distinction between correlation and causation.",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#understanding-relationships-between-variables",
    "href": "modules/module-9.1.html#understanding-relationships-between-variables",
    "title": "Module 9.1",
    "section": "Understanding Relationships Between Variables",
    "text": "Understanding Relationships Between Variables\nWhen we build statistical models, we need to distinguish between different types of variables. The response variable (also called the dependent variable, outcome variable, target, or Y variable) is what we are trying to understand or predict. The explanatory variables (also called independent variables, predictors, features, or X variables) are what we use to explain variation in the response.\n\n’&gt;}}\nFor example, if we want to understand what factors influence a country’s level of democracy, democracy would be our response variable. Potential explanatory variables might include GDP per capita, education levels, natural resource wealth, or historical factors.\nLet’s examine a real-world example using data on countries’ wealth and democratic institutions.\nExample: GDP per Capita and Democracy\nWe’ll use data from the Varieties of Democracy (V-Dem) project, which provides comprehensive measures of democratic institutions around the world.\n\nlibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(indicators = c(\"v2x_libdem\", \"e_gdppc\", \"v2cacamps\"),\n                       start_year = 2019, end_year = 2019) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps\n    ) |&gt;\n  filter(!is.na(lib_dem), !is.na(wealth))\n\nglimpse(model_data)\n\nRows: 174\nColumns: 7\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ country_text_id &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MMR\"…\n$ country_id      &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 2…\n$ year            &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, …\n$ lib_dem         &lt;dbl&gt; 0.434, 0.580, 0.871, 0.866, 0.615, 0.607, 0.755, 0.260…\n$ wealth          &lt;dbl&gt; 16.814, 11.752, 48.804, 56.110, 5.608, 11.345, 39.061,…\n$ polarization    &lt;dbl&gt; 1.643, -0.631, -1.784, -1.543, -0.615, 0.204, -1.340, …\n\n\nLet’s visualize the relationship between these two variables:\n\nggplot(model_data, aes(x = wealth, y = lib_dem)) +\n  geom_point(color = \"steelblue\") +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  scale_x_log10(labels = scales::label_dollar(suffix = \"k\")) +\n  labs(\n    title = \"Wealth and Democracy, 2019\",\n    x = \"GDP per capita (log scale)\", \n    y = \"Liberal Democracy Index\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere we use scale_x_log10() to transform the x-axis to a logarithmic scale. This is often useful when dealing with variables that are not normally distributed or that span several orders of magnitude. Later, when we analyze the relationship between GDP and other variables, we will take a log transformation of it (for the same reason). To do this we will call the log() function on the wealth variable in our model data, e.g. log(model_data$wealth).\n\n\nWhat do you notice about this relationship? There appears to be a positive association between wealth and democracy; countries with higher GDP per capita tend to have higher democracy scores. At the same time there are a number of outliers in the southwest corner of the plot. These represent wealthy countries that are not very democratic. A lot of these tend to be oil rich states like Saudi Arabia and the United Arab Emirates.",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#correlation-measuring-linear-relationships",
    "href": "modules/module-9.1.html#correlation-measuring-linear-relationships",
    "title": "Module 9.1",
    "section": "Correlation: Measuring Linear Relationships",
    "text": "Correlation: Measuring Linear Relationships\nBefore we fit a formal model, let’s quantify the strength of the linear relationship using a correlation coefficient. The correlation coefficient (often denoted as r) measures the strength and direction of a linear relationship between two quantitative variables.\nCorrelation coefficients range from -1 to +1:\n\n\nr = +1: Perfect positive linear relationship\n\nr = 0: No linear relationship\n\n\nr = -1: Perfect negative linear relationship\n\n|r| &gt; 0.7: Strong linear relationship\n\n0.3 &lt; |r| &lt; 0.7: Moderate linear relationship\n\n\n|r| &lt; 0.3: Weak linear relationship\n\nThere are several ways to calculate correlations in R. The base R function cor() provides a quick way:\n\n# Using base R - note we need to use log(wealth) to match our plot\ncor(log(model_data$wealth), model_data$lib_dem)\n\n[1] 0.5295884\n\n\nWe can also use tidyverse approaches for more complex analyses:\n\n# Using tidyverse approach with summarize\nmodel_data |&gt;\n  summarize(\n    correlation = cor(polarization, lib_dem),\n    correlation_rounded = round(correlation, 3)\n  )\n\n  correlation correlation_rounded\n1  -0.4267386              -0.427\n\n\nThe correlation of approximately 0.53 indicates a moderate positive linear relationship between log GDP per capita and democracy levels. The relationship would likely be stronger if it were not for the outliers we saw in the plot.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nCalculate the correlation between democracy and the polarization variable. How does it compare to the correlation using logged wealth?",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#linear-models-as-functions",
    "href": "modules/module-9.1.html#linear-models-as-functions",
    "title": "Module 9.1",
    "section": "Linear Models as Functions",
    "text": "Linear Models as Functions\n\n’&gt;}}\nWe can represent the relationship between variables using mathematical functions. A function describes the relationship between an output and one or more inputs - plug in the inputs and receive back the output.\nFor a simple linear model with one explanatory variable, we write:\n\\[Y = a + bX\\]\nWhere:\n\n\n\\(Y\\) is the response variable (outcome, target)\n\n\\(X\\) is the explanatory variable (predictor)\n\n\n\\(a\\) is the intercept (predicted value of Y when X = 0)\n\n\\(b\\) is the slope (change in Y for a one-unit change in X)\n\nRunning a Linear Model in R\nLet’s fit a linear model to our democracy and wealth data using the base R lm() function. Since we saw that the relationship looks more linear when we log-transform wealth, we’ll use that approach. We will then use the summary() function to display the model results.\n\n# Run the model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Display model results\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nOur fitted model is:\n\\[Democracy = 0.13 + 0.12 × log(wealth)\\]\nModel Interpretation\nLet’s break down what these coefficients mean:\nIntercept (a = 0.13): This is the predicted democracy level when log(wealth) = 0. Since log(wealth) = 0 when wealth = $1, this represents the predicted democracy score for a country with $1 GDP per capita - essentially a theoretical baseline.\nSlope (b = 0.12): This is the key coefficient for interpretation. For every one-unit increase in log(wealth), we predict democracy to increase by 0.12 points.\nWhen we use logarithmic transformations, interpretation requires special care. When we change from one wealth level to another, we need to calculate the difference in their logarithms and multiply by our slope coefficient.\nFor any percentage increase in GDP per capita, we multiply our slope (0.12) by the natural log of the multiplier. For example, a 10% increase means multiplying by 1.1, doubling means multiplying by 2, tripling means multiplying by 3, and so on:\n\nA 10% increase (e.g., from $10,000 to $11,000) increases the democracy score by 0.0114 points since \\(0.12 \\times \\ln(1.1) \\approx 0.12 \\times 0.0953 = 0.0114\\)\nDoubling GDP per capita (e.g., $10,000 → $20,000) increases the democracy score by \\(0.12 \\times \\ln(2) \\approx 0.12 \\times 0.693 = 0.083\\)\nTripling GDP per capita (e.g., $10,000 → $30,000) increases the democracy score by: \\(0.12 \\times \\ln(3) \\approx 0.12 \\times 1.099 = 0.132\\)\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse the model we just ran to make predictions for specific countries or wealth levels, e.g. $50,000 per capita, $100,000 per capita, etc.\nTry regressing democracy on polarization. Interpret the model coefficients for different levels of polarization. (Note that this variable is not logged.)",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#understanding-predicted-values-and-residuals",
    "href": "modules/module-9.1.html#understanding-predicted-values-and-residuals",
    "title": "Module 9.1",
    "section": "Understanding Predicted Values and Residuals",
    "text": "Understanding Predicted Values and Residuals\nEvery model produces predicted values - these are the outputs from our model function given specific input values (like the ones we calculate earlier for our wealth and democracy model). We often write these as Ŷ (Y-hat) to distinguish them from observed values.\nResiduals measure how far each observation is from its predicted value:\n\\[Residual = Observed Value (Y) - Predicted Value (\\hat{Y})\\]\nLet’s visualize this:\n\n\n\n\n\n\n\n\nCountries above the line have positive residuals (higher democracy than predicted by wealth alone), while countries below the line have negative residuals (lower democracy than predicted).",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#model-performance-and-limitations",
    "href": "modules/module-9.1.html#model-performance-and-limitations",
    "title": "Module 9.1",
    "section": "Model Performance and Limitations",
    "text": "Model Performance and Limitations\nHow well does our model perform? We can assess this using R-squared, which tells us what proportion of variation in democracy is explained by wealth. If we call summary again on our model, we can see this value:\n\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nThe R-squared value is approximately 0.28, meaning that about 28% of the variation in democracy scores can be explained by log GDP per capita. This is a reasonable result for a social science model, but it also indicates that there are many other factors influencing democracy that we have not accounted for. Approximately 72% of the variation remains unexplained by our model!\n\n\n\n\n\n\nCorrelation vs. Causation\n\n\n\nOur model shows a strong association between wealth and democracy, but this does not prove that wealth causes democracy. There are several possible explanations for this relationship:\n\n\nWealth → Democracy: Perhaps economic development creates conditions that support democratic institutions\n\nDemocracy → Wealth: Perhaps democratic institutions promote economic growth\n\n\nThird Variable: Perhaps other factors (education, culture, geography) influence both wealth and democracy\n\nReverse Causation: The relationship might work in both directions simultaneously\n\nEstablishing causation requires more sophisticated methods beyond simple correlation and regression, such as natural experiments, instrumental variables, or randomized controlled trials. In the context of linear regression, causal identification is beyond the scope of this course.",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-9.1.html#conclusion",
    "href": "modules/module-9.1.html#conclusion",
    "title": "Module 9.1",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression is a foundational tool in data science, but it’s just the beginning. In future modules, we’ll explore multiple regression (with several explanatory variables), non-linear relationships, and more sophisticated modeling techniques.",
    "crumbs": [
      "Course Modules",
      "Module 9.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html",
    "href": "modules/module-11.1.html",
    "title": "Module 11.1",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the peacesciencer package (install.packages(\"peacesciencer\"))\nHave a look at the peacesciencer documentation to familiarize yourself with its contents and basic functions",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#overview",
    "href": "modules/module-11.1.html#overview",
    "title": "Module 11.1",
    "section": "Overview",
    "text": "Overview\nSo far in our data science journey, we’ve focused on modeling continuous numerical outcomes like income, temperature, or test scores. But many of the most interesting questions in research involve outcomes that are binary: yes or no, success or failure, presence or absence.\nIn this module, we will explore why we need a different modeling approach when our outcome variable is binary rather than continuous. We will use the compelling example of civil war onset to understand the conceptual foundations that motivate logistic regression, setting the stage for the technical details that we will cover in upcoming modules. By the end of this module, you’ll be able to distinguish between continuous and binary outcome variables, understand why linear regression is inappropriate for binary outcomes, grasp the concept of Bernoulli trials in the context of real research, appreciate why probabilities must be constrained between 0 and 1, and recognize when a different modeling approach is needed.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#from-continuous-to-binary-outcomes",
    "href": "modules/module-11.1.html#from-continuous-to-binary-outcomes",
    "title": "Module 11.1",
    "section": "From Continuous to Binary Outcomes",
    "text": "From Continuous to Binary Outcomes\nThroughout our exploration of linear regression, we’ve been working with continuous numerical outcomes. These are variables that can theoretically take on any value within a range, like someone’s height (5.8 feet, 5.83 feet, 5.834 feet), annual income ($45,000, $45,231, $45,231.67), or a test score that could be anywhere from 0 to 100.\nBut many research questions center on outcomes that are fundamentally different: binary outcomes. These variables have exactly two possible values, often coded as yes/no, success/failure, present/absent, or simply 1/0. Consider research questions like whether a patient recovers from treatment, whether a voter turns out for an election, whether an email gets classified as spam, or whether a student graduates within four years. Each of these involves a binary outcome where there are only two possibilities for each observation.\nLet’s examine an example from political science research that perfectly illustrates why we need different modeling approaches for binary outcomes. The research question is straightforward: did a civil war begin in a given country in a given year?\nThis question was central to groundbreaking research by Fearon and Laitin (2003), who wanted to understand what factors make civil war more or less likely to begin. For any country in any year, there are only two possibilities: either a civil war began (coded as 1, or “success” in statistical terms) or no civil war began (coded as 0, or “failure”). Note that calling conflict onset a “success” might feel strange since we’re certainly not celebrating war! In statistics, “success” simply refers to the outcome we’re modeling, regardless of whether it’s socially desirable.\nResearchers hypothesized that various factors might influence the probability of conflict onset, including economic wealth (GDP per capita), level of democracy, mountainous terrain (which might facilitate insurgency), ethnic diversity, population size, and previous conflict history. The key insight is that we want to model how these factors influence the probability that conflict will begin, not predict an exact numerical outcome.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nThink about research questions in your field of interest. For each scenario below, identify whether the outcome variable is continuous or binary:\n\nPredicting a student’s final exam score based on study hours\nDetermining whether a loan application will be approved\nForecasting tomorrow’s temperature\nClassifying whether a tumor is malignant or benign\nEstimating household spending on groceries\nPredicting whether a new product launch will be successful\nModeling changes in unemployment rate over time\n\nAfter categorizing each one, think about: - What makes binary outcomes fundamentally different from continuous ones? - Can you think of examples from your own research interests or career field?",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#the-problem-with-linear-regression-for-binary-data",
    "href": "modules/module-11.1.html#the-problem-with-linear-regression-for-binary-data",
    "title": "Module 11.1",
    "section": "The Problem with Linear Regression for Binary Data",
    "text": "The Problem with Linear Regression for Binary Data\nNow comes the crucial question: why can’t we just use regular linear regression for binary outcomes?\nThink about what linear regression does. It creates a straight line that predicts numerical values based on our predictors. For our conflict example, a linear model might look like:\n\\[\\text{Conflict Onset} = \\beta_0 + \\beta_1(\\text{GDP per capita}) + \\beta_2(\\text{Democracy level}) + \\beta_3(\\text{Terrain roughness}) + \\ldots\\]\nBut there is a fundamental problem here. Linear regression can predict any value along a continuous range. It might predict that a country has a “-0.3” probability of conflict onset, or a “1.7” probability.\nWhat does it mean for something to have a 170% chance of happening? Or a negative probability? These predictions are mathematically nonsensical because probabilities must be constrained between 0 and 1 (or 0% and 100%).\nAnother problem with applying linear regression to a binary outcome is that it can shift the decision boundary in unstable ways. Linear regression implicitly uses the point where the predicted value crosses 0.5 as the cutoff for classifying an observation as “success” or “failure.”\nLet’s assume one predictor has an extreme value — for example, Nepal’s very mountainous terrain — this can pull the fitted line up or down, shifting the decision boundary along the predictor axis. As a result, cases that should be classified as high risk for conflict might now be on the wrong side of the boundary and be misclassified as low risk. Logistic regression addresses this problem by directly modeling the probability and keeping the decision boundary consistent and interpretable.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#bernoulli-trials-and-probability-modeling",
    "href": "modules/module-11.1.html#bernoulli-trials-and-probability-modeling",
    "title": "Module 11.1",
    "section": "Bernoulli Trials and Probability Modeling",
    "text": "Bernoulli Trials and Probability Modeling\nTo properly model binary outcomes, we should think about each observation as a Bernoulli trial. A Bernoulli trial is a random experiment with exactly two possible outcomes (success and failure), where the probability of success remains constant for that specific trial. A classic example is a fair coin flip: heads (success) or tails (failure), with a fixed 50% chance of success.\nIn our conflict onset example, each country-year combination represents a separate Bernoulli trial:\n\nAfghanistan in 2001: One trial with its own probability of conflict onset\nSwitzerland in 2001: A different trial with its own (likely much lower) probability\n\nAfghanistan in 2002: Yet another trial with its own probability\n\nThe key insight is that while each trial has only two possible outcomes, the probability of “success” can vary between trials based on the specific circumstances (the predictor variables) of that country in that year.\nMathematically, we can express this as:\n\\[y_i \\sim \\text{Bernoulli}(p_i)\\]\nThis notation means that each outcome \\(y_i\\) follows a Bernoulli distribution with its own probability \\(p_i\\).\n\n\n\n\n\n\nYour Turn!!\n\n\n\nLet’s deepen our understanding of how the conflict onset example works as Bernoulli trials:\nPart A: Different Trials\nConsider these three scenarios: 1. Afghanistan in 2000 (before major international intervention) 2. Afghanistan in 2010 (during NATO presence)\n3. Denmark in 2010\nWhy might each of these represent different Bernoulli trials with different probabilities of conflict onset? What factors might make the probability higher or lower in each case?\nPart B: Same Trial, Different Factors\nFor a single country-year (say, Syria in 2010), brainstorm factors that might influence the probability of conflict onset. How might these factors work together to increase or decrease the overall probability?",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#generalized-linear-models",
    "href": "modules/module-11.1.html#generalized-linear-models",
    "title": "Module 11.1",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nOur exploration reveals that we need a modeling framework that can handle binary outcome variables appropriately, ensure predicted probabilities stay between 0 and 1, allow different observations to have different success probabilities, and incorporate multiple predictor variables in a systematic way.\nThis is where Generalized Linear Models (GLMs) come in. GLMs provide a suitable framework for extending regression concepts beyond continuous outcomes. Rather than forcing binary data into an inappropriate linear framework, GLMs use mathematical transformations that naturally respect the constraints of probability.\nAll GLMs share three key characteristics:\n\nA probability distribution that describes how the outcome variable is generated (for binary outcomes, this is the Bernoulli distribution)\n\nA linear model that combines predictor variables:\n\\[\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k\\]\n\nA link function that connects the linear model to the parameter of the outcome distribution (ensuring probabilities stay between 0 and 1)\n\nA link function is a mathematical function that connects the linear model (which can range from −∞−∞ to +∞+∞) to the parameter of the outcome distribution (such as the probability of success) in a way that respects its natural constraints. For binary outcomes, the link function ensures that the linear combination of predictors maps to a probability between 0 and 1.\nLogistic regression is one of the most common and useful examples of a GLM. It uses a special mathematical transformation (the logistic function) that takes any real number from the linear model and converts it to a probability between 0 and 1.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#summary-and-looking-ahead",
    "href": "modules/module-11.1.html#summary-and-looking-ahead",
    "title": "Module 11.1",
    "section": "Summary and Looking Ahead",
    "text": "Summary and Looking Ahead\nIn this module, we have explored why binary outcomes require a different modeling approach than continuous variables. We saw that many important research questions involve binary rather than continuous outcomes, and that linear regression fails for binary data because it can produce impossible probability predictions. We learned to think of each observation as a Bernoulli trial with its own success probability and to recognize that we need a modeling framework that respects probability constraints while incorporating multiple predictors.\nLogistic regression, as part of the GLM family, provides exactly this framework. It allows us to model how various factors influence the probability of binary outcomes while ensuring our predictions remain mathematically sensible. In our next modules, we will dive into the mathematical details of how logistic regression works, learn to fit and interpret these models, and practice making predictions with real data.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#the-logistic-regression-model",
    "href": "modules/module-11.1.html#the-logistic-regression-model",
    "title": "Module 11.1",
    "section": "The Logistic Regression Model",
    "text": "The Logistic Regression Model",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#the-sigmoid-function-the-key-to-valid-probabilities",
    "href": "modules/module-11.1.html#the-sigmoid-function-the-key-to-valid-probabilities",
    "title": "Module 11.1",
    "section": "The Sigmoid Function: The Key to Valid Probabilities",
    "text": "The Sigmoid Function: The Key to Valid Probabilities\nRemember our fundamental problem from Module 5.1: linear regression can predict impossible probabilities like -0.3 or 1.7 when we apply it to binary outcomes. We need a mathematical function that can take any real number (from negative infinity to positive infinity) and transform it into a valid probability between 0 and 1.\nEnter the sigmoid function (also called the logistic function). The sigmoid function is defined as:\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nwhere \\(z\\) can be any real number, and \\(\\sigma(z)\\) will always be between 0 and 1.\nLet’s visualize what this function looks like:\n\n\n\n\n\n\n\n\nThe sigmoid function has several important properties that make it perfect for our needs. Most importantly, it always produces outputs between 0 and 1, no matter what value we put in for \\(z\\). The function has a characteristic S-shaped curve that rises slowly at first, then more rapidly in the middle, then slowly again as it approaches its limits. It’s symmetric around 0.5, meaning that when \\(z = 0\\), we get \\(\\sigma(z) = 0.5\\). Unlike a step function that would create abrupt jumps, the sigmoid provides smooth probability transitions as our predictors change.\nThis is exactly what we need for binary classification! The sigmoid function takes our linear combination of predictors (which can be any value) and converts it to a probability.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#the-logit-function-the-other-side-of-the-equation",
    "href": "modules/module-11.1.html#the-logit-function-the-other-side-of-the-equation",
    "title": "Module 11.1",
    "section": "The Logit Function: The Other Side of the Equation",
    "text": "The Logit Function: The Other Side of the Equation\nWhile the sigmoid function shows us how to convert linear predictors to probabilities, we actually need to set up our model the other way around. Remember that in regression, we want to model something as a linear function of our predictors. But we can’t model probabilities directly as linear functions because probabilities are constrained between 0 and 1, while linear functions can produce any value from negative infinity to positive infinity.\nThis is where we need to “go the other direction” - we need a function that takes probabilities and transforms them onto an unrestricted scale where we can model them linearly. The logit function does exactly this transformation:\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\nThe logit function takes a probability (between 0 and 1) and transforms it to any real number (between negative and positive infinity). Let’s visualize this:\n\n\n\n\n\n\n\n\nThe term \\(\\frac{p}{1-p}\\) in the logit function is called the odds. When \\(p = 0.5\\) (equal chance of success and failure), the odds equal 1, and \\(\\log(1) = 0\\). When \\(p &gt; 0.5\\), the odds are greater than 1, and the logit is positive. When \\(p &lt; 0.5\\), the odds are less than 1, and the logit is negative.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#the-complete-logistic-regression-model",
    "href": "modules/module-11.1.html#the-complete-logistic-regression-model",
    "title": "Module 11.1",
    "section": "The Complete Logistic Regression Model",
    "text": "The Complete Logistic Regression Model\nNow we can put together the complete picture of how logistic regression works. Remember from Module 5.1 that all GLMs have three components:\n\n\nDistribution: \\(y_i \\sim \\text{Bernoulli}(p_i)\\) (each observation is a Bernoulli trial)\n\nLinear predictor: \\(\\eta_i = \\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}\\) (familiar linear combination)\n\nLink function: \\(\\text{logit}(p_i) = \\eta_i\\) (connects the linear predictor to the probability)\n\nPutting it all together:\n\\[\\text{logit}(p_i) = \\eta_i = \\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}\\]\nTo get back to probabilities, we apply the sigmoid function:\n\\[p_i = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-(\\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}}\\]\nOr an equivalent form that is often used in practice:\n\\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\]\nThe key insight is that we model the logit of the probability as a linear function of our predictors, then use the sigmoid function to convert back to probabilities that make sense. The logit gets us from constrained probabilities to an unrestricted scale where we can do linear modeling, and the sigmoid gets us back from that unrestricted scale to meaningful probabilities.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#worked-example-logistic-regression-with-conflict-onset-data",
    "href": "modules/module-11.1.html#worked-example-logistic-regression-with-conflict-onset-data",
    "title": "Module 11.1",
    "section": "Worked Example: Logistic Regression with Conflict Onset Data",
    "text": "Worked Example: Logistic Regression with Conflict Onset Data\nLet’s implement logistic regression using our conflict onset example. We can use data from the peacesciencer package. The create_stateyears() function will help us set up a dataset where each row represents one country in one year, and our binary outcome variable will indicate whether a civil war began in that country in that year. Then we add different sets of predictors using peacesciencer “add” functions like add_ucdp_acd(), add_democracy(), and others to create a rich dataset for our analysis.\n\nlibrary(peacesciencer)\nlibrary(dplyr)\n\n# Create the conflict dataset\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()\n\n# Take a look at our data\nglimpse(conflict_df)\n\nRows: 7,624\nColumns: 22\n$ gwcode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ gw_name        &lt;chr&gt; \"United States of America\", \"United States of America\",…\n$ microstate     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ year           &lt;dbl&gt; 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1…\n$ ucdpongoing    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ucdponset      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ maxintensity   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ conflict_ids   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ euds           &lt;dbl&gt; 1.293985, 1.308359, 1.343539, 1.330836, 1.354015, 1.350…\n$ aeuds          &lt;dbl&gt; 0.4862558, 0.5006298, 0.5358093, 0.5231064, 0.5462858, …\n$ polity2        &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…\n$ v2x_polyarchy  &lt;dbl&gt; 0.603, 0.607, 0.599, 0.580, 0.585, 0.611, 0.611, 0.612,…\n$ ethfrac        &lt;dbl&gt; 0.2226323, 0.2248701, 0.2271561, 0.2294918, 0.2318781, …\n$ ethpol         &lt;dbl&gt; 0.4152487, 0.4186156, 0.4220368, 0.4255134, 0.4290458, …\n$ relfrac        &lt;dbl&gt; 0.4980802, 0.5009111, 0.5037278, 0.5065309, 0.5093204, …\n$ relpol         &lt;dbl&gt; 0.7769888, 0.7770017, 0.7770303, 0.7770729, 0.7771274, …\n$ wbgdp2011est   &lt;dbl&gt; 28.539, 28.519, 28.545, 28.534, 28.572, 28.635, 28.669,…\n$ wbpopest       &lt;dbl&gt; 18.744, 18.756, 18.781, 18.804, 18.821, 18.832, 18.848,…\n$ sdpest         &lt;dbl&gt; 28.478, 28.456, 28.483, 28.469, 28.510, 28.576, 28.611,…\n$ wbgdppc2011est &lt;dbl&gt; 9.794, 9.762, 9.764, 9.730, 9.752, 9.803, 9.821, 9.857,…\n$ rugged         &lt;dbl&gt; 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073,…\n$ newlmtnest     &lt;dbl&gt; 3.214868, 3.214868, 3.214868, 3.214868, 3.214868, 3.214…\n\n# Check our binary outcome variable\ntable(conflict_df$ucdponset, useNA = \"always\")\n\n\n   0    1 &lt;NA&gt; \n7481  143    0 \n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nIn the last line, we use the table() function to summarize our binary outcome variable ucdponset, which indicates whether a civil war began in that country in that year. The useNA = \"always\" argument ensures we also see how many observations have missing values. this helps us understand the distribution of our outcome variable, how many observations we have in total and whether we have any missing data that we need to be concerned with before running our regressions.\n\n\nTake a moment to examine this output. Consider how many observations we have and what our binary outcome variable is called. Notice the distribution of 1s versus 0s in the outcome variable. Most importantly, think about what each row represents in terms of Bernoulli trials.\nRemember, each row represents one country in one year, and we’re asking: “Did a civil war begin in this country in this year?” Each observation is a separate Bernoulli trial with its own probability of “success” (conflict onset) based on that country’s characteristics in that year.\nRunning a Logistic Regression in R\nThe good news is that implementing logistic regression in R is very similar to linear regression. We just need to make a few changes to tell R that we are working with a binary outcome. Instead of using\nlm(continuous_outcome ~ predictor1 + predictor2, data = mydata)\nas we did for linear regression, we now use\nglm(binary_outcome ~ predictor1 + predictor2, data = mydata, family = \"binomial\")\nfor logistic regression.\nThe key changes are switching from lm() to glm() and adding the family = \"binomial\" argument to specify we’re working with binary data. This family specification tells R to use the logit link function automatically, handling all the mathematical transformations we discussed.\nLet’s start with a simple bivariate example, examining how GDP per capita relates to conflict onset:\n\n# Fit a logistic regression model\nconflict_model &lt;- glm(ucdponset ~ wbgdppc2011est,\n                      data = conflict_df,\n                      family = \"binomial\")\n\n# Look at the summary\nsummary(conflict_model)\n\n\nCall:\nglm(formula = ucdponset ~ wbgdppc2011est, family = \"binomial\", \n    data = conflict_df)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -1.00735    0.41297  -2.439   0.0147 *  \nwbgdppc2011est -0.35695    0.05089  -7.015 2.31e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1420.5  on 7623  degrees of freedom\nResidual deviance: 1381.9  on 7622  degrees of freedom\nAIC: 1385.9\n\nNumber of Fisher Scoring iterations: 7\n\n\nThe output looks similar to linear regression, but the interpretation is different because we’re modeling the log-odds (logit) rather than the outcome directly.\nUnderstanding the Model Output\nWhen you look at the summary output from a logistic regression, remember that the coefficients are on the log-odds scale. A coefficient of 0.5 means a one-unit increase in that predictor increases the log-odds by 0.5. The intercept represents the log-odds of the outcome when all predictors equal zero. Standard errors and p-values are interpreted similarly to linear regression for hypothesis testing, but you won’t see an R-squared value since we don’t use that measure of fit with logistic regression.\nThe coefficients tell us about direction and significance, but interpreting the magnitude on the log-odds scale can be challenging. This is why we typically transform these results into more intuitive measures when making practical interpretations.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nNow it’s your turn to fit a logistic regression model. Use the conflict_df dataset we created above and run a logistic regression model with ucdponset as the outcome variable. Choose at least one predictor variable from the dataset, such as v2x_polyarchy (the V-Dem measure of democracy), and fit the model using glm().\nAs you examine your results, consider whether the coefficient for your chosen variable is positive or negative and what this suggests about the relationship between that predictor and conflict onset. Remember that you are looking at effects on the log-odds scale. We will learn to interpret this more meaningfully in the next module!",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-11.1.html#summary-and-looking-ahead-1",
    "href": "modules/module-11.1.html#summary-and-looking-ahead-1",
    "title": "Module 11.1",
    "section": "Summary and Looking Ahead",
    "text": "Summary and Looking Ahead\nIn this module, we’ve explored the mathematical foundations that make logistic regression work. The sigmoid function provides the crucial capability to transform any real number into a valid probability between 0 and 1, while the logit function allows us to transform probabilities into an unrestricted scale that we can model linearly. Together, these functions enable the complete GLM framework that connects our linear predictors to binary outcomes through the logit link function. Implementing this approach in R requires only small changes from linear regression, using glm() with family = \"binomial\" instead of our familiar lm() function.\nIn our next module, we’ll learn how to interpret these log-odds coefficients in more meaningful ways through odds ratios and predicted probabilities. We will discover how to answer questions like “How much does democracy change the probability of conflict onset?” and “What’s the predicted probability of conflict for a specific country profile?” Thanfully, the mathematical foundation that we laid today will make those interpretations much clearer and more intuitive.",
    "crumbs": [
      "Course Modules",
      "Module 11.1"
    ]
  },
  {
    "objectID": "modules/module-12.2.html",
    "href": "modules/module-12.2.html",
    "title": "Module 12.2",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the modelsummary package(install.packages(\"modelsummary\")) and look over the documentation\n\nHave a quick look at the marginaleffects documentation for predictions and effects\nInstall the gt package (install.packages(\"gt\")) for creating professional tables (we will use this as a dependecy of modelsummary))",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#overview",
    "href": "modules/module-12.2.html#overview",
    "title": "Module 12.2",
    "section": "Overview",
    "text": "Overview\nThroughout this series, we’ve built a comprehensive understanding of logistic regression, from conceptual foundations through complex interaction effects. But understanding our results is only half the battle because we also need to communicate them effectively to diverse audiences. Whether presenting to academic peers, policymakers, or the general public, the way we visualize and present our findings can make the difference between impact and obscurity.\nThis module brings together all the analytical skills that you have developed and focuses on professional presentation of logistic regression results. We will master the creation of publication-ready regression tables that clearly communicate model specifications and key findings. We will learn to create compelling coefficient plots that make effect sizes and uncertainty immediately apparent. We will discover how to visualize predicted probabilities and marginal effects in ways that make complex relationships accessible to non-technical audiences. Finally, we will develop skills for writing up results that appropriately match technical depth to audience needs.\nBy the end of this module, you’ll be able to create professional regression tables using modelsummary, design effective coefficient plots that highlight key findings, visualize complex predicted probabilities and marginal effects, and write clear, audience-appropriate interpretations of logistic regression results. We’ll continue with our Fearon and Laitin replication to maintain continuity, but the visualization and communication skills you learn will apply broadly across research contexts.",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#professional-regression-tables-with-modelsummary",
    "href": "modules/module-12.2.html#professional-regression-tables-with-modelsummary",
    "title": "Module 12.2",
    "section": "Professional Regression Tables with modelsummary",
    "text": "Professional Regression Tables with modelsummary\n\nWhen presenting logistic regression results to academic audiences, well-formatted regression tables remain the gold standard. The modelsummary package excels at creating publication-ready tables that can accommodate multiple models while allowing extensive customization of variable names, significance indicators, and goodness-of-fit statistics.\nLet’s begin by recreating our conflict onset models from previous modules, then demonstrate how to present them professionally:\n\nlibrary(peacesciencer)\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(broom)\n\n# Recreate the conflict dataset\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain() |&gt;\n  mutate(democracy = ifelse(v2x_polyarchy &gt; 0.5, 1, 0)) |&gt;\n  select(-ucdpongoing, -maxintensity, -conflict_ids, -sdpest) |&gt;\n  drop_na()\n\n# Fit our key models from previous modules\nbaseline_model &lt;- glm(ucdponset ~ v2x_polyarchy + newlmtnest + wbpopest + \n                     wbgdppc2011est + ethfrac + relfrac,\n                     data = conflict_df, family = \"binomial\")\n\nterrain_interaction &lt;- glm(ucdponset ~ democracy * newlmtnest + wbpopest + \n                          wbgdppc2011est + ethfrac + relfrac,\n                          data = conflict_df, family = \"binomial\")\n\nwealth_interaction &lt;- glm(ucdponset ~ v2x_polyarchy * wbgdppc2011est + newlmtnest + \n                         wbpopest + ethfrac + relfrac,\n                         data = conflict_df, family = \"binomial\")\n\nNow we’ll create a professional table that compares these models side by side. The key to effective table presentation lies in thoughtful customization:\n\n# Create list of models with descriptive names\nmodels_list &lt;- list(\n  \"Baseline\" = baseline_model,\n  \"Democracy × Terrain\" = terrain_interaction,\n  \"Democracy × Wealth\" = wealth_interaction\n  )\n\n# Create coefficient mapping for clean variable names\ncoef_mapping &lt;- c(\n  \"v2x_polyarchy\" = \"Democracy (Polyarchy)\",\n  \"democracy\" = \"Democracy (Binary)\",\n  \"newlmtnest\" = \"Mountainous Terrain\",\n  \"wbgdppc2011est\" = \"GDP per Capita (log)\",\n  \"wbpopest\" = \"Population (log)\",\n  \"ethfrac\" = \"Ethnic Fractionalization\",\n  \"relfrac\" = \"Religious Fractionalization\",\n  \"democracy:newlmtnest\" = \"Democracy × Terrain\",\n  \"v2x_polyarchy:wbgdppc2011est\" = \"Democracy × GDP\",\n  \"(Intercept)\" = \"Constant\"\n  )\n\n# Create notes for the table\n  notes = c(\n    \"Dependent variable: Binary indicator of civil war onset.\",\n    \"Standard errors in parentheses. * p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01.\",\n    \"Data sources: UCDP/PRIO Armed Conflict Dataset, World Bank, V-Dem.\"\n  )\n\n# Create the table\nmodelsummary(\n  models_list,\n  coef_map = coef_mapping,\n  stars = TRUE,\n  gof_map = c(\"nobs\", \"aic\"),\n  title = \"Table 1: Logistic Regression Models of Civil War Onset, 1946-1999\",\n  notes = notes,\n  output = \"gt\"\n  )\n\n\n\n\nTable 1: Logistic Regression Models of Civil War Onset, 1946-1999\n\n\nBaseline\nDemocracy × Terrain\nDemocracy × Wealth\n\n\n\nDemocracy (Polyarchy)\n-0.593\n\n8.928*\n\n\n\n(0.533)\n\n(4.098)\n\n\nDemocracy (Binary)\n\n-0.105\n\n\n\n\n\n(0.526)\n\n\n\nMountainous Terrain\n0.145*\n0.190*\n0.177*\n\n\n\n(0.072)\n(0.077)\n(0.074)\n\n\nGDP per Capita (log)\n-0.418***\n-0.315*\n-0.115\n\n\n\n(0.127)\n(0.123)\n(0.176)\n\n\nPopulation (log)\n0.275***\n0.283***\n0.284***\n\n\n\n(0.075)\n(0.076)\n(0.075)\n\n\nEthnic Fractionalization\n0.651+\n0.601\n0.599\n\n\n\n(0.379)\n(0.378)\n(0.374)\n\n\nReligious Fractionalization\n-0.233\n-0.154\n-0.077\n\n\n\n(0.421)\n(0.418)\n(0.427)\n\n\nDemocracy × Terrain\n\n-0.417+\n\n\n\n\n\n(0.227)\n\n\n\nDemocracy × GDP\n\n\n-1.086*\n\n\n\n\n\n(0.470)\n\n\nConstant\n-5.223***\n-6.336***\n-8.032***\n\n\n\n(1.487)\n(1.517)\n(1.905)\n\n\nNum.Obs.\n5964\n5964\n5964\n\n\nAIC\n1119.3\n1111.4\n1115.6\n\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nDependent variable: Binary indicator of civil war onset.\n\n\nStandard errors in parentheses. * p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01.\n\n\nData sources: UCDP/PRIO Armed Conflict Dataset, World Bank, V-Dem.\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nThe modelsummary() function has a couple of unique features that are worth highlighting. First, it takes a list of models that we want to display, which we need to identify with the list() function. Second, we use the coef_map argument to provide a named vector that maps our variable names to more descriptive labels. We can then also write our notes as a character vector to provide context about the table.\nWe then take the list of models, the coefficient map, and the notes and enter them into the modelsummary() function. The stars argument adds significance stars next to coefficients based on p-values. The gof_map argument specifies which goodness-of-fit statistics to include at the bottom of the table. Finally, we add a title and notes to provide context about the dependent variable, significance conventions, and data sources.\nNote that we are also rendering the output as gt, which allows us to create a professional-looking table that can be easily exported to formats like Word or PDF. The gt package provides extensive customization options for styling tables, but we will keep it simple here.\n\n\nThis table effectively communicates several key pieces of information. The coefficient mapping transforms cryptic variable names into descriptive labels that readers can immediately understand. The inclusion of multiple goodness-of-fit statistics allows readers to compare model performance. The detailed notes provide essential context about the dependent variable, significance conventions, and data sources.\nNotice how the interaction terms appear clearly labeled, making it easy for readers to identify which models include interaction effects. The consistent ordering of variables across models facilitates comparison of how coefficients change when we add complexity.",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#coefficient-plots-for-single-models",
    "href": "modules/module-12.2.html#coefficient-plots-for-single-models",
    "title": "Module 12.2",
    "section": "Coefficient Plots for Single Models",
    "text": "Coefficient Plots for Single Models\n\nWhile tables excel at presenting multiple models, coefficient plots often communicate single model results more effectively. Coefficient plots make effect sizes and uncertainty immediately visible, helping readers quickly identify the most important predictors and assess the reliability of estimates.\nLet’s create a professional coefficient plot for our baseline model using the modelplot() function:\n\nlibrary(ggplot2)\n\n# Create coefficient plot\nmodelplot(\n  baseline_model, \n  coef_map = rev(coef_mapping),  # rev() reverses order for bottom-up plotting\n  coef_omit = \"Intercept\",       # omit intercept for cleaner visualization\n  color = \"darkblue\"\n) + \n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 0.75) +\n  labs(\n    title = \"Figure 1: Predictors of Civil War Onset\",\n    subtitle = \"Logistic regression coefficients with 95% confidence intervals\",\n    caption = \"Note: Coefficients shown on log-odds scale. Dashed line indicates null effect.\",\n    x = \"Coefficient Estimate (Log-Odds Scale)\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\nThis visualization immediately reveals several important patterns. Variables whose confidence intervals cross the red dashed line (zero) are not statistically significant. The relative magnitude of effects becomes apparent through the horizontal distance from zero. The uncertainty around each estimate is clearly visible through the confidence interval width.\nThe coefficient plot is particularly effective for communicating with audiences who may find regression tables intimidating. The visual format makes it immediately obvious which variables matter most and how confident we should be in each estimate.\n\n\n\n\n\n\nYour Turn!\n\n\n\nCreate a coefficient plot for one of the interaction models. Experiment with:\n\nDifferent color schemes using the color argument\nAdding custom themes or styling elements\nModifying the title and subtitle to highlight key findings\nUsing coef_omit to hide variables that aren’t central to your story\n\nConsider how the interaction terms appear in your plot. Do they tell a clear story about conditional relationships?",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#visualizing-predicted-probabilities",
    "href": "modules/module-12.2.html#visualizing-predicted-probabilities",
    "title": "Module 12.2",
    "section": "Visualizing Predicted Probabilities",
    "text": "Visualizing Predicted Probabilities\nWhile coefficient plots show us relative effects on the log-odds scale, predicted probability plots translate these effects into the concrete terms that matter for practical decision-making. These visualizations answer questions like “What’s the actual conflict risk for a poor, mountainous, non-democratic country?” or “How much does economic development really matter in practical terms?”\nLet’s create visualizations that show predicted probabilities across realistic ranges of our key variables:\n\nlibrary(marginaleffects)\n\n# Create predicted probability plot for GDP effects\ngdp_predictions &lt;- plot_predictions(\n  baseline_model, \n  condition = \"wbgdppc2011est\",\n  points = 0.5  # show some data points for context\n) +\n  labs(\n    title = \"Figure 2: How Economic Development Affects Conflict Risk\",\n    subtitle = \"Predicted probability of civil war onset by GDP per capita\",\n    x = \"GDP per Capita (log scale)\",\n    y = \"Predicted Probability of Conflict Onset\",\n    caption = \"Shaded area represents 95% confidence interval. Points show actual data.\"\n  ) +\n  theme_minimal()\n\ngdp_predictions\n\n\n\n\n\n\n\nThis plot transforms the abstract coefficient of -0.365 for GDP into a concrete story: countries with very low GDP (around $1,000 per capita) have conflict probabilities around 3-4%, while wealthy countries (around $30,000 per capita) have probabilities closer to 1%. The confidence intervals show that this relationship is quite precisely estimated.\nNow let’s visualize how multiple variables interact to create different risk profiles:\n\n# Create prediction plot showing democracy-terrain interaction\ninteraction_predictions &lt;- plot_predictions(\n  terrain_interaction, \n  condition = c(\"newlmtnest\", \"democracy\")\n) +\n  labs(\n    title = \"Figure 3: Democracy Changes How Terrain Affects Conflict Risk\",\n    subtitle = \"Predicted probabilities by terrain roughness and political system\",\n    x = \"Mountainous Terrain\",\n    y = \"Predicted Probability of Conflict Onset\",\n    color = \"Political System\",\n    fill = \"Democracy\"\n  ) +\n  scale_color_manual(\n    values = c(\"0\" = \"red\", \"1\" = \"blue\"),\n    labels = c(\"0\" = \"Non-Democratic\", \"1\" = \"Democratic\")\n  ) +\n  scale_fill_manual(\n    values = c(\"0\" = \"red\", \"1\" = \"blue\"),\n    labels = c(\"0\" = \"Non-Democratic\", \"1\" = \"Democratic\")\n  ) +\n  theme_minimal()\n\ninteraction_predictions\n\n\n\n\n\n\n\nThis visualization makes the interaction effect immediately comprehensible. In non-democratic countries (red line), mountainous terrain increases conflict risk substantially. In democratic countries (blue line), the same terrain has little effect on conflict risk, and may even be slightly protective. This translates our statistical interaction into a clear substantive story about how political institutions moderate geographic risk factors.",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#marginal-effects-visualization",
    "href": "modules/module-12.2.html#marginal-effects-visualization",
    "title": "Module 12.2",
    "section": "Marginal Effects Visualization",
    "text": "Marginal Effects Visualization\nSometimes we want to focus specifically on how the effect of one variable changes across levels of another variable. Marginal effects plots show us these conditional relationships directly, highlighting where effects are strongest or weakest.\nLet’s create a marginal effects plot to illustrate how the effect of democracy on conflict risk varies across levels of economic development using the plot_slopes() function from the marginaleffects package:\n\n# Plot marginal effect of democracy across wealth levels\ndemocracy_marginal &lt;- plot_slopes(\n  wealth_interaction,\n  variables = \"v2x_polyarchy\",\n  condition = \"wbgdppc2011est\"\n) +\n  labs(\n    title = \"Figure 4: Democracy's Effect Depends on Economic Development\",\n    subtitle = \"Marginal effect of democracy on conflict risk by GDP per capita\",\n    x = \"GDP per Capita (log scale)\",\n    y = \"Marginal Effect of Democracy\",\n    caption = \"Effect shows change in conflict probability per unit increase in democracy score\"\n  ) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  theme_minimal()\n\ndemocracy_marginal\n\n\n\n\n\n\n\nThis plot reveals that democracy’s peace-promoting effects are strongest in middle-income countries. In very poor countries, democracy has little effect on conflict risk - perhaps because economic desperation overwhelms institutional protections. In very wealthy countries, democracy’s effects are also smaller - perhaps because both democratic and non-democratic wealthy countries have low conflict risk.\n\n\n\n\n\n\nYour Turn!\n\n\n\nCreate your own marginal effects visualization:\n\nChoose an interaction from one of our models\nUse plot_slopes() to show how the effect of one variable changes across levels of another\nAdd appropriate titles, labels, and reference lines\nExperiment with different variable combinations to see what stories emerge\n\nThink about what practical insights your visualization reveals. How would you explain these conditional effects to a policy audience?",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#writing-up-results-for-different-audiences",
    "href": "modules/module-12.2.html#writing-up-results-for-different-audiences",
    "title": "Module 12.2",
    "section": "Writing Up Results for Different Audiences",
    "text": "Writing Up Results for Different Audiences\nThe same statistical findings need to be communicated very differently depending on your audience. Academic papers require technical precision and comprehensive reporting of uncertainty. Policy briefs need clear, actionable insights with minimal jargon. Media interviews demand compelling stories supported by concrete examples.\nAcademic Writing\nFor academic audiences, emphasize methodological transparency and precise reporting of statistical results:\n\n“Table 1 presents three logistic regression models examining predictors of civil war onset from 1946-1999. Model 1 establishes our baseline specification, revealing that GDP per capita (β = -0.365, p &lt; 0.001) and population size (β = 0.265, p &lt; 0.001) are robust predictors of conflict risk, consistent with Fearon and Laitin’s original findings. Models 2 and 3 introduce interaction terms to test whether democracy moderates the effects of geographic and economic factors.\nThe democracy-terrain interaction (Model 2) is statistically significant (β = -0.415, p = 0.041), indicating that mountainous terrain increases conflict risk in non-democratic countries but has the opposite effect in democratic systems. Figure 3 illustrates this interaction: in authoritarian countries, moving from low to high terrain roughness increases predicted conflict probability from 1.2% to 2.6%, while in democratic countries the probability decreases from around 1.4% to about .05%.”\n\nPolicy Writing\nFor policy audiences, focus on practical implications and concrete risk assessments:\n\n“Our analysis of conflict patterns from 1946-1999 reveals that economic development and democratic governance are powerful conflict prevention tools, but they work differently depending on context.\nEconomic Development: Countries with GDP per capita below $2,000 face conflict risks of 3-4% annually, while countries above $10,000 face risks below 1%. This suggests that poverty reduction programs and economic development aid can significantly improve stability.\nDemocratic Institutions: Democracy’s effectiveness depends on economic conditions. In middle-income countries ($3,000-$15,000 GDP per capita), democratic institutions reduce conflict risk by approximately 40%. However, in extremely poor countries, democratic reforms alone may be insufficient without accompanying economic development.\nGeographic Factors: Mountainous terrain increases conflict risk only in non-democratic countries. This suggests that democratic institutions provide alternative channels for addressing grievances that might otherwise lead to insurgency in remote areas.”\n\nGeneral Public Communication\nFor general audiences, use accessible language and compelling examples:\n\n“What makes some countries more likely to experience civil war? Our research analyzing conflicts from 1946-1999 reveals three key insights:\nPoverty breeds instability: Poor countries are much more likely to experience civil wars. Countries with average incomes below $2,000 per person face about twice the conflict risk of middle-income countries, and four times the risk of wealthy nations.\nDemocracy helps, but timing matters: Democratic countries are generally more peaceful, but democracy works best in middle-income countries. Very poor countries may need economic development before democratic institutions can effectively prevent violence.\nGeography is political: Mountainous terrain only increases conflict risk in dictatorships. In democracies, people living in remote mountain regions have political voice and don’t need to resort to violence to address their grievances.”",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-12.2.html#synthesis-and-best-practices",
    "href": "modules/module-12.2.html#synthesis-and-best-practices",
    "title": "Module 12.2",
    "section": "Synthesis and Best Practices",
    "text": "Synthesis and Best Practices\nEffective communication of logistic regression results requires matching your presentation format to your audience and research goals. Use regression tables when comparing multiple model specifications or when academic transparency requires showing all coefficients. Choose coefficient plots when highlighting key predictors in a single model or when visual impact matters more than comprehensive detail. Employ predicted probability plots when translating statistical effects into concrete, practical terms that non-technical audiences can understand. Deploy marginal effects visualizations when conditional relationships are central to your argument and you need to show how effects vary across contexts.\nBeyond format selection, several principles enhance the effectiveness of any presentation approach. Always provide sufficient context about your dependent variable, sample, and time period. Use descriptive variable names that immediately convey meaning to your intended audience. Include appropriate uncertainty indicators (confidence intervals, standard errors, significance tests) but don’t let technical details obscure your main findings. Most importantly, connect your statistical results to substantive insights that matter for theory, policy, or public understanding.",
    "crumbs": [
      "Course Modules",
      "Module 12.2"
    ]
  },
  {
    "objectID": "modules/module-10.3.html",
    "href": "modules/module-10.3.html",
    "title": "Module 10.3",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the performance package: install.packages(\"performance\") and read the documentation\n\nInstall the GGally package: install.packages(\"GGally\") and read the documentation\n\nRun the following code to get set up for this module:\n\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(\n  indicators = c(\n  \"v2x_libdem\", \n  \"e_gdppc\", \n  \"v2cacamps\",\n  \"v2x_gender\",\n  \"v2x_corr\",\n  \"e_regionpol_6C\"),\n  start_year = 2006, \n  end_year = 2006\n  ) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps,\n    women_empowerment = v2x_gender,\n    corruption = v2x_corr,\n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = factor(\n    region,\n    labels = c(\n      \"Eastern Europe\", \n      \"Latin America\", \n      \"MENA\", \n      \"SS Africa\", \n      \"The West\", \n      \"Asia & Pacific\"))\n    ) \n\n#glimpse(model_data)",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#overview",
    "href": "modules/module-10.3.html#overview",
    "title": "Module 10.3",
    "section": "Overview",
    "text": "Overview\nYou’ve learned to fit linear regression models, interpret coefficients, handle outliers, and select variables. But how do you know if your model is actually valid? Linear regression makes several important assumptions, and violating these can lead to misleading results, incorrect inferences, and poor predictions.\nThe key insight is that we should examine our data before fitting models and then validate our assumptions after fitting them. This two-stage approach—exploratory data analysis (EDA) followed by residual diagnostics—helps ensure our models are both appropriate and reliable. Think of EDA as reconnaissance before battle and residual diagnostics as quality control after production.\nIn this module, we’ll explore comprehensive EDA techniques using the performance and GGally packages, then learn to check the four key assumptions underlying linear regression. These assumptions are often remembered by the acronym LINE, which we’ll explore in detail. We’ll continue using the democracy and development data from previous modules to illustrate these diagnostic techniques and remedial strategies.\nBy the end of this module, you’ll understand how to conduct thorough exploratory data analysis to spot potential modeling issues before they become problems. You’ll master the four key assumptions of linear regression and know how to check them systematically. Most importantly, you’ll develop the judgment to know when assumption violations are serious enough to require action and when they can be acknowledged but not necessarily fixed.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#exploratory-data-analysis-eda",
    "href": "modules/module-10.3.html#exploratory-data-analysis-eda",
    "title": "Module 10.3",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nBefore fitting any regression model, we should thoroughly explore our data. This exploratory phase can reveal potential problems and guide our modeling strategy. The GGally package provides powerful tools for comprehensive exploratory data analysis (EDA) that integrate seamlessly with our tidyverse workflow, allowing us to examine multiple relationships simultaneously rather than creating dozens of individual plots.\nThe ggpairs() function creates a matrix of plots showing relationships between all pairs of variables. This gives us a comprehensive overview in a single visualization, something that would otherwise require creating many individual plots. The beauty of this approach is that it reveals patterns we might miss when examining variables one at a time.\n\nlibrary(GGally)\n\nmodel_data |&gt;\n  select(lib_dem:corruption) |&gt; \n  ggpairs()\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nggpairs() fits right into our Tidyverse workflow. Here we are selecting the continuous variables we want to examine from our model_data (dataset, which includes )lib_dem, wealth, polarization, corruption, and region) with the : operator and then piping those variables into ggpairs().\n\n\nThis single command creates a rich matrix of information. The diagonal shows the distribution of each variable, helping us assess whether variables are normally distributed, skewed, or have unusual patterns. The upper triangle displays correlation coefficients, giving us a quick sense of which variables are most strongly related. The lower triangle presents scatterplots showing the actual relationships between variables, where we can spot non-linear patterns, outliers, or changes in variance.\nWhen examining the ggpairs() output, we’re looking for several key patterns. Linearity is crucial since linear regression assumes straight-line relationships, so we watch for curves or bends in the scatterplots. Outliers appear as points far from the main pattern and can dramatically influence our regression results. Skewness in the diagonal plots might suggest the need for transformations. Heteroscedasticity refers to changing variance across the range of predictors, which often manifests as a funnel shape in scatterplots. Finally, strong correlations between predictors might signal multicollinearity problems we’ll need to address.\nThe correlation coefficients in the upper triangle deserve special attention. Values close to +1 or -1 indicate strong linear relationships, while values near 0 suggest weak relationships. As a rule of thumb, we are on the lookout for correlations that are stronger than 0.7 or weaker than -0.7, as these suggest potentially important relationships that we should explore further. Note, however, correlation coefficients can be misleading if relationships are non-linear, which is why we also examine the scatterplots below the diagonal.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#patterns-across-groups",
    "href": "modules/module-10.3.html#patterns-across-groups",
    "title": "Module 10.3",
    "section": "Patterns Across Groups",
    "text": "Patterns Across Groups\nRemember ridge plots from earlier modules? They’re perfect for comparing distributions across groups, and they provide a elegant way to examine how our outcome variable varies across different categories. Let’s examine how women’s empowerment varies by region using this technique we learned in an earlier module.\n\nlibrary(ggridges)\n\nmodel_data |&gt;\n  ggplot(aes(x = women_empowerment, y = region)) +\n  geom_density_ridges(fill = \"steelblue\", alpha = 0.7, scale = .9) +\n  theme_ridges() +\n  labs(x = \"\", y = \"\",\n       title = \"Distribution of Women's Empowerment Scores by Region\")\n\n\n\n\n\n\n\nThis plot reveals important patterns that will inform our modeling strategy. One thing that really jumps out is that the distributions are much more normally distributed at the regional level relative to the overall distribution that we saw in our earlier pairs plot (which was heavily left-skewed). But we also see very different distributions across regions. For example, Western Europe and North America show high women’s empowerment scores just above 0.9, creating a tight distribution that suggests consistency within this region. Whereas Asia shows lower scores concentrated around 0.6, indicating systematically different political systems and Sub-Saharan Africa displays a wide spread of scores, suggesting substantial variation within the region.\nThese regional patterns suggest that inculding region as a control variable in our models will be important. It also suggests that we might want to explore interactions between region and other predictors, such as wealth, to see if the relationships differ across regions.\n\n\n\n\n\n\nYour Turn!\n\n\n\n\nLooking at the ggpairs() output, are there variables that show heavy skewness?\nAre there any variables that appear to have non-linear relationships with lib_dem?\nDo you notice any strong correlations between predictors that might suggest multicollinearity?\nAre there any outliers that stand out in the scatterplots?\nTry creating a ridge plot for some of the variables in the dataset, particularly ones with skewness and outliers. Do these patterns persist at the regional level? How does this influence your modeling decisions?",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#the-line-conditions-what-linear-regression-assumes",
    "href": "modules/module-10.3.html#the-line-conditions-what-linear-regression-assumes",
    "title": "Module 10.3",
    "section": "The LINE Conditions: What Linear Regression Assumes",
    "text": "The LINE Conditions: What Linear Regression Assumes\nNow that we’ve explored our data, let’s formalize the assumptions underlying linear regression. These assumptions aren’t arbitrary mathematical requirements but reflect important properties that make linear regression a sensible approach to modeling relationships. The assumptions are often remembered by the acronym LINE: Linearity, Independence, Normality, and Equal variance.\nLinearity assumes that the relationship between predictors and outcome is linear. This doesn’t mean the relationship has to be perfectly straight, but the general pattern should follow a linear trend. If the true relationship is curved, a straight line will systematically over-predict in some regions and under-predict in others, leading to biased results.\nIndependence requires that observations are independent of each other. This assumption is often violated in ways that aren’t immediately obvious, such as when data points are clustered in time, space, or social groups. When observations are correlated, our standard errors become unreliable, leading to overconfident statistical inferences.\nNormality refers to the distribution of residuals, not the original variables. The residuals should be approximately normally distributed around zero. This assumption is particularly important for hypothesis testing and confidence intervals, though it becomes less critical with larger sample sizes due to the Central Limit Theorem.\nEqual variance, also called homoscedasticity, requires that residuals have constant variance across all values of the predictors. When this assumption is violated (called heteroscedasticity), our model’s predictions become more reliable for some values than others, and our standard errors become incorrect.\nUnderstanding why these assumptions matter helps us make better modeling decisions. Each assumption serves a specific purpose in ensuring that our regression results are trustworthy and interpretable. Linearity ensures that our linear model accurately captures the relationship. Independence is crucial for valid statistical inference because correlated observations invalidate our standard error calculations. Normality of residuals is needed for valid hypothesis tests and confidence intervals, particularly in smaller samples. Equal variance ensures that our model’s predictions are equally reliable across all values of the predictors.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#residual-diagnostics",
    "href": "modules/module-10.3.html#residual-diagnostics",
    "title": "Module 10.3",
    "section": "Residual Diagnostics",
    "text": "Residual Diagnostics\nWhile our earlier exploratory data analysis provided early warnings about potential assumption violations, EDA alone is not sufficient for checking assumptions. The patterns we see in raw data guide our transformation decisions, but we need to examine residuals from our fitted model (using the transformed variables) to formally check whether our assumptions are met. Residuals represent what’s left unexplained after our model has done its best to capture the relationships in the data.\nLet’s fit our model and then examine the residuals to systematically check our assumptions. We’ll use the democracy model we have been developing throughout this module series, which includes log-transformed wealth (based on what we learned from our EDA), polarization, corruption, and regional controls.\n\n# Fit our democracy model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth) + polarization + corruption + women_empowerment + region, data = model_data)\n\n# Quick model summary\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth) + polarization + corruption + \n    women_empowerment + region, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43252 -0.06331  0.00253  0.07402  0.26311 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.125962   0.093258   1.351   0.1787    \nlog(wealth)          -0.001450   0.014026  -0.103   0.9178    \npolarization         -0.009154   0.008323  -1.100   0.2730    \ncorruption           -0.344607   0.055068  -6.258 3.35e-09 ***\nwomen_empowerment     0.640860   0.072556   8.833 1.61e-15 ***\nregionLatin America   0.062175   0.033837   1.837   0.0680 .  \nregionMENA           -0.004997   0.043303  -0.115   0.9083    \nregionSS Africa      -0.011206   0.034987  -0.320   0.7491    \nregionThe West        0.109096   0.039341   2.773   0.0062 ** \nregionAsia & Pacific -0.029083   0.036100  -0.806   0.4216    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1228 on 162 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.8055,    Adjusted R-squared:  0.7947 \nF-statistic: 74.55 on 9 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nNow we’ll systematically check each assumption using residual plots. Residuals are the differences between our observed values and the values predicted by our model. If our model is appropriate and our assumptions are met, these residuals should exhibit random patterns with no systematic structure.\nChecking Linearity: Residuals vs. Fitted\nThe residuals versus fitted values plot is perhaps the most important diagnostic plot because it can reveal multiple types of problems simultaneously. If our linearity assumption is met, residuals should be randomly scattered around zero with no systematic patterns.\n\nCodelibrary(plotly)\n\n# Extract model information\nmodel_data_diag &lt;- model_data |&gt;\n  drop_na() |&gt;\n  mutate(\n    fitted = fitted(democracy_model),\n    residuals = residuals(democracy_model)\n  )\n\n# Residuals vs. fitted plot\nr_vs_fitted &lt;- model_data_diag |&gt;\n  ggplot(aes(x = fitted, y = residuals)) +\n  geom_point(aes(text = country), alpha = 0.6) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\",\n       title = \"Residuals vs. Fitted Values\") +\n  theme_minimal()\n\nggplotly(r_vs_fitted) |&gt;\n   config(displayModeBar = FALSE)\n\n\n\n\n\nHere we are looking at the residuals (the differences between observed and predicted values) plotted against the fitted values (the predicted values from our model). The red dashed line represents zero, and the blue line is a smoothing line that helps us see the overall trend in the residuals. Points above the red line indicate that our model under-predicted those observations (meaning the model predicted that the country was less democracy than it actually was), while points below the line indicate over-predictions (meaning that the model predicted that the country was more democratic than in reality).\nIdeally we would want to see points that are randomly scattered around the horizontal line at y = 0. The blue smoothing line should be roughly horizontal and close to zero. If we see systematic patterns such as U-shapes, inverted U-shapes, or consistent trends, this suggests that our linear model is missing important non-linear relationships.\nIn this case, we definitely see some curvature to the line. The model is under-predicting countries with low democracy scores, over-predicting countries with mid-level democracy scores and under-predicting again at higher levels of democracy. This suggests that the relationship between our predictors and the outcome is not purely linear, and we might need to consider non-linear transformations or additional predictors to capture this pattern.\n\n\n\n\n\n\nNote\n\n\n\nThe model predicts values of democracy below what an actual country could achieve, which is not possible. This may be due to nonlinearity in the relationship between our predictors and the dependent variable as we already discussed. But it could also be due to the fact that our dependent variable, lib_dem, is bounded between 0 and 1. We could therefore consider other models that are designed for bounded outcomes, such as beta regression or tobit models. However, for the purposes of this module, we will continue with linear regression and focus on checking our assumptions.\n\n\nChecking Normality: Q-Q Plot\nThe Q-Q (quantile-quantile) plot compares the distribution of our residuals to what we would expect from a normal distribution. This plot is particularly useful because it makes deviations from normality easy to spot.\n\nCode# Calculate Q-Q values manually to preserve country names\nqq_data &lt;- model_data_diag |&gt;\n  arrange(residuals) |&gt;\n  mutate(\n    theoretical = qnorm(ppoints(length(residuals))),\n    ordered_residuals = sort(residuals),\n    country_ordered = country[order(residuals)]\n  )\n\nqq_plot &lt;- qq_data |&gt;\n  ggplot(aes(x = theoretical, y = ordered_residuals, text = country_ordered)) +\n  geom_point() +\n  geom_abline(slope = sd(model_data_diag$residuals), \n              intercept = mean(model_data_diag$residuals), \n              color = \"red\") +\n  labs(title = \"Normal Q-Q Plot of Residuals\",\n       x = \"Theoretical Quantiles\", \n       y = \"Sample Quantiles\") +\n  theme_minimal()\n\nqq_plot\n\n\n\n\n\n\nCode# ggplotly(qq_plot) |&gt;\n#   config(displayModeBar = FALSE)\n\n\nPoints should fall roughly along the red diagonal line if residuals are normally distributed. Systematic deviations from this line suggest non-normal residuals. Points that curve away from the line at the ends indicate heavy tails, while S-shaped curves suggest skewness. Small deviations aren’t usually concerning, especially with larger sample sizes, but dramatic departures from the line warrant attention.\nLooking at this Q-Q plot, we can see a pattern that is fairly reassuring about our model’s performance. The points follow the red diagonal line quite well through the middle range (roughly between -1.5 and +1.5 theoretical quantiles), indicating that the central portion of our residuals is approximately normally distributed. However, at both ends of the distribution, the points fall slightly below the line, suggesting that our residuals have “light tails” compared to a perfect normal distribution. This means our model produces fewer extreme prediction errors—both very large over-predictions and very large under-predictions—than we would expect from a normal distribution.\nChecking Equal Variance: Scale-Location Plot\nThe scale-location plot helps us assess whether the variance of residuals remains constant across all fitted values. This plot uses the square root of the absolute value of residuals, which makes patterns easier to see.\n\nCode# Scale-location plot\nscale_location_plot &lt;- model_data_diag |&gt;\n  mutate(sqrt_abs_resid = sqrt(abs(residuals))) |&gt;\n  ggplot(aes(x = fitted, y = sqrt_abs_resid)) +\n  geom_point(aes(text = country), alpha = 0.6) +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"√|Residuals|\",\n       title = \"Scale-Location Plot\") +\n  theme_minimal()\n\nggplotly(scale_location_plot) |&gt;\n  config(displayModeBar = FALSE)\n\n\n\n\n\nHere we are looking for a roughly horizontal blue line and equal spread of points across all fitted values. If the blue line shows an increasing or decreasing trend, this suggests heteroscedasticity. Heteroscedasticity means that the variance of residuals changes with fitted values, which violates the equal variance assumption. The most common pattern is increasing variance with larger fitted values, which appears as an upward-sloping trend in this plot.\nWith our data we are seeing a line that slopes downward, then upward and then downward again. This pattern suggests that our model performs most consistently when predicting highly democratic countries (mainly established Western democracies with stable institutions), while showing much more variability when predicting countries with low to moderate democracy scores (possibly reflecting the inherent volatility and unpredictability of transitional political systems or authoritarian regimes).\nSubstantively, this heteroscedasticity pattern makes theoretical sense—established democracies are more predictable based on economic and social indicators, while countries undergoing political transitions or operating under authoritarian systems may be subject to sudden changes that our model cannot capture. From a statistical perspective, however, this represents a clear violation of the equal variance assumption, indicating that our standard errors may be incorrect and that we should use robust standard errors when making statistical inferences.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#automated-diagnostics",
    "href": "modules/module-10.3.html#automated-diagnostics",
    "title": "Module 10.3",
    "section": "Automated Diagnostics",
    "text": "Automated Diagnostics\nNow that we know what the basic residual plots are and how to interpret them, we can lighten our load a little bit by using a package that automates the diagnostic process. The performance package provides a convenient check_model() function that generates a comprehensive set of diagnostic plots and statistics for our fitted model. This function produces the same types of plots we created manually and also includes additional diagnostics like multicollinearity checks (VIF values) and checks for influential observations.\nLet’s try running check_model() on our democracy model to see what it tells us about our assumptions:\n\nlibrary(performance)\n\ncheck_model(democracy_model)\n\n\n\n\n\n\n\nHere we we see the same issues highlighted in terms of linearity, normality, and equal variance that we identified manually. The residuals versus fitted plot shows a clear non-linear pattern, the Q-Q plot indicates slight deviations from normality, and the scale-location plot suggests heteroscedasticity. The VIF values indicate that multicollinearity is not a concern in this model, as all values are below the common threshold of 5 and there are no influential observations.\nVariable-level Diagnostics with GGally\nNow that we know we have these issues, we can use the GGally package to create variable-level diagnostic plots that help us understand how each predictor contributes to these patterns. The ggnostic() function provides a comprehensive set of diagnostic plots for each predictor in our model, allowing us to see how they relate to the residuals and fitted values.\n\nggnostic(democracy_model)\n\n\n\n\n\n\n\nThe ggnostic() output helps us identify the specific source of the issues that the check_model() diagnostics revealed concerning patterns of non-linearity and heteroscedasticity in our overall model. Examining the first two rows of the ggnostic() plots, we can see that the women empowerment variable stands out with a distinctly wiggly blue line in the residuals plot and changing variance patterns in the scale-location plot, while other predictors show relatively flat, well-behaved relationships. This suggests that women empowerment is contributing to the assumption violations we observed in the overall model diagnostics.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#fixing-the-problems-transformation-strategies",
    "href": "modules/module-10.3.html#fixing-the-problems-transformation-strategies",
    "title": "Module 10.3",
    "section": "Fixing the Problems: Transformation Strategies",
    "text": "Fixing the Problems: Transformation Strategies\nWhen we detect assumption violations, we have several remedial strategies available. Variable transformations are often the first line of defense because they can simultaneously address multiple issues.\nLog transformations are particularly useful for skewed relationships and can address both non-linearity and heteroscedasticity simultaneously. We’ve already seen how log transformations help with wealth data, which tends to be highly skewed with a few very wealthy countries. Another option for addressing nonlinearity is to add polynomial terms, which allow us to capture curved relationships without transforming the original variable.\n\n\n\n\n\n\nNote\n\n\n\nThere are many other strategies for addressing assumption violations, such as adding interaction terms, using robust standard errors, or employing generalized linear models. However, variable transformations are often the most straightforward and effective first step and also the most suitable for an introductory module like this one. As you gain more experience, you can learn about more strategies and how to apply them.\n\n\nTo address the model assumptions we identified in our democracy model, we can implement both a logarithmic transformation and a polynomial term for women empowerment log(women_empowerment) + I(log(women_empowerment)^2), allowing the model to capture the curved relationship and potentially reducing both the systematic residual patterns and heteroscedasticity that were evident in our original specification.\n\n# Fit our democracy model\ndemocracy_model2 &lt;- lm(lib_dem ~ log(wealth) + polarization + corruption + log(women_empowerment) + I(log(women_empowerment)^2) + region, data = model_data)\n\n# Quick model summary\nsummary(democracy_model2)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth) + polarization + corruption + \n    log(women_empowerment) + I(log(women_empowerment)^2) + region, \n    data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43129 -0.06233  0.00449  0.07454  0.25560 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  0.786701   0.059870  13.140  &lt; 2e-16 ***\nlog(wealth)                 -0.006507   0.014176  -0.459  0.64682    \npolarization                -0.010578   0.008298  -1.275  0.20421    \ncorruption                  -0.326338   0.055780  -5.850 2.66e-08 ***\nlog(women_empowerment)       0.706621   0.102286   6.908 1.08e-10 ***\nI(log(women_empowerment)^2)  0.250075   0.054304   4.605 8.33e-06 ***\nregionLatin America          0.063280   0.033694   1.878  0.06218 .  \nregionMENA                   0.002799   0.043352   0.065  0.94860    \nregionSS Africa             -0.012929   0.034752  -0.372  0.71035    \nregionThe West               0.110825   0.039155   2.830  0.00524 ** \nregionAsia & Pacific        -0.017515   0.036552  -0.479  0.63246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1222 on 161 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.8086,    Adjusted R-squared:  0.7967 \nF-statistic: 68.03 on 10 and 161 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nHere we use the inhibit I() function so that R treats it is a literal mathematical expression, e.g. “take the log of women_empowerment and then square it”. This allows us to include polynomial terms in our model without R interpreting them as interaction terms of special formula operators.\n\n\nThe results of the analysis are interesting from a substantive perspective. Both the linear term and the quadratic term are positive and statistically significant. This suggests an upward opening parabola, whereby as women’s empowerment increases, democracy increases at an accelerating rate. Now let’s run check_model() on our new democracy model and see if our transformations had the desired effect with respect to satisfying model assumptions:\n\ncheck_model(democracy_model2)\n\n\n\n\n\n\n\nHere we see a slight improvement in our plots. The residuals versus fitted plot still shows some non-linearity, but the pattern is less pronounced than before. The Q-Q plot shows that the residuals are closer to normality, and the scale-location plot indicates that the variance of residuals is more constant across fitted values.\nWe definitely still have a high degree of heteroscedasticity, which could be an argument in favor of using robust standard errors or considering a different modeling approach, but at least this provides you with a sense of how we can systematically address assumption violations with the help of diagnostic plots.\n\n\n\n\n\n\nYour Turn!\n\n\n\n\nTry running ggnostic(democracy_model2) to see how the variable-level diagnostics have changed. Are there any other predictors that could benefit from transformations?\nTry tansforming the corruption variable using a log transformation and see how that affects the model diagnostics using check_model(). Does it help with the non-linearity or heteroscedasticity?\nNow try adding a polynomial term for polarization, e.g. I(polarization^2), and see how that affects the model diagnostics. Does it help with the non-linearity or heteroscedasticity?\nTry running ggnostic() on your new model and make any additional changes as you see fit.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.3.html#conclusion",
    "href": "modules/module-10.3.html#conclusion",
    "title": "Module 10.3",
    "section": "Conclusion",
    "text": "Conclusion\nChecking model assumptions is not just a statistical formality but an essential component of producing reliable, interpretable results. The combination of thorough exploratory data analysis and systematic residual diagnostics gives you confidence that your models are appropriate for your data and research questions.\nThe process we’ve outlined—starting with comprehensive EDA, fitting thoughtfully chosen models, and systematically checking assumptions—represents best practice in regression analysis. This approach helps you catch problems early, make informed decisions about model specifications, and communicate limitations honestly to your audience.\nRemember that linear regression remains one of the most powerful and interpretable tools in data science, but like any tool, it works best when used appropriately. Perfect adherence to all assumptions is rare in real-world data, so developing judgment about when violations matter and when they don’t is crucial for practical data analysis.\nIn our next module, we’ll build on these foundations to explore more advanced modeling techniques and learn when linear regression might not be the right tool for the job. Understanding when and why linear regression works prepares you to recognize situations where alternative approaches might be more appropriate.",
    "crumbs": [
      "Course Modules",
      "Module 10.3 (Optional)"
    ]
  },
  {
    "objectID": "modules/module-10.2.html",
    "href": "modules/module-10.2.html",
    "title": "Module 10.2",
    "section": "",
    "text": "Prework\n\n\n\n\nLook over the documentation for the broom package, which we will use to compute regression diagnostics.\nLook over the documentation for the datawizard package, which we will use to winsorize our data).\nRun this code chunk to load the necessary packages and data for this module:\n\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(\n  indicators = c(\n  \"v2x_libdem\", \n  \"e_gdppc\", \n  \"v2cacamps\"),\n  start_year = 2019, \n  end_year = 2019\n  ) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps\n    ) |&gt;\n  filter(!is.na(lib_dem), !is.na(wealth))",
    "crumbs": [
      "Course Modules",
      "Module 10.2"
    ]
  },
  {
    "objectID": "modules/module-10.2.html#overview",
    "href": "modules/module-10.2.html#overview",
    "title": "Module 10.2",
    "section": "Overview",
    "text": "Overview\nRemember those wealthy but undemocratic countries (like Saudi Arabia and UAE) that appeared as outliers in our GDP-democracy analysis? In this module, we’ll learn systematic approaches for identifying and handling such outliers. You’ll discover that outliers aren’t always “bad” data points to remove—sometimes they represent the most interesting cases in your analysis!\nWe will explore a number of key strategies for dealing with outliers once identified, helping you make informed decisions for your own projects. By the end of this module, you’ll have a toolkit for handling outliers in your regression analyses and understand when each approach is most appropriate.",
    "crumbs": [
      "Course Modules",
      "Module 10.2"
    ]
  },
  {
    "objectID": "modules/module-10.2.html#what-are-outliers-and-why-do-they-matter",
    "href": "modules/module-10.2.html#what-are-outliers-and-why-do-they-matter",
    "title": "Module 10.2",
    "section": "What Are Outliers and Why Do They Matter?",
    "text": "What Are Outliers and Why Do They Matter?\nLet’s start by revisiting our democracy and GDP analysis from Module 4.1. Remember this plot?\n\n\n\n\n\n\nNotice those points in the southwest corner—wealthy countries with low democracy scores. These are our outliers: observations that don’t fit the general pattern of the data. Try hovering over the points to see which countries they include.\nBut it is important to note that not all outliers are created equal. In our case, these outliers are not measurement errorr but instead substantively interesting cases! They largely represent oil-rich authoritarian states, which tells us something important about the relationship between wealth and democracy.\nTypes of Outliers in Regression\nIn regression analysis, we distinguish between different types of unusual observations:\n\n\nLeverage points: Extreme values on the x-axis (very high or low GDP)\n\nInfluential points: Points that significantly change the regression line when removed\n\nResidual outliers: Points far from the regression line (high residuals)\n\nA point can be one, two, or all three of these simultaneously.",
    "crumbs": [
      "Course Modules",
      "Module 10.2"
    ]
  },
  {
    "objectID": "modules/module-10.2.html#identifying-outliers-visual-and-statistical-methods",
    "href": "modules/module-10.2.html#identifying-outliers-visual-and-statistical-methods",
    "title": "Module 10.2",
    "section": "Identifying Outliers: Visual and Statistical Methods",
    "text": "Identifying Outliers: Visual and Statistical Methods\nBoxplots for Visual Identification\nThe simplest way to spot outliers is with boxplots. Let’s examine both our variables with boxplots. Here we will create boxplots for both the democracy scores and the raw GDP per capita (before any transformations) using geom_boxplot(), which we learned about in an earlier lesson. We will store each object as a separate object and then use patchwork to combine them into a single figure.\n\n# Create boxplots for both variables\nlibrary(patchwork)  # for combining plots\n\n# Boxplot for democracy scores\np1 &lt;- ggplot(model_data, aes(y = lib_dem)) +\n  geom_boxplot() +\n  labs(y = \"Liberal Democracy Score\", \n       title = \"Democracy Outliers\") +\n  theme_minimal()\n\n# Boxplot for raw GDP (before transformation)\np2 &lt;- ggplot(model_data, aes(y = wealth)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(y = \"GDP per Capita\", \n       title = \"GDP Outliers (Raw Data)\") +\n  theme_minimal()\n\n# Combine the plots\np1 | p2\n\n\n\n\n\n\n\nThe IQR Method\nThe Interquartile Range (IQR) method is a common statistical approach for identifying outliers and forms the basis of the boxplot. It classifies a data point as an outlier if it falls more than 1.5 times the IQR below the first quartile (Q1) or above the third quartile (Q3). This helps flag unusually high or low values that may deserve further scrutiny.\nIn the code below, we identify outliers in GDP per capita using the base R function boxplot.stats(), which applies the IQR method internally. We use the %in% operator to check which GDP values are identified as outliers and store this as a new logical column, gdp_outlier. Since our earlier boxplot analysis showed no democracy outliers, we simplify the analysis here by focusing only on GDP. We then summarize how many outliers there are and list the countries identified as GDP outliers.\n\n# Identify GDP outliers \nmodel_data &lt;- model_data |&gt;\n  mutate(gdp_outlier = wealth %in% boxplot.stats(wealth)$out)\n\n# Summarize how many outliers\nmodel_data |&gt;\n  summarize(\n    gdp_outliers = sum(gdp_outlier),\n    percent_outliers = round(100 * mean(gdp_outlier), 1)\n  )\n\n  gdp_outliers percent_outliers\n1            6              3.4\n\n# View GDP outlier countries\nmodel_data |&gt;\n  filter(gdp_outlier) |&gt;\n  arrange(desc(wealth)) |&gt;\n  select(country, wealth, lib_dem)\n\n                   country wealth lib_dem\n1               Luxembourg 92.389   0.798\n2                    Qatar 80.190   0.084\n3                  Ireland 75.467   0.825\n4                Singapore 72.025   0.333\n5     United Arab Emirates 64.628   0.092\n6 United States of America 60.641   0.737\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nIn the code above, we use the base R function boxplot.stats() to identify GDP outliers. This function returns several components, including a named element called $out that contains the values considered outliers.\nSo when we write wealth %in% boxplot.stats(wealth)$out we are checking which values of wealth are included in the set of outliers returned by boxplot.stats(). The result is a logical vector (TRUE for outliers, FALSE otherwise), which we store in a new column using mutate().\n\n\nIdentifying Influential Points in Regression\nTo assess the quality of a regression model and identify observations that may unduly affect the results, we can compute diagnostic statistics. These help us detect data points that are surprising, unusual, or overly influential in determining the regression line. While there are multiple diagnostic measures, three are commonly used: leverage, standardized residuals, and Cook’s distance.\nWe use the broom package’s augment() function to compute these diagnostics in a tidy, data-frame format. augment() takes a fitted model object (like one created by lm()) and returns the original data along with new columns containing fitted values, residuals, and other diagnostic measures:\n\n\n.fitted: the predicted values from the model\n\n.resid: the raw residuals (observed – fitted)\n\n.std.resid: standardized residuals (adjusted for their expected variance)\n\n.hat: leverage values, which measure how extreme the predictor values are\n\n.cooksd: Cook’s distance, which combines leverage and residual size to estimate how much a point influences the model\n\nThese statistics allow us to flag potentially problematic points:\n\n\nHigh leverage points have unusual predictor values. They don’t necessarily distort the model, but they have the potential to. A common rule of thumb is that leverage values greater than twice the mean are worth inspecting.\n\nHigh residual points are poorly fit by the model—they lie far from the regression line. Standardized residuals larger than ±2 are typically considered large.\n\nHigh influence points affect the model’s coefficients disproportionately. Cook’s distance greater than 4/n (where n is the number of observations) is a common informal threshold.\n\nBy combining these diagnostics, we can identify cases that might merit further attention—due to unusual inputs, poor fit, or disproportionate influence on the model’s results.\n\nlibrary(broom)\n\n# Fit model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Get model diagnostics (with original data attached)\nmodel_diagnostics &lt;- augment(democracy_model, data = model_data)\n\n# Add thresholds for outlier detection\nmodel_diagnostics &lt;- model_diagnostics |&gt;\n  mutate(\n    high_leverage = .hat &gt; 2 * mean(.hat, na.rm = TRUE),\n    high_residual = abs(.std.resid) &gt; 2,\n    high_influence = .cooksd &gt; 4 / nrow(model_data)\n  )\n\n# Filter problematic cases\nmodel_diagnostics |&gt;\n  filter(high_leverage | high_residual | high_influence) |&gt;\n  select(country, wealth, lib_dem, high_leverage, high_residual, high_influence)\n\n# A tibble: 16 × 6\n   country             wealth lib_dem high_leverage high_residual high_influence\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;         &lt;lgl&gt;         &lt;lgl&gt;         \n 1 Venezuela            1.19    0.059 TRUE          FALSE         FALSE         \n 2 Niger                1.19    0.399 TRUE          FALSE         FALSE         \n 3 Burundi              0.735   0.054 TRUE          FALSE         FALSE         \n 4 Central African Re…  0.827   0.22  TRUE          FALSE         FALSE         \n 5 Ireland             75.5     0.825 TRUE          FALSE         FALSE         \n 6 Liberia              1.19    0.412 TRUE          FALSE         FALSE         \n 7 Malawi               1.31    0.412 TRUE          FALSE         FALSE         \n 8 Qatar               80.2     0.084 TRUE          TRUE          TRUE          \n 9 Democratic Republi…  0.913   0.145 TRUE          FALSE         FALSE         \n10 Eritrea             21.1     0.009 FALSE         TRUE          FALSE         \n11 Madagascar           1.31    0.258 TRUE          FALSE         FALSE         \n12 Turkmenistan        25.0     0.037 FALSE         TRUE          FALSE         \n13 Bahrain             30.0     0.052 FALSE         TRUE          TRUE          \n14 Luxembourg          92.4     0.798 TRUE          FALSE         FALSE         \n15 Saudi Arabia        33.3     0.047 FALSE         TRUE          TRUE          \n16 United Arab Emirat… 64.6     0.092 FALSE         TRUE          TRUE          \n\n\nThe analysis identified a fairly large number of cases as having high leverage, large residuals, or high influence. But it is important to note that these are not necessarily errors in the data or points that necessarily have to be removed. Rather, they are observations that deviate from what the model expects and exert a disproportionate pull on the regression line.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nTry running the regression with polarization as the independent variable instead of wealth.\nNow use the code above to identify influential observations in this new model.\nWhat do you notice? Are there any countries that are influential in both models? Or are most of the influential points new ones?",
    "crumbs": [
      "Course Modules",
      "Module 10.2"
    ]
  },
  {
    "objectID": "modules/module-10.2.html#dealing-with-outliers-strategies-and-considerations",
    "href": "modules/module-10.2.html#dealing-with-outliers-strategies-and-considerations",
    "title": "Module 10.2",
    "section": "Dealing with Outliers: Strategies and Considerations",
    "text": "Dealing with Outliers: Strategies and Considerations\nOutliers and influential observations warrant closer inspection. Outliers may represent countries with unusual political or economic profiles, or cases that do not conform well to the general trend. Influential observations often reveal the limits of a simple model and can point to deeper questions about the structure of the data or the need for additional variables.\nAfter considering extreme or influential data points more closely, we can decide what to do with them. One option is to do nothing and leave them in the dataset as is. Another option to assess how much they affect the results by re-estimating the model with and without them. Another option is to tranform the data to reduce their influence. Finally, we can perform an operation called winsorizing, which caps extreme values at a specified percentile rather than removing them entirely.\nRemoving Outliers\nOne approach is to remove outliers entirely. This is appropriate when the outliers represent data entry errors, they come from a different population than your main analysis, or you want to understand the relationship for the “typical” cases. But another option is to remove outliers or influential points termporarily to see how they affect the results. If they change the results significantly, this should be acknowledged and discussed.\nLet’s start by rerunning the regression model with the original data, including all observations:\n\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nNow let’s remove some of the problematic points we found earlier. Since we identified several countries as having high leverage, large residuals, or high influence, let’s see how removing them affects our regression results. We’ll fit the model both with and without these outliers and compare the results. Let’s start by removing the outliers.\n\n# Remove countries with extreme leverage or influence\nmodel_data_no_outliers &lt;- model_diagnostics |&gt;\n  filter(!high_leverage | !high_residual | !high_influence) |&gt;\n  select(country, wealth, lib_dem,)\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nHere we use the filter() function in conjunction with the ! and | operators. The ! operator negates the condition, so !high_leverage means “not high leverage.” The | operator means “or,” so we are filtering out any rows that have high leverage, large residuals, or high influence.\n\n\nNow let’s fit the model without the outliers to see if our results substantially change.\n\nmodel_without_outliers &lt;- lm(lib_dem ~ log(wealth), data = model_data_no_outliers)\n\nsummary(model_without_outliers)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data_no_outliers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55358 -0.14282  0.04255  0.18092  0.37282 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.12166    0.03756   3.239  0.00144 ** \nlog(wealth)  0.12568    0.01459   8.614 4.53e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2195 on 171 degrees of freedom\nMultiple R-squared:  0.3026,    Adjusted R-squared:  0.2985 \nF-statistic:  74.2 on 1 and 171 DF,  p-value: 4.534e-15\n\n\nHere we do not notice a substantial difference between these results and those of the original model. We see that the coefficient for log(wealth) remains positive and statistically significant, and the R-squared value is similar. This suggests that while the outliers were influential, they did not fundamentally change the relationship between GDP and democracy in this case.\nData Transformation\nNow that we’ve identified outliers in our raw data, let’s see how transformations can help reduce their influence. We’ve already been using one transformation—taking the log of GDP! Let’s see why this helps with outliers.\n\n\n\n\n\n\n\n\nNotice how the log transformation improves the model fit. This is because logging compresses large values, reducing the influence of those extremely wealthy outlier countries.\nOther common transformations, like the square root and Box-Cox transformations, can also help manage skewed data and reduce the influence of outliers. Like the log transformation, these approaches work by compressing large values, which pulls extreme observations closer to the main body of the data. This can lead to a better-fitting model and more stable inferences.\nThe square root transformation is useful when data are moderately skewed and values are all positive. It’s a simpler alternative to logging and is often used when data contain some high values but not extreme outliers.\nThe Box-Cox transformation is more flexible—it finds the “best” power transformation (e.g., square root, log, reciprocal) based on the data itself. It’s especially helpful when you’re unsure which transformation is appropriate. Like log and square root transformations, Box-Cox can improve linearity and reduce the impact of outliers.\nWhen we compare models using different transformations, we often look at goodness-of-fit measures like R-squared, adjusted R-squared, and AIC. These metrics help us assess whether the transformation improves model fit while avoiding overfitting. In general, a higher adjusted R-squared and a lower AIC suggest a better-fitting model.\nWe won’t go deep into the code here, but it’s important to know that transformations are a standard part of the data science toolkit. They help models perform better when relationships are nonlinear or when extreme values distort the picture. The log transformation is one example—but not the only one—of how transforming variables can lead to more meaningful and interpretable results.\nWinsorizing\nWinsorizing is another technique to reduce the impact of extreme values, especially when we don’t want to throw out data points entirely. Unlike trimming (which removes outliers), Winsorizing caps them at a specified percentile. For example, using the 95th percentile replaces all values above it with the 95th percentile value.\nThis can help tame the influence of outliers in a way that’s less aggressive than deletion and doesn’t distort model assumptions as much as leaving extreme values untouched. It’s especially useful when you suspect that very high or low values are distorting the fit of your regression.\nLet’s Winsorize our GDP data at the 95th percentile using the winsorize() function from the datawizard package. This will cap extreme values at the 95th percentile, reducing their influence without removing them entirely.\n\nlibrary(datawizard)\n\n# Winsorize the data\nmodel_data_winsorized &lt;- model_data |&gt;\n  mutate(\n    wealth_win95 = winsorize(wealth, threshold = 0.05)\n  )\n\nNow let’s visualizing the winsorizing effects.\n\n# Show the effect of winsorizing on raw GDP data\noriginal_plot &lt;- ggplot(model_data, aes(x = wealth)) +\n  geom_histogram(bins = 30, alpha = 0.7, fill = \"darkblue\") +\n  scale_x_continuous(labels = scales::label_dollar(suffix = \"k\")) +\n  labs(title = \"Original Distribution\", x = \"GDP per Capita\") +\n  theme_minimal()\n\nwinsorized_plot &lt;- ggplot(model_data_winsorized, aes(x = wealth_win95)) +\n  geom_histogram(bins = 30, alpha = 0.7, fill = \"darkred\") +\n  scale_x_continuous(labels = scales::label_dollar(suffix = \"k\")) +\n  labs(title = \"Winsorized Distribution\", x = \"GDP per Capita\") +\n  theme_minimal()\n\noriginal_plot | winsorized_plot\n\n\n\n\n\n\n\nHere we clearly see how winsorizing reduces the impact of extreme values. The histogram on the left shows the original distribution with its long right tail, while the winsorized histogram on the right caps those extreme values, making the distribution a bit more symmetric.\nRobust Regression\nOne last approach we will briefly explore is robust regression. This technique is designed to be less sensitive to outliers than ordinary least squares (OLS) regression. It uses different loss functions that reduce the influence of extreme values, making it a good choice when you have outliers that you do not want to remove or transform. We can perform robust regression using the rlm() function from the MASS package, which implements a robust version of linear regression.\n\n# Robust regression is less sensitive to outliers\nlibrary(MASS)\n\nrobust_model &lt;- rlm(lib_dem ~ log(wealth), data = model_data)\n\nsummary(robust_model)\n\n\nCall: rlm(formula = lib_dem ~ log(wealth), data = model_data)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60880 -0.15368  0.03422  0.17024  0.37024 \n\nCoefficients:\n            Value  Std. Error t value\n(Intercept) 0.1178 0.0385     3.0619 \nlog(wealth) 0.1311 0.0149     8.8195 \n\nResidual standard error: 0.2429 on 172 degrees of freedom\n\n\nHere we see that the coefficients and significance levels are similar to our original OLS model, but we can be more confident that the results are not overly influenced by the outliers we identified earlier.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nTry removing the influential points in the polarization model that you created earlier.\nRerun the model with those influential points removed and compare the results to the original model.\nHow do the results change? Do you notice any differences in the coefficients or significance levels?\nNow try winsorizing the polarization data at the 95th percentile and rerunning the model. How does this affect the results?\nFinally, try running a robust regression on the polarization data. How do the results compare to the original model and the winsorized model?",
    "crumbs": [
      "Course Modules",
      "Module 10.2"
    ]
  },
  {
    "objectID": "modules/module-10.2.html#making-the-right-choice",
    "href": "modules/module-10.2.html#making-the-right-choice",
    "title": "Module 10.2",
    "section": "Making the Right Choice",
    "text": "Making the Right Choice\nWhen dealing with outliers and influential observations, the most important step is to understand their nature and context. Outliers caused by data entry errors or coming from clearly different populations (like microstates in a global dataset) may justify removal. But many outliers are legitimate and may be the most substantively interesting cases in your data. In those situations, it’s often better to retain them and use transformations (like logging or Box-Cox) to reduce their influence, especially when the data are naturally skewed. Winsorizing offers a middle ground—keeping all observations while capping extreme values—particularly useful in predictive modeling or when you suspect noise in the extreme tails.\nWhatever approach you take—removal, transformation, winsorizing, or none at all—it’s essential to report your decisions clearly and check how they affect your results. Describe how you identified outliers, explain your reasoning for handling them in a specific way, and conduct a brief sensitivity analysis to see if your main conclusions change. Outlier handling is not just a technical step; it’s a modeling decision that should be transparent and justifiable to your audience.",
    "crumbs": [
      "Course Modules",
      "Module 10.2"
    ]
  },
  {
    "objectID": "modules/module-2.1.html",
    "href": "modules/module-2.1.html",
    "title": "Module 2.1",
    "section": "",
    "text": "Prework\n\n\n\n\nHave a look at the documentation for ggplot2\n\nFamiliarize yourself with the ggplot2 cheatseet\n\nGenerate a quarto document named “module-2.qmd” in youe modules project folder so that you can code along with me\n\nIf you have installed the Tidyverse, then you should already have the packages for this model, including ggplot2. You can go ahead and load ggplot2 along with readr and dplyr.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nNote that you could also load these three packages by running library(tidyverse). However, it is good to be intentional about which packages we are loading as we are learning them.",
    "crumbs": [
      "Course Modules",
      "Module 2.1"
    ]
  },
  {
    "objectID": "modules/module-2.1.html#overview",
    "href": "modules/module-2.1.html#overview",
    "title": "Module 2.1",
    "section": "Overview",
    "text": "Overview\nLast week we learned how to gather and wrangle data. This week we are going to start visualizing it with the ggplot2. In this module we will learn to make bar charts and histograms.\nAlong the way we are going to be talking about the “grammar of graphics” that ggplot2 is based on. The “gg” in ggplot stands for “grammar of graphics.” The grammar of graphics is a layered approach to constructing graphs based on a book by Leland Wilkinson.\nThe idea is that each visualization you make is going to contain cerain elements. You will start with some data. Then you will incorporate some “aesthetics” which you can think of as the dimensions of the visualization (x-axis, y-axis and color, size or shapes for additional dimensions). Next you identify a geometric obejct that you want to use such as a bar, a line or a point. From there you can customize various elements of the plot like the title and axis scales and labels.",
    "crumbs": [
      "Course Modules",
      "Module 2.1"
    ]
  },
  {
    "objectID": "modules/module-2.1.html#bar-charts",
    "href": "modules/module-2.1.html#bar-charts",
    "title": "Module 2.1",
    "section": "Bar charts",
    "text": "Bar charts\n\nLet’s get started with our first visualization–a basic bar chart. Bar charts are good for comparing data across cases. Our aim here is going to be to summarize levels of democracy across different regions like we did in the last lesson, but this time we will illustrate the differences with a chart.\nWe will start by loading in the dem_summary.csv file that you can find here. Next we will do our first ggplot() call. The ggplot() function takes two arguments: data and mapping. data refers to the data frame that includes the variables we want to visualize and mapping refers to the aesthetics mappings for the visualization. The aesthetics mappings are themselves presented in a quoting function aes() that defines the x and y values of the plot along with other aesthetic values like fill, color and linetype. We will focus on x and y values here and return to these additional aesthetic values later.\nAfter our ggplot() call, we can add a series of additional functions to define our visualization following a + sign. The most important group are the geoms which will define the basic type of plot we want to make. In this case, we are calling geom_col() for our histogram and specifying that the fill color should be “steelblue.”\nFrom there we will further customize our visualization with the labs() function to provide a title, axis labels and a caption.\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + # ggplot call\n  geom_col(fill = \"steelblue\") + # we use geom_col() for a a bar chart\n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\nThis looks pretty good but frequently we would want the bars of our bar chart to be sorted in order of the values being displayed. Let’s go ahead and add the fct_reorder() from the forcats package to our aes() call so that we are reordering the bars based on descending values of the average polyarchy score.\n\nlibrary(forcats)\n\nggplot(dem_summary, aes(x = fct_reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )",
    "crumbs": [
      "Course Modules",
      "Module 2.1"
    ]
  },
  {
    "objectID": "modules/module-2.1.html#histograms",
    "href": "modules/module-2.1.html#histograms",
    "title": "Module 2.1",
    "section": "Histograms",
    "text": "Histograms\n\nNow let’s do another ggplot() call to make a histogram. We use histograms when we want to show how our data are distributed.\nWe’ll start by reading in the dem_women.csv file that you can download here. From there, we call ggplot(), specifying the polyarchy score on x-axis. But this time we change the geom to geom_histogram(). We also change the title and axis labels to reflect the fact that we are plotting the number of cases falling in each bin.\n\n\n\n\n\n\nNote\n\n\n\nNote that we leave the y-axis blank for the histogram because ggplot will automatically know to plot the number of units in each bin on the y-axis.\n\n\n\ndem_women_2015 &lt;- read_csv(\"data/dem_women.csv\") |&gt; \n  filter(year == 2015) \n\nggplot(dem_women_2015, aes(x = polyarchy)) + # only specify x for histogram\n  geom_histogram(fill = \"steelblue\") + # geom is a histogram\n  labs(\n    x = \"Polyarchy Score, 2015\", \n    y = \"Count\",\n    title = \"Distribution of democracy, 2015\", \n    caption = \"Source: V-Dem Institute\"\n    )",
    "crumbs": [
      "Course Modules",
      "Module 2.1"
    ]
  },
  {
    "objectID": "modules/module-4.1.html",
    "href": "modules/module-4.1.html",
    "title": "Module 4.1",
    "section": "",
    "text": "Introduction to the RStudio interface and script execution\nPractices for running and modifying R code\nUse of the pipe operator (|&gt;) for chaining operations\nInstallation and loading of essential packages like tidyverse\nBasic data manipulation functions like read.csv() and head()\n\n\n\n\n\nUnderstanding Quarto’s integration with R and its benefits for reproducibility\nDetailed instruction on Markdown for text formatting, including headers, lists, and links\nEmbedding R code within Quarto using code chunks\nSetting chunk options for better control over code execution and output\n\n\n\n\n\nOverview of Leland Wilkinson’s grammar of graphics as implemented in ggplot2\nIntroduction to ggplot() function and its parameters\nUnderstanding aesthetic mappings using aes() function\nUtilization of geom_bar() for bar chart creation\nConstructing histograms using geom_histogram()\nCustomizing charts with labels, colors, and themes\n\n\n\n\n\nTechniques for creating line charts using geom_line()\nBuilding scatter plots with geom_point() and applying color scales and themes\nAdding layers and annotations\nIntroduction to interactive graphs with plotly\nConsiderations for color blindness and visual accessibility in data visualization\n\n\n\n\n\nTechniques for importing and cleaning data in R\nDiscussion on principles of tidy data and its importance\nMethods for retrieving data from APIs\nUtilizing filter(), select(), and mutate() for data manipulation\nUnderstanding logical operators for data filtering\n\n\n\n\n\nTechniques for using group_by() and summarize() functions to aggregate data\nApplying arrange() for sorting data frames and using desc() for descending order\nIntroduction to common functions for summarizing data like mean(), median(), and sd()\nStrategies for dealing with common errors and warnings in R",
    "crumbs": [
      "Course Modules",
      "Module 4.1"
    ]
  },
  {
    "objectID": "modules/module-4.1.html#data-viz-review",
    "href": "modules/module-4.1.html#data-viz-review",
    "title": "Module 4.1",
    "section": "",
    "text": "Introduction to the RStudio interface and script execution\nPractices for running and modifying R code\nUse of the pipe operator (|&gt;) for chaining operations\nInstallation and loading of essential packages like tidyverse\nBasic data manipulation functions like read.csv() and head()\n\n\n\n\n\nUnderstanding Quarto’s integration with R and its benefits for reproducibility\nDetailed instruction on Markdown for text formatting, including headers, lists, and links\nEmbedding R code within Quarto using code chunks\nSetting chunk options for better control over code execution and output\n\n\n\n\n\nOverview of Leland Wilkinson’s grammar of graphics as implemented in ggplot2\nIntroduction to ggplot() function and its parameters\nUnderstanding aesthetic mappings using aes() function\nUtilization of geom_bar() for bar chart creation\nConstructing histograms using geom_histogram()\nCustomizing charts with labels, colors, and themes\n\n\n\n\n\nTechniques for creating line charts using geom_line()\nBuilding scatter plots with geom_point() and applying color scales and themes\nAdding layers and annotations\nIntroduction to interactive graphs with plotly\nConsiderations for color blindness and visual accessibility in data visualization\n\n\n\n\n\nTechniques for importing and cleaning data in R\nDiscussion on principles of tidy data and its importance\nMethods for retrieving data from APIs\nUtilizing filter(), select(), and mutate() for data manipulation\nUnderstanding logical operators for data filtering\n\n\n\n\n\nTechniques for using group_by() and summarize() functions to aggregate data\nApplying arrange() for sorting data frames and using desc() for descending order\nIntroduction to common functions for summarizing data like mean(), median(), and sd()\nStrategies for dealing with common errors and warnings in R",
    "crumbs": [
      "Course Modules",
      "Module 4.1"
    ]
  },
  {
    "objectID": "modules/module-6.2.html",
    "href": "modules/module-6.2.html",
    "title": "Module 6.2",
    "section": "",
    "text": "Prework\n\n\n\nComplete the following before diving into this module:\n\nInstall the tidymodels package. This includes the infer and rsample packages, which we will use to simulate sampling and construct confidence intervals.\n\ninstall.packages(\"tidymodels\")\n\n\nInstall the openintro package, which contains the dataset we will use in this module: install.packages(\"openintro\")\n\nReview the documentation for the infer package and the rsample package.\nThink about sampling. What does the word “sample” mean to you? What is a sample you have encountered in your own life? What larger group was that sample meant to represent?",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-6.2.html#overview",
    "href": "modules/module-6.2.html#overview",
    "title": "Module 6.2",
    "section": "Overview",
    "text": "Overview\nIn this module, we will explore the concept of statistical inference, which is the process through which we use samples to make informed guesses about a broader population. Since we rarely have access to data from every individual or item in a population, we must rely on samples to estimate quantities of interest, like proportions or averages.\nBut sampling introduces uncertainty. What if your sample isn’t typical? How much might your estimate differ if you sampled again?\nWe’ll build an understanding of what it means to sample from a population, how repeated sampling leads to a sampling distribution, and how to quantify uncertainty using standard errors and confidence intervals. We explore two ways to estimate uncertainty: 1) using mathematical formulas; and 2) using a computational approach called bootstrapping.\nAlong the way, you’ll simulate sampling, compute estimates, and build confidence intervals with R. By the end, you’ll be able to explain what a confidence interval means and construct one from data using the infer package.\nLet’s begin by understanding what sampling is, and why it’s essential to doing data science.",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-6.2.html#what-is-sampling",
    "href": "modules/module-6.2.html#what-is-sampling",
    "title": "Module 6.2",
    "section": "What is Sampling?",
    "text": "What is Sampling?\nImagine trying to learn something about a large group of people, like how many hours college students sleep each night. You could try asking every student on Earth, but that’s not very practical. Instead, you look at a smaller group that hopefully represents the whole or, in other words, you take a sample.\nThis is the core idea of sampling: selecting a subset of individuals from a larger population to learn something about the population as a whole. The population is the full group you’re interested in studying. The sample is a smaller subset of the population that you actually observe.\nRelated to this, we have two important concepts: a parameter and a statistic. A parameter is a true, but usually unknown, characteristic of the population (like the actual average sleep time). A statistic is the number you compute from your sample (like the sample average sleep time).\nYour goal as a data scientist is to use statistics to make educated guesses about parameters. This process is what we refer to as inference.\nThe Target Population and Sampling Frame\nIn practice, it’s important to define your target population—the group you want to learn about. Then you have to find a sampling frame, which is the actual list or method you use to select your sample.\nFor example:\n\n\nTarget population: All high school students in the U.S.\n\nSampling frame: Students enrolled in a particular school district’s database\n\nOften, the sampling frame doesn’t perfectly match the target population. This mismatch can introduce sampling bias, which we’ll revisit in a future module.\nNow that you understand what sampling is, let’s explore what happens when we repeat the sampling process.\nSampling Distributions and Uncertainty\nIf you take a single sample from a population and compute a statistic—say, the average—what do you get? One answer. But what if you took a different sample? Would your answer be the same? Probably not.\nThis variability is the heart of sampling distributions: the idea that every time you take a sample and compute a statistic, the result can change. If you repeat the process many times, those statistics themselves form a distribution.\n\n\n\n\n\n\nActivity: Sampling with M&Ms\n\n\n\nImagine you have a big bowl of M&Ms in front of you. You reach in and grab 20 at random. Count how many are blue and calculate the proportion. Now, put them all back and grab 20 more. Do it again. And again.\nEach time you grab a handful, you’re taking a sample. Because you’re putting them back in the bowl each time, this is called sampling with replacement. You’ll notice the proportion of blue M&Ms changes slightly from sample to sample.\nTry this at home if you have M&M fun packs where each fun pack represents a random sample of M&Ms. Record the proportion of blue M&Ms (your sample statistic) in a CSV file. This is your sampling distribution. You can even upload your CSV to R and use your data to create a histogram of the proportions.\nNow take an average of all of your recorded sample statistics. How close does it come to the actual known population parameter (the proportion of M&Ms that are blue)?\nThis simple activity mirrors what we mean by repeated sampling and helps you build intuition for how a sampling distribution works.",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-6.2.html#standard-error-measuring-the-spread",
    "href": "modules/module-6.2.html#standard-error-measuring-the-spread",
    "title": "Module 6.2",
    "section": "Standard Error: Measuring the Spread",
    "text": "Standard Error: Measuring the Spread\nThe spread of this sampling distribution is called the standard error (SE). It tells us, on average, how much a statistic varies from one sample to another. A smaller SE means more precise estimates while a larger SE means more variability from sample to sample.\nStandard errors depend on both the sample size and the variability in the data. Bigger samples tend to produce smaller standard errors.\nUnderstanding this variability is key to making smart inferences. Next, we’ll learn how to use this idea to construct confidence intervals—a powerful way to express uncertainty in our estimates.",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-6.2.html#estimating-uncertainty-with-confidence-intervals",
    "href": "modules/module-6.2.html#estimating-uncertainty-with-confidence-intervals",
    "title": "Module 6.2",
    "section": "Estimating Uncertainty with Confidence Intervals",
    "text": "Estimating Uncertainty with Confidence Intervals\nA confidence interval gives us a range of values within which we believe the true population parameter likely falls. It’s based on the idea of the sampling distribution and its standard error.\n\nWhen we interpret a confidence interval we say that we are “X% confident” that the true parameter lies within the interval. For example, if we say we are 95% confident that the true proportion of M&Ms that are blue is between 0.2 and 0.3, it means that if we repeated our sampling many times, about 95% of those intervals would contain the true proportion.\nThere are two distinct ways to calculate confidence intervals: using mathematical formulas or through computational methods like bootstrapping. Most of our focus in this course is going to be on the computational approach, or bootstrapping, but it’s important to understand the math behind confidence intervals as well.",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-6.2.html#math-based-confidence-intervals",
    "href": "modules/module-6.2.html#math-based-confidence-intervals",
    "title": "Module 6.2",
    "section": "Math-Based Confidence Intervals",
    "text": "Math-Based Confidence Intervals\nConfidence intervals can be calculated using formulas based on the sampling distribution of the statistic. The most common type is the normal approximation method, which assumes that the sampling distribution of the sample proportion is approximately normal when the sample size is large enough.\nHere’s the basic formula for a confidence interval:\n\\[\n\\text{Estimate} \\pm z \\times \\text{Standard Error}\n\\]\nThe \\(z\\) value depends on how confident you want to be:\n\nFor a 95% confidence level, \\(z \\approx 1.96\\)\n\nFor a 90% confidence level, \\(z \\approx 1.645\\)\n\n\nExample: Confidence Interval for a Proportion\nSuppose we survey 100 people, and 64 say they like pineapple on pizza. Our sample proportion \\(\\hat{p}\\) is 0.64.\nWe estimate the standard error using:\n\\[\nSE = \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\]\nPlugging in the numbers:\n\\[\nSE = \\sqrt{ \\frac{0.64 \\times 0.36}{100} } \\approx 0.048\n\\]\nThe 95% confidence interval is:\n\\[\n0.64 \\pm 1.96 \\times 0.048 = (0.546, 0.734)\n\\]\nWe are 95% confident that between 54.6% and 73.4% of the population likes pineapple on pizza.\nIn the next section, we’ll explore a more flexible approach: bootstrapping.",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-6.2.html#estimating-uncertainty-with-bootstrapping",
    "href": "modules/module-6.2.html#estimating-uncertainty-with-bootstrapping",
    "title": "Module 6.2",
    "section": "Estimating Uncertainty with Bootstrapping",
    "text": "Estimating Uncertainty with Bootstrapping\nBootstrapping is a resampling technique that involves repeatedly drawing samples, with replacement, from the observed data to estimate the sampling distribution of a statistic. Bootstrapping is an example of a nonparametric approach to inference. Nonparametric means that we do not assume a specific shape or distribution for the population (like normality). Instead of relying on formulas, we use the data we have to approximate what repeated sampling might look like.\n\nBootstrapping is a computational method for estimating the variability of a statistic when you only have one sample from the population. It works by simulating what might happen if you could sample again and again—from your existing data.\nTo understand the idea, imagine that your original sample is the “best guess” we have of the population. If we randomly resample with replacement from that sample, we can simulate what other samples might have looked like.\nBy computing the same statistic (e.g., a proportion or mean) from each resample, we build up a bootstrap distribution. From this, we can estimate standard errors and construct confidence intervals.\nThere are a number of good reasons to use bootstrapping:\n\nIt doesn’t require formulas or assumptions about the shape of the distribution;\nIt works well even with small sample sizes or skewed data;\nIt’s easy to implement using R and the infer package.\n\nWorked Example Using openintro and tidymodels\n\nLet’s use a dataset from the openintro package. In this Pew Research survey, 506 Russians were asked whether they believe their country tried to interfere in the 2016 U.S. presidential election. We’ll recode the responses into a binary variable.\n\n# Load packages\nlibrary(openintro)\nlibrary(tidyverse)\n\n# Load and inspect data\nglimpse(russian_influence_on_us_election_2016)\n\nRows: 506\nColumns: 1\n$ influence_2016 &lt;chr&gt; \"Did not try\", \"Did not try\", \"Did not try\", \"Don't kno…\n\n# Recode as binary variable\nrussiaData &lt;- russian_influence_on_us_election_2016 |&gt; \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))\n\n# Summary stats\nrussiaData |&gt; \n  summarize(mean = mean(try_influence), sd = sd(try_influence))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.150 0.358\n\n\nThis tells us the observed proportion who believe Russia tried to influence the election.\nNext, we will create the bootstrap distribution:\n\nlibrary(tidymodels)\n\nset.seed(66)\n\nboot_dist &lt;- russiaData |&gt;\n  specify(response = try_influence) |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\nNow, let’s use get_ci() and visualize() from the infer package to compute and visualize the confidence interval for our bootstrap distribution:\n\n# Get confidence interval\nci &lt;- boot_dist |&gt; get_ci(level = 0.95)\n\n# Visualize bootstrap distribution with CI\nboot_dist |&gt;\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Means of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nThis plot shows the bootstrap distribution of the mean proportion of Russians who believe their country interfered in the election. The shaded region marks the 95% confidence interval.\n\n\n\n\n\n\nWarning\n\n\n\nThe 95% confidence interval was calculated as (lower_bound, upper_bound). Which of the following best describes the correct interpretation?\n\n\n95% of the time the percentage of Russians who believe that Russia interfered is between these values.\n\n\n95% of all Russians believe the probability of interference is within the interval.\n\n\nWe are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 election is between these values.\n\n\nWe are 95% confident that the proportion of Russians who supported interference is between these values.\n\n\n\nCode# The answer is (c) We are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 election is between these values.",
    "crumbs": [
      "Course Modules",
      "Module 6.2"
    ]
  },
  {
    "objectID": "modules/module-1.2.html",
    "href": "modules/module-1.2.html",
    "title": "Module 1.2",
    "section": "",
    "text": "Once you have R, R Studio and Quarto installed, you are ready to start integrating text and code with Quarto. Quarto is an open source publishing platform that enables you to integrate text with code. If you have used R Markdown before then Quarto will look familiar to you because Quarto is the next generation of R Markdown.\nRStudio comes with a version of Quarto already installed, but it can be useful to install the most recent version separately and because doing so will allow you to use Quarto with another IDE like VS Code. You can install the most recent version of Quarto by visiting this page and selecting the version for your operating system.\nNow take a little time to create a Quarto project in R Studio and make sure everything is working properly. But before you get started, create a new folder(directory) for this course on your computer somewhere. Once that is done, go to File &gt; New Project. Then select Quarto Project and name the project something like “test-project” in the Directory name field. Next, select Browse and navigate to the folder that you created for this course. Select Create Project.\nYou will notice that in your new project folder there is a file with an .Rproj extension. The .Rproj file is what tells RStudio which files are associated with the project and it obviates the need to set the working directory. It also makes it possible to share the folder with anyone who is running R and RStudio and have them run your code without having to set a working directory. This is what we refer to as a project-based workflow and we will use it for every assignment in this class.\nNow try rendering the document with the Render toggle button. By default, Quarto renders an .html file that it will open in a browser and save to your project folder.\nNext we want to try rendering a .pdf document. To do this, we have to install tinytex, a lightweight version of LaTeX. To do this, go to the Terminal and type quarto install tinytex. Now, change the format from html to pdf by inserting format: pdf in the YAML header. Then render the document again. A .pdf document should open up.\nNow take a few minutes and try changing more of the code in the YAML header. You can try changing the title, adding a subtitle (subtitle:) or changing the execution options. By default, Quarto uses the visual editor but behind the scenes it is using Markdown. Try and edit some text using the toggle buttons available in the visual editor and then switch to Source to view the underlying Markdown code. Play with the R code chunks embedded in the document or try adding new code chunks.\nYou may already have some experience writing in Markdown, which is a lightweight markup language that enables you to format plaintext files. If you have not used Markdown, or if your memory is hazy, don’t worry: it is really easy to learn. Have a look at this Markdown cheat sheet and try to familiarize yourself with its basic syntax. Finally, take some time to get familiar with the Guide and Reference sections of the Quarto website. Then take a look at the gallery so that you can get an idea of the kinds of things you can produce with Quarto.",
    "crumbs": [
      "Course Modules",
      "Module 1.2"
    ]
  },
  {
    "objectID": "modules/module-1.2.html#quarto",
    "href": "modules/module-1.2.html#quarto",
    "title": "Module 1.2",
    "section": "",
    "text": "Once you have R, R Studio and Quarto installed, you are ready to start integrating text and code with Quarto. Quarto is an open source publishing platform that enables you to integrate text with code. If you have used R Markdown before then Quarto will look familiar to you because Quarto is the next generation of R Markdown.\nRStudio comes with a version of Quarto already installed, but it can be useful to install the most recent version separately and because doing so will allow you to use Quarto with another IDE like VS Code. You can install the most recent version of Quarto by visiting this page and selecting the version for your operating system.\nNow take a little time to create a Quarto project in R Studio and make sure everything is working properly. But before you get started, create a new folder(directory) for this course on your computer somewhere. Once that is done, go to File &gt; New Project. Then select Quarto Project and name the project something like “test-project” in the Directory name field. Next, select Browse and navigate to the folder that you created for this course. Select Create Project.\nYou will notice that in your new project folder there is a file with an .Rproj extension. The .Rproj file is what tells RStudio which files are associated with the project and it obviates the need to set the working directory. It also makes it possible to share the folder with anyone who is running R and RStudio and have them run your code without having to set a working directory. This is what we refer to as a project-based workflow and we will use it for every assignment in this class.\nNow try rendering the document with the Render toggle button. By default, Quarto renders an .html file that it will open in a browser and save to your project folder.\nNext we want to try rendering a .pdf document. To do this, we have to install tinytex, a lightweight version of LaTeX. To do this, go to the Terminal and type quarto install tinytex. Now, change the format from html to pdf by inserting format: pdf in the YAML header. Then render the document again. A .pdf document should open up.\nNow take a few minutes and try changing more of the code in the YAML header. You can try changing the title, adding a subtitle (subtitle:) or changing the execution options. By default, Quarto uses the visual editor but behind the scenes it is using Markdown. Try and edit some text using the toggle buttons available in the visual editor and then switch to Source to view the underlying Markdown code. Play with the R code chunks embedded in the document or try adding new code chunks.\nYou may already have some experience writing in Markdown, which is a lightweight markup language that enables you to format plaintext files. If you have not used Markdown, or if your memory is hazy, don’t worry: it is really easy to learn. Have a look at this Markdown cheat sheet and try to familiarize yourself with its basic syntax. Finally, take some time to get familiar with the Guide and Reference sections of the Quarto website. Then take a look at the gallery so that you can get an idea of the kinds of things you can produce with Quarto.",
    "crumbs": [
      "Course Modules",
      "Module 1.2"
    ]
  },
  {
    "objectID": "modules/module-1.1.html",
    "href": "modules/module-1.1.html",
    "title": "Module 1.1",
    "section": "",
    "text": "All of our work for this course will be done in the R language and we will be working with R in RStudio. RStudio is an integrated development environment (IDE) develop by a company named Posit. Please be sure to download and install the most recent versions of R and R Studio from Posit’s website.\nIt is a good idea to periodically update R and RStudio. RStudio will prompt you when it is time to update and you can follow the same process of downloading and installing from the Posit website that we just did here. For R, there are a number of ways to do it, but the easiest is to use packages like installr for Windows and updateR for Mac. Here is a good blog post that walks you through the steps of how to update R using these packages. I usually update R once a semester.\nWe are going to be using a number of R packages throughout the course. One essential set of packages are those that comprise the Tidyverse, but especially readr, dplyr, ggplot2 and tidyr. You can install the entire Tidyverse collection of packages by typing install.packages(\"tidyverse\") in your console. We will talk about these packages in detail as we go through the course, but have a look at this basic description now to gain some basic familiarity.\nAnother thing that you really want to do is to make sure that you have the native pipe operator (|&gt;) enabled. In RStudio, go to Tools&gt;Global Options, then go to Code and select “Use native pipe operator.”\nWhile you are here, you can also go to Appearance to select a different editor theme or to Pane Layout to change how the four panes in R Studio are organized. Next, familiarize yourself with how to expand and minimize the four windows. The most important window that I want to highlight here is the source window. This is where we are going to be working most of the time in this course. And if I tell you to send your source code, I mean to send the file that you are working on in this window. This could be a Quarto document, an R script or an app.R file for a Shiny app.\nThe next window is the Console and there we also see tabs for Terminal and Background Jobs. The console is where you can run R code one line at a time. The terminal is relevant for more advanced users and we will make some use of it when we talk about publishing Quarto documents. Background Jobs is going to be helpful when we want troubleshoot a Quarto document that is not rendering properly.\nFrom there, the next pane we want to explore is Environment, History, etc. Environment tells us what files are currently available to us. The other important tab here is Git which will be where we push things to GitHub. You will be using this a lot in the course.\nFinally, we see a pane with Files, Plots, Packages etc. Files tells us what files are in our project folder and enables us to copy, and delete files associated with our project. Plots is a window for viewing our visualizations. And Packages shows us what packages are available and loaded into our environment.\nBefore you move on to the next section, take some time to familiarize yourself with the various user-friendly buttons and shortcuts available to you like the drop down menu for the pane layout, a spell checker, a button for inserting a code chunk and other features that you can play around with.",
    "crumbs": [
      "Course Modules",
      "Module 1.1"
    ]
  },
  {
    "objectID": "modules/module-1.1.html#sec-rstudio-setup",
    "href": "modules/module-1.1.html#sec-rstudio-setup",
    "title": "Module 1.1",
    "section": "",
    "text": "All of our work for this course will be done in the R language and we will be working with R in RStudio. RStudio is an integrated development environment (IDE) develop by a company named Posit. Please be sure to download and install the most recent versions of R and R Studio from Posit’s website.\nIt is a good idea to periodically update R and RStudio. RStudio will prompt you when it is time to update and you can follow the same process of downloading and installing from the Posit website that we just did here. For R, there are a number of ways to do it, but the easiest is to use packages like installr for Windows and updateR for Mac. Here is a good blog post that walks you through the steps of how to update R using these packages. I usually update R once a semester.\nWe are going to be using a number of R packages throughout the course. One essential set of packages are those that comprise the Tidyverse, but especially readr, dplyr, ggplot2 and tidyr. You can install the entire Tidyverse collection of packages by typing install.packages(\"tidyverse\") in your console. We will talk about these packages in detail as we go through the course, but have a look at this basic description now to gain some basic familiarity.\nAnother thing that you really want to do is to make sure that you have the native pipe operator (|&gt;) enabled. In RStudio, go to Tools&gt;Global Options, then go to Code and select “Use native pipe operator.”\nWhile you are here, you can also go to Appearance to select a different editor theme or to Pane Layout to change how the four panes in R Studio are organized. Next, familiarize yourself with how to expand and minimize the four windows. The most important window that I want to highlight here is the source window. This is where we are going to be working most of the time in this course. And if I tell you to send your source code, I mean to send the file that you are working on in this window. This could be a Quarto document, an R script or an app.R file for a Shiny app.\nThe next window is the Console and there we also see tabs for Terminal and Background Jobs. The console is where you can run R code one line at a time. The terminal is relevant for more advanced users and we will make some use of it when we talk about publishing Quarto documents. Background Jobs is going to be helpful when we want troubleshoot a Quarto document that is not rendering properly.\nFrom there, the next pane we want to explore is Environment, History, etc. Environment tells us what files are currently available to us. The other important tab here is Git which will be where we push things to GitHub. You will be using this a lot in the course.\nFinally, we see a pane with Files, Plots, Packages etc. Files tells us what files are in our project folder and enables us to copy, and delete files associated with our project. Plots is a window for viewing our visualizations. And Packages shows us what packages are available and loaded into our environment.\nBefore you move on to the next section, take some time to familiarize yourself with the various user-friendly buttons and shortcuts available to you like the drop down menu for the pane layout, a spell checker, a button for inserting a code chunk and other features that you can play around with.",
    "crumbs": [
      "Course Modules",
      "Module 1.1"
    ]
  },
  {
    "objectID": "modules/module-1.1.html#running-and-modifying-code",
    "href": "modules/module-1.1.html#running-and-modifying-code",
    "title": "Module 1.1",
    "section": "Running and Modifying Code",
    "text": "Running and Modifying Code\nNext we are going to get our hands dirty by running and modifying some code. Create a folder for this class somewhere on your machine. Next, create a sub-folder called “classwork” and save week1-classwork.qmd in that folder.\nNow open the week1-classwork.qmd file in RStudio, which has code for 3 data viz activities: map making; democracy over time; and UN voting patterns.\nFollow the instructions in the .qmd file to update the code. Click Render to update your HTML output and examine. Complete as much as you can.\nAlternatively, you can create your own .qmd file and copy and paste the code below into your file.\n\nExample 1: Make a map!\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")\n\n\n\n\n\n\n\nExample 2: Plotting Democracy Over Time\n\n# For nice colors that work for color blind\ncbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nlibrary(vdemdata)\nlibrary(ggplot2)\nlibrary(tidyverse)\n# We will use the data called vdem\nvdem %&gt;%\n  # and then we are going to filter out and only use France and India\n  filter(country_name == \"France\" | country_name == \"India\") %&gt;%\n  # and then we are going to only use years since 1850\n  filter(year &gt; 1849) %&gt;% \n    # and then we are going to use this filtered data to make a plot\n    #with democracy on the y axis and year on the x\n    # we want this plot to use different colors for each country\n    ggplot(., aes(y = v2x_polyarchy, x = year, color=country_name)) +\n      geom_line() +\n      theme_minimal() +\n      xlab(\"Year\") +\n      ylab(\"Electoral Democracy Index\") +\n      ggtitle(\"Electoral Democracy, 1850-2022\") +\n      geom_hline(yintercept = .5, linetype = \"dashed\", color = \"grey\") +\n       scale_color_manual(name=\"Country\", values=c(\"#E69F00\", \"#56B4E9\")) +\n      ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\nExample 3: UN Voting Trends\n\n# = c(\"tidyverse\", \"lubridate\", \"scales\", \"DT\", \"unvotes\", \"pacman\")\n\nlibrary(tidyverse)\nlibrary(unvotes)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(pacman)\n\nunvotes &lt;- un_votes %&gt;%\n  inner_join(un_roll_calls, by = \"rcid\") %&gt;%\n  inner_join(un_roll_call_issues, by = \"rcid\")\n\nunvotes %&gt;%\n  # then filter out to only include the countries we want\n  filter(country %in% c(\"South Africa\", \"United States\", \"France\")) %&gt;%\n  # then make sure R understands the the year variable is a data\n  mutate(year = year(date)) %&gt;%\n  # Then group the data by country and year\n  group_by(country, year, issue) %&gt;%\n  # then take the average Yes votes for each country_year\n  summarize(percent_yes = mean(vote == \"yes\")) %&gt;%\n  # then make a nice plot\n  ggplot(mapping = aes(x = year, y = percent_yes, color = country)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  facet_wrap(~issue) +\n  scale_y_continuous(labels = percent) +\n  scale_color_manual( values=c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n  labs(\n    title = \"Percentage of 'Yes' votes in the UN General Assembly\",\n    subtitle = \"1946 to 2019\",\n    y = \"% Yes\",\n    x = \"Year\",\n    color = \"Country\"\n  )",
    "crumbs": [
      "Course Modules",
      "Module 1.1"
    ]
  },
  {
    "objectID": "modules/module-7.2.html",
    "href": "modules/module-7.2.html",
    "title": "Module 7.2",
    "section": "",
    "text": "In this module, you’ll learn how to test whether relationships exist between two variables using permutation tests. We’ll explore this through a case study examining potential racial discrimination in hiring practices, using data from résumés sent to employers with randomly assigned names. Here is a video that introduces the concept of using bootstrapping methods to evaluate discrimination but through the lense of gender:",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#overview",
    "href": "modules/module-7.2.html#overview",
    "title": "Module 7.2",
    "section": "",
    "text": "In this module, you’ll learn how to test whether relationships exist between two variables using permutation tests. We’ll explore this through a case study examining potential racial discrimination in hiring practices, using data from résumés sent to employers with randomly assigned names. Here is a video that introduces the concept of using bootstrapping methods to evaluate discrimination but through the lense of gender:",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#understanding-hypotheses-for-group-comparisons",
    "href": "modules/module-7.2.html#understanding-hypotheses-for-group-comparisons",
    "title": "Module 7.2",
    "section": "Understanding Hypotheses for Group Comparisons",
    "text": "Understanding Hypotheses for Group Comparisons\nWhen we want to determine whether a treatment or grouping variable has a real effect on an outcome, we need to set up two competing hypotheses. The null hypothesis states that there is no relationship between treatment and outcome, meaning any difference we observe is due to chance. The alternative hypothesis proposes that there is a genuine relationship, and the difference is not due to chance alone.\nThe key insight behind our approach is that under the null hypothesis, treatment has no impact on the outcome variable. This means that if we were to change the values of the treatment variable, the values on the outcome would stay the same. We can use this logic to simulate what we would expect to see if there truly was no effect.\nOur strategy involves reshuffling the treatment variable, calculating the treatment effect, and repeating this process many times. This allows us to ask a fundamental question: how likely would we be to observe the treatment effect in our data if there really is no effect of the treatment?",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#the-résumé-experiment",
    "href": "modules/module-7.2.html#the-résumé-experiment",
    "title": "Module 7.2",
    "section": "The Résumé Experiment",
    "text": "The Résumé Experiment\nTo illustrate these concepts, we’ll examine a study by Bertrand and Mullainathan that investigated racial discrimination in responses to job applications in Chicago and Boston. The researchers sent 4,870 résumés to potential employers, randomly assigning names associated with different racial groups to otherwise identical résumés.\n\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nresume_dta &lt;- resume",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#analyzing-callback-rates-by-race",
    "href": "modules/module-7.2.html#analyzing-callback-rates-by-race",
    "title": "Module 7.2",
    "section": "Analyzing Callback Rates by Race",
    "text": "Analyzing Callback Rates by Race\nSince race of applicant was randomly assigned in this experiment, any systematic differences in callback rates can be attributed to the racial associations of the names. Let’s examine the callback rates for each group:\n\nmeans &lt;- resume_dta |&gt;\n  group_by(race) |&gt; \n  summarize(calls = mean(received_callback))\n\nmeans\n\n# A tibble: 2 × 2\n  race   calls\n  &lt;chr&gt;  &lt;dbl&gt;\n1 black 0.0645\n2 white 0.0965\n\n\nWe can save these means for easier access and then calculate the treatment effect, which is simply the difference in means between the two groups:\n\nmean_white = means$calls[2]\nmean_black = means$calls[1]\n\nteffect &lt;- mean_white - mean_black\nteffect\n\n[1] 0.03203285",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#examining-the-data-with-confidence-intervals",
    "href": "modules/module-7.2.html#examining-the-data-with-confidence-intervals",
    "title": "Module 7.2",
    "section": "Examining the Data with Confidence Intervals",
    "text": "Examining the Data with Confidence Intervals\nBefore conducting formal hypothesis tests, it’s valuable to examine both the point estimates and their confidence intervals. This gives us a sense of the precision of our estimates and whether the observed differences might be meaningful.\nLet’s start by calculating the mean callback rates for each racial group:\n\n# Bootstrap CIs for black applicants\nci_black &lt;- resume_dta |&gt;\n  filter(race == \"black\") |&gt;\n  specify(response = received_callback) |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\") |&gt;\n  get_ci(level = 0.95)\n\n# Bootstrap CIs for white applicants  \nci_white &lt;- resume_dta |&gt;\n  filter(race == \"white\") |&gt;\n  specify(response = received_callback) |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\") |&gt;\n  get_ci(level = 0.95)\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nThese two code chunks create bootstrap confidence intervals for the callback rates of Black and White applicants in a resume audit study. specify() identifies the response variable of interest (received_callback), generate() creates 10,000 bootstrap resamples for each group, calculate(stat = \"mean\") computes the callback rate in each resample and get_ci() calculates a 95% confidence interval from these bootstrap statistics.\n\n\nNext, let’s put together the means and confidence intervals for both groups into a tibble so we can visualize them together:\n\n# Combine for plotting\nplot_dta &lt;- tibble(\n  race = c(\"black\", \"white\"),\n  mean_calls = c(mean_black, mean_white),\n  lower_95 = c(ci_black$lower_ci, ci_white$lower_ci),\n  upper_95 = c(ci_black$upper_ci, ci_white$upper_ci)\n)\n\nplot_dta\n\n# A tibble: 2 × 4\n  race  mean_calls lower_95 upper_95\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 black     0.0645   0.0550   0.0743\n2 white     0.0965   0.0846   0.109 \n\n\nNow we can use ggplot2 to create a visualization that shows both the callback rates and their uncertainty:\n\nggplot(plot_dta, aes(\n  y = mean_calls, \n  x = race, \n  ymin = lower_95, \n  ymax = upper_95\n  )) +\n  geom_col(fill = \"steelblue4\") +\n  geom_errorbar(width = .05) +\n  theme_bw()  +\n ylim(0, .15) +\n  labs(x = \"Race of Applicant\",\n       y = \"Call Back Rate\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nThis ggplot creates a bar chart with error bars to visualize callback rates by race. The geom_col() function creates bars showing the mean callback rates, while geom_errorbar() adds the 95% confidence intervals. The ymin and ymax aesthetics define the error bar endpoints. We use theme_bw() for a clean appearance and ylim() to set consistent y-axis limits for better comparison between groups.\n\n\nLooking at this plot, we can see clear differences between the groups and non-overlapping confidence intervals, which suggests there may be evidence of racial discrimination. But how can we formally test the null hypothesis to decide whether to reject it?",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#the-logic-of-permutation-testing",
    "href": "modules/module-7.2.html#the-logic-of-permutation-testing",
    "title": "Module 7.2",
    "section": "The Logic of Permutation Testing",
    "text": "The Logic of Permutation Testing\nTo conduct a formal hypothesis test, we need to simulate what would happen under the null hypothesis. Our approach involves calculating the difference in means between white and black applicants, then shuffling the race variable and calculating the difference in means for the shuffled data. By repeating this process many times, we can simulate the null distribution of differences in callbacks. This is a called a permutation test—a method where we simulate the null distribution by shuffling group labels (e.g., race) to see what differences in means we would expect if the treatment had no effect.",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#understanding-the-permutation-process",
    "href": "modules/module-7.2.html#understanding-the-permutation-process",
    "title": "Module 7.2",
    "section": "Understanding the Permutation Process",
    "text": "Understanding the Permutation Process\nLet’s walk through this process using a simplified hypothetical example with just six applicants:\nHypothetical Original Data\n\n\nApplicant\nRace\nCallback\n\n\n\nA\nBlack\nYes\n\n\nB\nBlack\nNo\n\n\nC\nBlack\nNo\n\n\nD\nWhite\nYes\n\n\nE\nWhite\nNo\n\n\nF\nWhite\nNo\n\n\n\nThe first step is to calculate the original difference in callback rates. This establishes our baseline understanding of the initial association between race and callback rates.\nThe second step involves shuffling or permuting the race variable. We randomly reassign race labels while keeping the callback outcomes exactly the same. This simulation reflects what we would expect to see if race truly had no effect on callbacks.\nHypothetical Shuffled Data\n\n\nApplicant\nRace (Shuffled)\nCallback\n\n\n\nA\nWhite\nYes\n\n\nB\nBlack\nNo\n\n\nC\nWhite\nNo\n\n\nD\nWhite\nYes\n\n\nE\nBlack\nNo\n\n\nF\nBlack\nNo\n\n\n\nAfter shuffling, we calculate the difference in callback rates again between the Black and White groups. This tells us what kind of difference we might observe purely due to chance.\nWe repeat this shuffling process thousands of times to generate a distribution of differences that could occur by chance alone. If our observed difference is extreme compared to this null distribution (meaning the p-value is low), we have strong evidence to reject the null hypothesis.",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#implementing-the-permutation-test",
    "href": "modules/module-7.2.html#implementing-the-permutation-test",
    "title": "Module 7.2",
    "section": "Implementing the Permutation Test",
    "text": "Implementing the Permutation Test\nIn practice, we use the tidymodels package to handle the computational details of this simulation. The infer package provides a clean workflow for permutation testing:\n\nnull_dist &lt;- resume_dta |&gt;\n  specify(response = received_callback, explanatory = race) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(10000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", \n            order = c(\"white\", \"black\"))\n\nOnce we have our null distribution, we can calculate the p-value using the get_pvalue function from the infer package:\n\nget_p_value(null_dist, obs_stat = teffect, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0001\n\n\nHere we get a p-value of exactly zero, suggesting that none of the simulated differences in means was as extreme as the observed difference. This indicates that the observed racial gap in callbacks is highly unlikely to have occurred by chance alone.\n\n\n\n\n\n\nNote\n\n\n\nYou may notice a message saying to “be careful of reporting a p-value of zero” and that this is an artifact of the number of reps chosen in the generate() step. If you increase the number of reps, you may get a p-value that is very close to zero but not exactly zero because the p-value is calculated as the proportion of simulated differences that are greater than or equal to the observed difference. But if you have the patience (or a fast computer), you can increase the number of reps to 100,000 or more and see what happens!",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#visualizing-the-results",
    "href": "modules/module-7.2.html#visualizing-the-results",
    "title": "Module 7.2",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nWe can visualize our null distribution along with our observed statistic to better understand our results:\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat = teffect, direction = \"greater\") +\n  labs(\n    x = \"Estimated Difference under the Null\",\n    y = \"Count\"\n  ) + \n  theme_bw()",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.2.html#drawing-conclusions",
    "href": "modules/module-7.2.html#drawing-conclusions",
    "title": "Module 7.2",
    "section": "Drawing Conclusions",
    "text": "Drawing Conclusions\nThe p-value we obtained is very small, well below the conventional 0.05 threshold. This means that if there were truly no racial discrimination, we would almost never observe a difference as large as what we found in the data. Therefore, we reject the null hypothesis and conclude that the racial gap is extremely unlikely to have occurred due to chance alone. This provides statistical evidence of racial discrimination in hiring.\n\n\n\n\n\n\nYour Turn!\n\n\n\nNow apply these same methods to investigate a different question using the same dataset:\n\nUse the gender variable in the resume data to assess whether there is gender discrimination in call backs\nPlot means and 95% confidence intervals for the call back rate for men and women\nWrite the null and alternative hypotheses\nSimulate the null distribution\nVisualize the null distribution and the gender gap\nCalculate the p-value\nWhat do you conclude from your test?",
    "crumbs": [
      "Course Modules",
      "Module 7.2"
    ]
  },
  {
    "objectID": "modules/module-7.1.html",
    "href": "modules/module-7.1.html",
    "title": "Module 7.1",
    "section": "",
    "text": "Tip\n\n\n\nBefore beginning this module, make sure to review:\n\nThe meaning of a proportion and sampling variability\nThe concepts of null and alternative hypotheses\nWhat p-values and significance levels represent",
    "crumbs": [
      "Course Modules",
      "Module 7.1"
    ]
  },
  {
    "objectID": "modules/module-7.1.html#prework",
    "href": "modules/module-7.1.html#prework",
    "title": "Module 7.1",
    "section": "",
    "text": "Tip\n\n\n\nBefore beginning this module, make sure to review:\n\nThe meaning of a proportion and sampling variability\nThe concepts of null and alternative hypotheses\nWhat p-values and significance levels represent",
    "crumbs": [
      "Course Modules",
      "Module 7.1"
    ]
  },
  {
    "objectID": "modules/module-7.1.html#motivation-a-claim-about-a-jobs-program",
    "href": "modules/module-7.1.html#motivation-a-claim-about-a-jobs-program",
    "title": "Module 7.1",
    "section": "Motivation: A Claim About a Jobs Program",
    "text": "Motivation: A Claim About a Jobs Program\nInternational development organizations sometimes run training programs to help people find employment. Suppose the national unemployment rate in a low-income country is 30%. One organization runs a jobs program and claims success because only 15 out of 60 participants are unemployed—a rate of 25%.\nIs this a meaningful improvement? Or could this difference just be due to random chance? This is where hypothesis testing comes in.\nTo begin, let’s simulate the data for this program in R:\n\nlibrary(tidyverse)\n\njobs_program &lt;- tibble(\n  outcome = c(rep(\"unemployed\", 15), rep(\"employed\", 45))\n)\n\njobs_program |&gt;\n  ggplot(aes(x = outcome)) +\n  geom_bar(fill = \"steelblue\") + theme_bw()",
    "crumbs": [
      "Course Modules",
      "Module 7.1"
    ]
  },
  {
    "objectID": "modules/module-7.1.html#understanding-hypothesis-testing",
    "href": "modules/module-7.1.html#understanding-hypothesis-testing",
    "title": "Module 7.1",
    "section": "Understanding Hypothesis Testing",
    "text": "Understanding Hypothesis Testing\nHypothesis testing is a statistical framework that allows us to make decisions about population parameters based on sample data. The process begins by establishing two competing hypotheses: the null hypothesis (\\(H_0\\)) and the alternative hypothesis (\\(H_A\\)). The null hypothesis represents the default position or “status quo”—it typically states that there is no effect, no difference, or that any observed pattern is simply due to random chance.\nYou can think of the null hypothesis as the skeptical position that says “nothing interesting is happening here.” The alternative hypothesis, in contrast, represents what we’re actually trying to find evidence for. It embodies our research question and claims that there is a real effect or meaningful difference that cannot be explained by random variation alone.\nThe logic of hypothesis testing follows a proof-by-contradiction approach. We begin by assuming that the null hypothesis is true, then examine our sample data to see whether it provides convincing evidence against this assumption. Specifically, we ask: “If the null hypothesis were actually true, how likely would it be to observe data like what we actually collected?” If our observed data would be reasonably likely under the null hypothesis, we fail to reject the null—we simply don’t have convincing evidence that anything unusual is going on. However, if our data would be extremely unlikely assuming the null hypothesis is true, we reject the null hypothesis in favor of the alternative, concluding that we have found evidence for a real effect.\nFor our example, we want to evaluate whether this jobs program under consideration genuinely reduced unemployment. To do this, we use hypothesis testing. First we state our null and alternative hypotheses:\nNull hypothesis (\\(H_0\\)): The unemployment rate among program participants is the same as the national rate (30%).\nAlternative hypothesis (\\(H_A\\)): The unemployment rate among participants is lower than the national rate.\nNext, we simulate what kinds of results we’d expect if the null hypothesis were true, and then see how our observed result compares. A p-value is the probability of seeing a result as extreme as ours—or more extreme—under the assumption that the null hypothesis is true.\nWe then compare the p-value to a critical value (\\(\\alpha\\)) which is the threshold at which we will reject the null hypothesis. If the p-value is less than \\(\\alpha\\), we reject the null hypothesis. A standard threshold for \\(\\alpha\\) is 0.05. But note that while the choice of \\(\\alpha\\) as .05 is standard, it is also somewhat arbitrary and so the choice of \\(\\alpha\\) can depend on the context of the study.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nThink about a real-world example where you might want to test a claim using hypothesis testing.\nThis could be a claim about a new product, a policy change, or an intervention in a community.\nWhat would your null hypothesis be?\nWhat would your alternative hypothesis be?",
    "crumbs": [
      "Course Modules",
      "Module 7.1"
    ]
  },
  {
    "objectID": "modules/module-7.1.html#simulating-a-null-distribution",
    "href": "modules/module-7.1.html#simulating-a-null-distribution",
    "title": "Module 7.1",
    "section": "Simulating a Null Distribution",
    "text": "Simulating a Null Distribution\nTo evaluate the claim about the jobs program, we need to simulate a null distribution of unemployment rates under the null hypothesis. A null distribution is a collection of simulated results that represent what we would expect to see if the null hypothesis were true (in other words “if nothing was going on”).\nTo simulate thousands of samples efficiently, we can use the infer package from the tidymodels framework. This package allows us to specify our null hypothesis, generate random samples, and calculate the proportion of unemployed individuals in each sample.\n\nlibrary(tidymodels)\n\nnull_dist &lt;- jobs_program |&gt;\n  specify(response = outcome, success = \"unemployed\") |&gt;\n  hypothesize(null = \"point\", p = c(\"unemployed\" = 0.30, \"employed\" = 0.70)) |&gt;\n  generate(reps = 10000, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\n\nnull_dist |&gt;\n  summarize(mean = mean(stat))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 0.300\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = \"steelblue4\") +\n  labs(title = \"Null distribution\")  + theme_bw()\n\n\n\n\n\n\n\nHere we specified the response variable as outcome, with “unemployed” as the success category. We then hypothesized that the unemployment rate is 30% (the national rate) and generated 5000 random samples from this distribution. Finally, we calculated the proportion of unemployed individuals in each sample.\nNext, we can calculate the p-value for our observed resultusing the get_p_value() function from the infer package. Here, the p-value tells us in how many of the simulated samples the proportion of unemployed individuals was at least as extreme as the observed sample proportion (15 out of 60, or 25%).\n\n# Calculate p-value using infer\nnull_dist |&gt;\n  get_p_value(obs_stat = 15/60, direction = \"less\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.243\n\n\nThen we can visualize the null distribution and shade the area corresponding to our observed p-value.\n\n# Visualize p-value in context of null distribution\nvisualize(null_dist) +\n  shade_p_value(obs_stat = 15/60, direction = \"less\")\n\n\n\n\n\n\n\nIf the p-value is greater than 0.05, we say that this result is not statistically significant. That is, we do not have strong evidence that the program was associated with reduced unemployment.\nIn this case, we find that if the true unemployment rate were 30 percent and we draw samples of 60, about 23 percent of the time we will get an unemployment rate lower than the one among the participants in the program (simply due to random chance). Therefore, we do not reject the null hypothesis (because the p-value is greater than 0.05).\n\n\n\n\n\n\nNote\n\n\n\nIn this scenario, we could say that the program caused a reduction in unemployment, even if the p-value were below .05. This is because we do not know whether the program participants were randomly selected from the population to participate in the program, and there may be other factors at play that influence their employment status. So we are simply testing whether the observed difference is statistically significant, meaning it is unlikely to have occurred by chance alone.\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nWhat if the unemployment rate for the program was only 10%? Would you reject the null hypothesis in this case?\nTry changing the null unemployment rate to 50% and the observed rate to 23%. Simulate the null distribution and decide: do you reject the null?",
    "crumbs": [
      "Course Modules",
      "Module 7.1"
    ]
  }
]